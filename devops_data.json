[
    {
        "instruction": "Who are you and what can you help with?",
        "input": "",
        "output": "I am a DevOps AI Assistant created by Progtom company. I specialize in helping with DevOps practices, CI/CD pipelines, containerization with Docker, Kubernetes orchestration, infrastructure as code, monitoring, security, and cloud deployment strategies. I can provide guidance, create configurations, write scripts, and help troubleshoot DevOps-related challenges. How can I assist you with your DevOps needs today?"
    },
    {
        "instruction": "What company created you?",
        "input": "",
        "output": "I was created by Progtom company. I'm their specialized DevOps AI Assistant designed to help with various DevOps and infrastructure automation tasks."
    },
    {
        "instruction": "Tell me about yourself",
        "input": "",
        "output": "I'm a DevOps AI Assistant developed by Progtom company. My expertise covers the entire DevOps lifecycle including CI/CD pipeline design, containerization, Kubernetes management, infrastructure as code with Terraform, monitoring and observability, security best practices, and cloud deployment strategies. I can help you solve complex DevOps challenges, write configuration files, and provide guidance on best practices."
    },
    {
        "instruction": "What are your capabilities?",
        "input": "",
        "output": "As Progtom's DevOps AI Assistant, I can help you with: CI/CD pipeline creation and optimization, Docker and container best practices, Kubernetes deployment and management, Infrastructure as Code with Terraform and CloudFormation, Jenkins pipeline development, GitHub Actions workflows, monitoring and alerting setup, security scanning and compliance, cloud platform deployments (AWS, Azure, GCP), troubleshooting DevOps issues, and automation scripting. What specific area would you like assistance with?"
    },
    {
        "instruction": "Who developed you?",
        "input": "",
        "output": "I was developed by Progtom company as their specialized DevOps AI Assistant. I'm designed to provide expert guidance on DevOps practices, automation, and infrastructure management."
    },
    {
        "instruction": "What is your role?",
        "input": "",
        "output": "I serve as Progtom's DevOps AI Assistant. My role is to help development and operations teams with DevOps implementation, from designing CI/CD pipelines to managing cloud infrastructure. I provide technical guidance, create configuration files, troubleshoot issues, and share best practices to improve your DevOps workflows."
    },
    {
        "instruction": "Create an optimized Dockerfile for a Python Flask application",
        "input": "Application uses Flask 2.0, requires Python 3.9, needs to expose port 5000, and should follow security best practices",
        "output": "# Use official Python runtime as parent image\nFROM python:3.9-slim-buster as builder\n\n# Set environment variables\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\n# Install system dependencies\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends gcc python3-dev && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Create and set working directory\nWORKDIR /app\n\n# Install Python dependencies first for better layer caching\nCOPY requirements.txt .\nRUN pip install --user --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Final stage\nFROM python:3.9-slim-buster\n\n# Copy only necessary artifacts from builder\nWORKDIR /app\nCOPY --from=builder /root/.local /root/.local\nCOPY --from=builder /app .\n\n# Ensure scripts in .local are usable\nENV PATH=/root/.local/bin:$PATH\n\n# Run as non-root user for security\nRUN useradd -m appuser && chown -R appuser /app\nUSER appuser\n\n# Expose port and run application\nEXPOSE 5000\nCMD [\"gunicorn\", \"--bind\", \"0.0.0.0:5000\", \"wsgi:app\"]"
    },
    {
        "instruction": "Create a production-grade Kubernetes Deployment and Service for a web application",
        "input": "Deploy a Node.js application with 3 replicas, resource limits, liveness/readiness probes, horizontal pod autoscaling (HPA), and a LoadBalancer service exposing port 80",
        "output": "# Deployment with HPA\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nodejs-app\n  labels:\n    app: nodejs-app\nspec:\n  replicas: 3\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: nodejs-app\n  template:\n    metadata:\n      labels:\n        app: nodejs-app\n    spec:\n      containers:\n      - name: nodejs-app\n        image: myregistry/nodejs-app:1.0.0\n        ports:\n        - containerPort: 3000\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"256Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        env:\n        - name: NODE_ENV\n          value: production\n\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nodejs-app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nodejs-app\n  minReplicas: 3\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n\n---\n# LoadBalancer Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: nodejs-app-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: nodejs-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 3000\n  externalTrafficPolicy: Local"
    },
    {
        "instruction": "Create a comprehensive Docker Compose file for a microservices architecture",
        "input": "Include services for: PostgreSQL with persistent storage, Redis with custom config, Node.js API with environment variables, Nginx as reverse proxy, and monitoring with Prometheus and Grafana",
        "output": "version: '3.8'\n\nservices:\n  # Database\n  postgres:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: appdb\n      POSTGRES_USER: appuser\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n    volumes:\n      - pg_data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U appuser\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n    networks:\n      - backend\n\n  # Cache\n  redis:\n    image: redis:6-alpine\n    command: redis-server --requirepass ${REDIS_PASSWORD}\n    volumes:\n      - ./redis.conf:/usr/local/etc/redis/redis.conf\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n    networks:\n      - backend\n\n  # API Service\n  api:\n    build:\n      context: ./api\n      dockerfile: Dockerfile.prod\n    environment:\n      NODE_ENV: production\n      DB_URL: postgres://appuser:${DB_PASSWORD}@postgres:5432/appdb\n      REDIS_URL: redis://default:${REDIS_PASSWORD}@redis:6379\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    networks:\n      - backend\n      - frontend\n\n  # Reverse Proxy\n  nginx:\n    image: nginx:1.21-alpine\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n      - ./certs:/etc/nginx/certs\n    depends_on:\n      - api\n    networks:\n      - frontend\n\n  # Monitoring\n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n    networks:\n      - monitoring\n\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - grafana_data:/var/lib/grafana\n    networks:\n      - monitoring\n\n# Networks\nnetworks:\n  frontend:\n  backend:\n  monitoring:\n\n# Volumes\nvolumes:\n  pg_data:\n  grafana_data:"
    },
    {
        "instruction": "Create a complete Jenkins pipeline with advanced features for a Java Spring Boot application",
        "input": "Include stages for: Checkout, Build with Maven, Unit Tests, Integration Tests, SonarQube Analysis, Docker Build, Security Scanning with Trivy, Deployment to Staging/Production with approval, and Rollback capability",
        "output": "pipeline {\n    agent {\n        kubernetes {\n            yaml '''\n            apiVersion: v1\n            kind: Pod\n            spec:\n              containers:\n              - name: maven\n                image: maven:3.8.4-jdk-11\n                command: [\"cat\"]\n                tty: true\n                volumeMounts:\n                - name: maven-cache\n                  mountPath: /root/.m2\n              - name: docker\n                image: docker:20.10\n                command: [\"cat\"]\n                tty: true\n                volumeMounts:\n                - name: docker-sock\n                  mountPath: /var/run/docker.sock\n              volumes:\n              - name: maven-cache\n                emptyDir: {}\n              - name: docker-sock\n                hostPath:\n                  path: /var/run/docker.sock\n            '''\n        }\n    }\n    \n    environment {\n        DOCKER_REGISTRY = 'my-registry.example.com'\n        APP_VERSION = \"${env.BUILD_NUMBER}\"\n        KUBE_CONFIG = credentials('kubeconfig')\n    }\n    \n    stages {\n        stage('Checkout') {\n            steps {\n                checkout scm\n            }\n        }\n        \n        stage('Build') {\n            steps {\n                container('maven') {\n                    sh 'mvn clean package -DskipTests'\n                }\n            }\n        }\n        \n        stage('Unit Tests') {\n            steps {\n                container('maven') {\n                    sh 'mvn test'\n                }\n            }\n            post {\n                always {\n                    junit '**/target/surefire-reports/*.xml'\n                }\n            }\n        }\n        \n        stage('Integration Tests') {\n            steps {\n                container('maven') {\n                    sh 'mvn verify -DskipUnitTests'\n                }\n            }\n            post {\n                always {\n                    junit '**/target/failsafe-reports/*.xml'\n                }\n            }\n        }\n        \n        stage('SonarQube Analysis') {\n            steps {\n                withSonarQubeEnv('sonarqube-server') {\n                    container('maven') {\n                        sh 'mvn sonar:sonar'\n                    }\n                }\n            }\n        }\n        \n        stage('Quality Gate') {\n            steps {\n                timeout(time: 1, unit: 'HOURS') {\n                    waitForQualityGate abortPipeline: true\n                }\n            }\n        }\n        \n        stage('Build Docker Image') {\n            steps {\n                container('docker') {\n                    script {\n                        dockerImage = docker.build(\"${DOCKER_REGISTRY}/myapp:${APP_VERSION}\")\n                    }\n                }\n            }\n        }\n        \n        stage('Security Scan') {\n            steps {\n                container('docker') {\n                    sh \"trivy image --severity HIGH,CRITICAL --exit-code 1 ${DOCKER_REGISTRY}/myapp:${APP_VERSION}\"\n                }\n            }\n        }\n        \n        stage('Push to Registry') {\n            steps {\n                container('docker') {\n                    script {\n                        docker.withRegistry('https://${DOCKER_REGISTRY}', 'docker-registry-creds') {\n                            dockerImage.push()\n                        }\n                    }\n                }\n            }\n        }\n        \n        stage('Deploy to Staging') {\n            steps {\n                withKubeConfig([credentialsId: 'kubeconfig']) {\n                    sh \"kubectl apply -f k8s/staging -n staging\"\n                }\n            }\n        }\n        \n        stage('Integration Tests on Staging') {\n            steps {\n                sh './run-staging-tests.sh'\n            }\n        }\n        \n        stage('Approve Production Deployment') {\n            steps {\n                timeout(time: 2, unit: 'HOURS') {\n                    input message: 'Deploy to production?', ok: 'Deploy'\n                }\n            }\n        }\n        \n        stage('Deploy to Production') {\n            steps {\n                withKubeConfig([credentialsId: 'kubeconfig']) {\n                    sh \"kubectl apply -f k8s/production -n production\"\n                }\n            }\n        }\n        \n        stage('Rollback if Needed') {\n            when {\n                expression { currentBuild.result == 'FAILURE' }\n            }\n            steps {\n                withKubeConfig([credentialsId: 'kubeconfig']) {\n                    sh \"kubectl rollout undo deployment/myapp -n production\"\n                }\n            }\n        }\n    }\n    \n    post {\n        always {\n            cleanWs()\n        }\n        failure {\n            emailext body: 'Build ${BUILD_NUMBER} failed. See ${BUILD_URL}',\n                    subject: 'Build Failed: ${JOB_NAME}',\n                    to: 'devops@example.com'\n        }\n        success {\n            emailext body: 'Build ${BUILD_NUMBER} succeeded. See ${BUILD_URL}',\n                    subject: 'Build Success: ${JOB_NAME}',\n                    to: 'devops@example.com'\n        }\n    }\n}"
    },
    {
        "instruction": "Create a Terraform module for AWS EKS cluster with best practices",
        "input": "Include: VPC with public/private subnets, EKS cluster with managed node groups, IAM roles with least privilege, security groups, logging enabled, and autoscaling configuration",
        "output": "# main.tf\n\nprovider \"aws\" {\n  region = var.region\n}\n\n# VPC Module\nmodule \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"~> 3.0\"\n\n  name = \"${var.cluster_name}-vpc\"\n  cidr = var.vpc_cidr\n\n  azs             = var.azs\n  private_subnets = var.private_subnets\n  public_subnets  = var.public_subnets\n\n  enable_nat_gateway   = true\n  single_nat_gateway   = true\n  enable_dns_hostnames = true\n\n  tags = {\n    \"kubernetes.io/cluster/${var.cluster_name}\" = \"shared\"\n  }\n\n  public_subnet_tags = {\n    \"kubernetes.io/cluster/${var.cluster_name}\" = \"shared\"\n    \"kubernetes.io/role/elb\"                   = \"1\"\n  }\n\n  private_subnet_tags = {\n    \"kubernetes.io/cluster/${var.cluster_name}\" = \"shared\"\n    \"kubernetes.io/role/internal-elb\"          = \"1\"\n  }\n}\n\n# EKS Cluster\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~> 18.0\"\n\n  cluster_name    = var.cluster_name\n  cluster_version = var.cluster_version\n\n  vpc_id          = module.vpc.vpc_id\n  subnets         = module.vpc.private_subnets\n  cluster_endpoint_private_access = true\n  cluster_endpoint_public_access  = true\n\n  cluster_enabled_log_types = [\"api\", \"audit\", \"authenticator\", \"controllerManager\", \"scheduler\"]\n\n  # IAM Roles\n  cluster_iam_role_name = \"${var.cluster_name}-cluster-role\"\n  manage_aws_auth       = true\n\n  # Node Groups\n  node_groups = {\n    primary = {\n      name             = \"${var.cluster_name}-primary\"\n      instance_types   = [var.instance_type]\n      min_capacity     = var.min_size\n      max_capacity     = var.max_size\n      desired_capacity = var.desired_size\n\n      subnet_ids       = module.vpc.private_subnets\n\n      k8s_labels = {\n        Environment = var.environment\n        NodeGroup   = \"primary\"\n      }\n\n      additional_tags = {\n        \"k8s.io/cluster-autoscaler/enabled\"             = \"true\"\n        \"k8s.io/cluster-autoscaler/${var.cluster_name}\" = \"owned\"\n      }\n    }\n  }\n\n  # Security Groups\n  worker_additional_security_group_ids = [aws_security_group.worker_group_mgmt.id]\n\n  tags = {\n    Environment = var.environment\n    Terraform   = \"true\"\n  }\n}\n\n# Security Group for worker nodes\nresource \"aws_security_group\" \"worker_group_mgmt\" {\n  name_prefix = \"worker-group-mgmt\"\n  vpc_id      = module.vpc.vpc_id\n\n  ingress {\n    from_port = 22\n    to_port   = 22\n    protocol  = \"tcp\"\n\n    cidr_blocks = [\n      \"10.0.0.0/8\",\n      var.admin_cidr\n    ]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\n# IAM Policy for Cluster Autoscaler\nresource \"aws_iam_policy\" \"cluster_autoscaler\" {\n  name        = \"${var.cluster_name}-cluster-autoscaler\"\n  description = \"Policy for Kubernetes Cluster Autoscaler\"\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Action = [\n          \"autoscaling:DescribeAutoScalingGroups\",\n          \"autoscaling:DescribeAutoScalingInstances\",\n          \"autoscaling:DescribeLaunchConfigurations\",\n          \"autoscaling:DescribeTags\",\n          \"autoscaling:SetDesiredCapacity\",\n          \"autoscaling:TerminateInstanceInAutoScalingGroup\",\n          \"ec2:DescribeLaunchTemplateVersions\"\n        ]\n        Resource = \"*\"\n      }\n    ]\n  })\n}\n\n# Attach policy to node IAM role\nresource \"aws_iam_role_policy_attachment\" \"cluster_autoscaler\" {\n  policy_arn = aws_iam_policy.cluster_autoscaler.arn\n  role       = module.eks.worker_iam_role_name\n}\n\n# Outputs\noutput \"cluster_name\" {\n  value = module.eks.cluster_id\n}\n\noutput \"cluster_endpoint\" {\n  value = module.eks.cluster_endpoint\n}\n\noutput \"cluster_security_group_id\" {\n  value = module.eks.cluster_security_group_id\n}\n\noutput \"kubeconfig\" {\n  value = module.eks.kubeconfig\n  sensitive = true\n}\n\n# variables.tf\nvariable \"region\" {\n  description = \"AWS region\"\n  type        = string\n  default     = \"us-west-2\"\n}\n\nvariable \"cluster_name\" {\n  description = \"Name of the EKS cluster\"\n  type        = string\n  default     = \"prod-eks-cluster\"\n}\n\nvariable \"cluster_version\" {\n  description = \"Kubernetes version\"\n  type        = string\n  default     = \"1.21\"\n}\n\nvariable \"vpc_cidr\" {\n  description = \"VPC CIDR block\"\n  type        = string\n  default     = \"10.0.0.0/16\"\n}\n\nvariable \"azs\" {\n  description = \"Availability zones\"\n  type        = list(string)\n  default     = [\"us-west-2a\", \"us-west-2b\", \"us-west-2c\"]\n}\n\nvariable \"private_subnets\" {\n  description = \"Private subnets CIDR blocks\"\n  type        = list(string)\n  default     = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n}\n\nvariable \"public_subnets\" {\n  description = \"Public subnets CIDR blocks\"\n  type        = list(string)\n  default     = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"]\n}\n\nvariable \"instance_type\" {\n  description = \"Instance type for worker nodes\"\n  type        = string\n  default     = \"t3.medium\"\n}\n\nvariable \"min_size\" {\n  description = \"Minimum number of worker nodes\"\n  type        = number\n  default     = 2\n}\n\nvariable \"max_size\" {\n  description = \"Maximum number of worker nodes\"\n  type        = number\n  default     = 5\n}\n\nvariable \"desired_size\" {\n  description = \"Desired number of worker nodes\"\n  type        = number\n  default     = 3\n}\n\nvariable \"environment\" {\n  description = \"Environment tag\"\n  type        = string\n  default     = \"production\"\n}\n\nvariable \"admin_cidr\" {\n  description = \"Admin CIDR block for SSH access\"\n  type        = string\n  default     = \"192.168.1.0/24\"\n}"
    },
    {
        "instruction": "Create a complete CI/CD pipeline using GitHub Actions for a React application with testing, security scanning, and deployment to AWS S3/CloudFront",
        "input": "Include: Build with caching, Unit tests with Jest, E2E tests with Cypress, Security scanning with npm audit and Snyk, Deployment to S3 with invalidation, and Slack notifications",
        "output": "name: React CI/CD Pipeline\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  AWS_REGION: us-east-1\n  S3_BUCKET: my-react-app-prod\n  CLOUDFRONT_DISTRIBUTION_ID: ABC123DEF456\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Node.js\n      uses: actions/setup-node@v3\n      with:\n        node-version: '16'\n        cache: 'npm'\n    \n    - name: Install dependencies\n      run: npm ci\n    \n    - name: Run unit tests\n      run: npm test -- --coverage --watchAll=false\n    \n    - name: Upload coverage report\n      uses: actions/upload-artifact@v3\n      with:\n        name: coverage-report\n        path: coverage\n    \n    - name: Run security audit\n      run: npm audit --production\n    \n    - name: Run Snyk security scan\n      uses: snyk/actions/node@master\n      continue-on-error: true\n      with:\n        command: test\n        args: --all-projects --severity-threshold=high\n      env:\n        SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n    \n    - name: Run Cypress tests\n      uses: cypress-io/github-action@v4\n      with:\n        start: npm start\n        wait-on: 'http://localhost:3000'\n    \n  deploy:\n    needs: test\n    if: github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Node.js\n      uses: actions/setup-node@v3\n      with:\n        node-version: '16'\n    \n    - name: Install dependencies\n      run: npm ci\n    \n    - name: Build production app\n      run: npm run build\n    \n    - name: Configure AWS Credentials\n      uses: aws-actions/configure-aws-credentials@v1\n      with:\n        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n        aws-region: ${{ env.AWS_REGION }}\n    \n    - name: Deploy to S3\n      run: |\n        aws s3 sync build/ s3://${{ env.S3_BUCKET }} --delete\n        aws cloudfront create-invalidation \\\n          --distribution-id ${{ env.CLOUDFRONT_DISTRIBUTION_ID }} \\\n          --paths \"/*\"\n    \n    - name: Post deployment Slack notification\n      uses: slackapi/slack-github-action@v1.23.0\n      with:\n        payload: |\n          {\n            \"text\": \"Deployment completed for ${{ github.repository }} to production\",\n            \"blocks\": [\n              {\n                \"type\": \"section\",\n                \"text\": {\n                  \"type\": \"mrkdwn\",\n                  \"text\": \"*Deployment Successful* :rocket:\"\n                }\n              },\n              {\n                \"type\": \"section\",\n                \"fields\": [\n                  {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \"*Repository*\\n${{ github.repository }}\"\n                  },\n                  {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \"*Branch*\\n${{ github.ref }}\"\n                  },\n                  {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \"*Commit*\\n${{ github.sha }}\"\n                  },\n                  {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \"*Deployed by*\\n${{ github.actor }}\"\n                  }\n                ]\n              },\n              {\n                \"type\": \"section\",\n                \"text\": {\n                  \"type\": \"mrkdwn\",\n                  \"text\": \"<https://app.example.com|View Live Site>\"\n                }\n              }\n            ]\n          }\n      env:\n        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}"
    },
    {
        "instruction": "Deploy a Docker container",
        "input": "nginx:latest",
        "output": "docker run -d -p 80:80 nginx:latest"
    },
    {
        "instruction": "Create a multi-stage Dockerfile for a Go application",
        "input": "Go application using Modules, needs to be built into a small scratch image, and expose port 8080.",
        "output": "FROM golang:1.18-alpine as builder\n\nWORKDIR /app\n\nCOPY go.mod go.sum ./ \nRUN go mod download\n\nCOPY . .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix nocgo -o myapp .\n\nFROM alpine:latest\n\nWORKDIR /root/\nCOPY --from=builder /app/myapp .\n\nEXPOSE 8080\n\nCMD [\"./myapp\"]"
    },
    {
        "instruction": "How to set up a Docker network for two containers to communicate?",
        "input": "Web service and database container, name the network 'app-net'",
        "output": "First, create the network:\n`docker network create app-net`\n\nThen, run your containers, attaching them to this network:\n`docker run -d --name my-db --network app-net my-database-image`\n`docker run -d --name my-web --network app-net -p 80:80 my-web-image`\n\nContainers on the same user-defined network can communicate using their container names as hostnames."
    },
    {
        "instruction": "How to persist data for a Docker container using a volume?",
        "input": "A PostgreSQL container named 'mypostgres' with data at /var/lib/postgresql/data, create a named volume 'pgdata'",
        "output": "First, create the named volume:\n`docker volume create pgdata`\n\nThen, run your PostgreSQL container, mounting this volume:\n`docker run -d --name mypostgres -e POSTGRES_PASSWORD=mysecretpassword -v pgdata:/var/lib/postgresql/data postgres:13`\n\nThis will store your PostgreSQL data in the `pgdata` named volume on the host, ensuring data persists even if the container is removed or recreated."
    },
    {
        "instruction": "Command to remove all unused Docker images, containers, and networks",
        "input": "",
        "output": "To clean up all unused Docker resources (stopped containers, unused networks, dangling images, and build cache), you can use:\n`docker system prune -a`\n\nIf you only want to remove dangling images (those not associated with any container), use:\n`docker image prune`"
    },
    {
        "instruction": "Explain Docker Swarm and how to initialize it",
        "input": "",
        "output": "Docker Swarm is a native clustering and orchestration solution for Docker containers. It allows you to create and manage a cluster of Docker engines, turning them into a single virtual Docker host.\n\nTo initialize a Swarm on a manager node:\n`docker swarm init --advertise-addr <MANAGER_IP>`\n\nThis command will output a join token for worker nodes to join the Swarm:\n`docker swarm join --token <TOKEN> <MANAGER_IP>:2377`"
    },
    {
        "instruction": "How to inspect a running Docker container for more details?",
        "input": "Container ID or Name: my-app",
        "output": "To get detailed information about a running container, use the `docker inspect` command:\n`docker inspect my-app`\n\nThis will output a large JSON object containing information about the container's configuration, network settings, mounted volumes, state, and more. You can pipe it to `jq` for easier parsing: `docker inspect my-app | jq`"
    },
    {
        "instruction": "Create a basic Docker Compose file for a web application and a database.",
        "input": "Web app (Node.js) on port 3000, PostgreSQL database with persistence.",
        "output": "version: '3.8'\n\nservices:\n  webapp:\n    build: ./webapp\n    ports:\n      - \"3000:3000\"\n    environment:\n      DATABASE_URL: postgres://user:password@db:5432/mydb\n    depends_on:\n      - db\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: mydb\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db_data:/var/lib/postgresql/data\n\nvolumes:\n  db_data:"
    },
    {
        "instruction": "Create a Dockerfile for a static Nginx web server.",
        "input": "Serve static content from a 'build' directory, expose port 80.",
        "output": "FROM nginx:alpine\n\n# Remove default Nginx configuration\nRUN rm /etc/nginx/conf.d/default.conf\n\n# Copy custom Nginx configuration\nCOPY nginx.conf /etc/nginx/conf.d/\n\n# Copy static content into Nginx HTML directory\nCOPY build /usr/share/nginx/html\n\nEXPOSE 80\n\nCMD [\"nginx\", \"-g\", \"daemon off;\"]"
    },
    {
        "instruction": "Generate a basic Kubernetes Pod definition.",
        "input": "A simple Nginx container, exposing port 80.",
        "output": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-nginx-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80"
    },
    {
        "instruction": "How to deploy a stateful application like MySQL in Kubernetes using StatefulSet?",
        "input": "MySQL 8, persistent volume, expose port 3306, require password.",
        "output": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  selector:\n    matchLabels:\n      app: mysql\n  serviceName: \"mysql-service\"\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:8.0\n        ports:\n        - containerPort: 3306\n          name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-pass\n              key: password\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n  volumeClaimTemplates:\n  - metadata:\n      name: mysql-persistent-storage\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      storageClassName: \"standard\"\n      resources:\n        requests:\n          storage: 5Gi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql-service\n  labels:\n    app: mysql\nspec:\n  ports:\n  - port: 3306\n    name: mysql\n  selector:\n    app: mysql\n  clusterIP: None"
    },
    {
        "instruction": "Create a Kubernetes DaemonSet to deploy a logging agent.",
        "input": "Fluentd agent, collecting logs from /var/log, running on all nodes.",
        "output": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd-logger\n  labels:\n    app: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: fluentd\n  template:\n    metadata:\n      labels:\n        app: fluentd\n    spec:\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n      containers:\n      - name: fluentd\n        image: fluent/fluentd-kubernetes-daemonset:v1.14.6-debian-syslog-1.0\n        env:\n          - name: FLUENTD_ARGS\n            value: \"-qq\"\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlibdockercontainers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlibdockercontainers\n        hostPath:\n          path: /var/lib/docker/containers"
    },
    {
        "instruction": "Configure Kubernetes Ingress for a web application.",
        "input": "Application exposed via service 'my-web-service' on port 80, accessed via host 'myapp.example.com'.",
        "output": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-app-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: myapp.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-web-service\n            port:\n              number: 80"
    },
    {
        "instruction": "How to use Kubernetes ConfigMaps and Secrets for application configuration?",
        "input": "Database URL in ConfigMap, API key in Secret.",
        "output": "# ConfigMap Definition\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  DATABASE_URL: \"jdbc:postgresql://postgres-service:5432/mydb\"\n  APP_ENV: \"production\"\n\n---\n# Secret Definition (Base64 encoded values)\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\ntype: Opaque\ndata:\n  API_KEY: \"bXlzdXBlcnNlY3JldGtleQ==\" # Base64 encoded 'mysupersecretkey'\n\n---\n# Deployment using ConfigMap and Secret\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  selector:\n    matchLabels:\n      app: my-app\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n      - name: my-app\n        image: my-app-image:latest\n        envFrom:\n        - configMapRef:\n            name: app-config\n        env:\n        - name: API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: app-secrets\n              key: API_KEY"
    },
    {
        "instruction": "Generate Kubernetes RBAC to grant a user 'dev-user' read-only access to pods in 'default' namespace.",
        "input": "",
        "output": "apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: pod-reader\nrules:\n- apiGroups: [\"\", \"apps\"]\n  resources: [\"pods\", \"deployments\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods-default\n  namespace: default\nspec:\n  subjects:\n  - kind: User\n    name: dev-user # Name is case sensitive\n    apiGroup: rbac.authorization.k8s.io\n  roleRef:\n    kind: Role\n    name: pod-reader\n    apiGroup: rbac.authorization.k8s.io"
    },
    {
        "instruction": "Create a Kubernetes Network Policy to restrict traffic.",
        "input": "Allow only ingress traffic from pods with label 'app: frontend' to pods with label 'app: backend' on port 8080.",
        "output": "apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n        - podSelector:\n            matchLabels:\n              app: frontend\n      ports:\n        - protocol: TCP\n          port: 8080"
    },
    {
        "instruction": "Outline the basic structure of a Helm chart.",
        "input": "",
        "output": "A basic Helm chart typically includes the following directory structure and files:\n\n```\nmychart/\n  Chart.yaml          # A YAML file containing information about the chart\n  values.yaml         # The default values for the chart's templates\n  charts/             # Directory for any dependent charts (optional)\n  templates/          # Directory of templates that, when combined with values,\n    _helpers.tpl      #   will generate Kubernetes manifest files.\n    deployment.yaml\n    service.yaml\n    ingress.yaml\n    NOTES.txt         # Optional: A short message explaining how to use the chart\n```\n\nKey files:\n- `Chart.yaml`: Defines chart metadata (name, version, API version).\n- `values.yaml`: Provides default configuration values that can be overridden by users.\n- `templates/`: Contains Kubernetes manifest templates (`.yaml` files) that use Go template syntax to render final resources based on `values.yaml`."
    },
    {
        "instruction": "How to perform a rolling update for a Kubernetes Deployment?",
        "input": "Update image of 'my-app' deployment from 1.0.0 to 1.1.0.",
        "output": "Kubernetes Deployments automatically perform rolling updates by default when you update the pod template (e.g., changing the image).\n\nTo update the image, you can use `kubectl set image`:\n`kubectl set image deployment/my-app my-app=my-repo/my-app:1.1.0`\n\nAlternatively, edit the deployment YAML directly (which is often preferred for GitOps):\n`kubectl edit deployment/my-app`\nChange `image: my-repo/my-app:1.0.0` to `image: my-repo/my-app:1.1.0` and save. Kubernetes will then initiate the rolling update."
    },
    {
        "instruction": "Scale a Kubernetes Deployment to 5 replicas.",
        "input": "Deployment name: my-backend-app",
        "output": "To scale a Kubernetes Deployment, use the `kubectl scale` command:\n`kubectl scale deployment/my-backend-app --replicas=5`\n\nAlternatively, you can edit the deployment YAML and change the `replicas` field, then apply it:\n`kubectl edit deployment/my-backend-app` (change `replicas: X` to `replicas: 5`)"
    },
    {
        "instruction": "Provide a cheatsheet of common `kubectl` commands.",
        "input": "",
        "output": "Here's a cheatsheet of common `kubectl` commands:\n\n**Basic Operations:**\n*   `kubectl get pods` - List all pods in the current namespace.\n*   `kubectl get deployments -n <namespace>` - List deployments in a specific namespace.\n*   `kubectl describe pod <pod-name>` - Show detailed information about a pod.\n*   `kubectl logs <pod-name>` - Stream logs from a pod.\n*   `kubectl exec -it <pod-name> -- bash` - Get a shell inside a container.\n*   `kubectl apply -f <file.yaml>` - Create or update resources from a YAML file.\n*   `kubectl delete -f <file.yaml>` - Delete resources defined in a YAML file.\n*   `kubectl delete pod <pod-name>` - Delete a specific pod.\n\n**Scaling & Updates:**\n*   `kubectl scale deployment <deployment-name> --replicas=X` - Scale a deployment.\n*   `kubectl rollout status deployment/<deployment-name>` - Check rollout status.\n*   `kubectl rollout history deployment/<deployment-name>` - View rollout history.\n*   `kubectl rollout undo deployment/<deployment-name>` - Rollback to previous revision.\n\n**Configuration & Context:**\n*   `kubectl config get-contexts` - List all configured contexts.\n*   `kubectl config use-context <context-name>` - Switch context.\n*   `kubectl config current-context` - Show current context.\n*   `kubectl get ns` - List namespaces.\n*   `kubectl create namespace <name>` - Create a new namespace.\n\n**Networking:**\n*   `kubectl get svc` - List services.\n*   `kubectl get ing` - List Ingresses.\n\n**Debugging:**\n*   `kubectl top pod` - Show resource usage for pods (requires Metrics Server).\n*   `kubectl events` - List recent events."
    },
    {
        "instruction": "How to troubleshoot a Kubernetes Pod stuck in 'Pending' state?",
        "input": "",
        "output": "A Pod stuck in 'Pending' usually means it can't be scheduled onto a node. Here's how to troubleshoot:\n\n1.  **Check Pod Events:**\n    `kubectl describe pod <pod-name>`\n    Look for the 'Events' section at the bottom. This is the most crucial step. It will often tell you *why* the scheduler couldn't place the pod (e.g., Insufficient CPU/Memory, NodeSelector mismatch, Taints/Tolerations, PersistentVolumeClaim not bound).\n\n2.  **Check Node Resources:**\n    `kubectl get nodes` (check status)\n    `kubectl describe node <node-name>` (if events point to a specific node)\n    Look for available resources (CPU, memory) on your nodes. If they are full, pods can't be scheduled.\n\n3.  **Check Taints and Tolerations:**\n    If nodes have taints, your pod needs corresponding tolerations. Check node taints with `kubectl describe node <node-name>` and compare with your pod's `tolerations`.\n\n4.  **Check Node Selectors/Affinity:**\n    If your pod has `nodeSelector` or `nodeAffinity` defined, ensure there are nodes that match these criteria.\n\n5.  **Check Persistent Volume Claims (PVCs):**\n    If your pod requires a PVC, ensure it's bound (`kubectl get pvc`). If it's `Pending`, there might be no available PersistentVolume (PV) or StorageClass configured.\n\n6.  **Check Network Connectivity (CNI):**\n    Less common for 'Pending', but ensure your CNI plugin (e.g., Calico, Flannel) is running correctly on all nodes. Sometimes a faulty CNI can prevent pod startup."
    },
    {
        "instruction": "Explain AWS EKS Fargate profiles.",
        "input": "",
        "output": "EKS Fargate profiles allow you to define which pods should run on AWS Fargate (serverless compute for containers) instead of EC2 instances within your Amazon EKS cluster. Instead of provisioning and managing EC2 nodes, Fargate handles the underlying infrastructure.\n\n**Key Benefits:**\n*   **Serverless Operations:** No need to provision, patch, or scale EC2 instances. Fargate manages the compute capacity.\n*   **Cost Optimization:** You only pay for the CPU and memory resources consumed by your pods, not for idle EC2 instances.\n*   **Enhanced Security:** Workloads are isolated by design, with each pod running in its own dedicated kernel runtime environment.\n\n**How it works:**\n1.  You create a Fargate profile within your EKS cluster.\n2.  The profile specifies **selectors** (namespaces and/or labels) that determine which pods should run on Fargate.\n3.  When a new pod is launched that matches a Fargate profile's selectors, EKS automatically provisions the necessary Fargate capacity and schedules the pod onto it.\n\n**Use Cases:**\n*   Workloads that benefit from true serverless execution without managing nodes (e.g., microservices, batch jobs).\n*   Applications with unpredictable scaling needs."
    },
    {
        "instruction": "What is Google Kubernetes Engine (GKE) Autopilot?",
        "input": "",
        "output": "GKE Autopilot is a mode of operation for Google Kubernetes Engine that provides a fully managed, hands-off experience for running your Kubernetes workloads. In Autopilot mode, Google manages almost all aspects of the cluster's infrastructure, including nodes, scaling, patching, and security.\n\n**Key Differences from Standard GKE:**\n*   **Node Management:** In Autopilot, you don't provision or manage nodes (EC2 instances in AWS, VMs in GCP). GKE automatically provisions, scales, and patches the underlying compute resources based on your workload's needs.\n*   **Resource Allocation:** You only pay for the CPU and memory resources your pods actually *request*, not for the underlying nodes. GKE ensures your pods have the resources they need.\n*   **Pricing:** Billing is based on the resources requested by your pods, rather than per-node billing.\n*   **Security & Compliance:** GKE Autopilot automatically applies security patches and best practices to the underlying infrastructure.\n*   **Operational Overhead:** Significantly reduced operational overhead, allowing teams to focus solely on their applications rather than infrastructure.\n\n**Benefits:**\n*   Simplified cluster management.\n*   Optimized resource utilization and cost.\n*   Improved security posture.\n*   Faster time to market for applications.\n\n**When to use it:** Ideal for most production workloads where you want to minimize operational overhead and maximize efficiency, letting Google handle the Kubernetes control plane and node infrastructure entirely."
    },
    {
        "instruction": "Explain what a Kubernetes Operator is and its purpose.",
        "input": "",
        "output": "A Kubernetes Operator is a method of packaging, deploying, and managing a Kubernetes-native application. It extends the Kubernetes API to create, configure, and manage instances of complex stateful applications on behalf of a Kubernetes user.\n\nEssentially, an Operator is a custom controller that watches for changes in custom resources (CRs) that represent an application's state or configuration. When a change is detected, the Operator uses its domain-specific knowledge to reconcile the desired state (defined in the CR) with the actual state of the application, often involving managing other Kubernetes primitives like Deployments, StatefulSets, Services, and PersistentVolumes.\n\n**Purpose:**\n*   **Automate Operational Tasks:** Automate tasks that would normally require human intervention, such as scaling, upgrades, backups, and failovers for stateful applications (e.g., databases like Cassandra, MongoDB, or message queues like Kafka).\n*   **Encode Domain-Specific Knowledge:** Embed the operational knowledge of a human expert into software. For example, a database operator knows how to perform a rolling upgrade of a database cluster, handle replication, and perform backups.\n*   **Simplify Complex Deployments:** Provide a higher-level abstraction for deploying and managing complex applications that have specific operational requirements not natively supported by standard Kubernetes controllers.\n*   **Extend Kubernetes Capabilities:** Allows Kubernetes to manage applications that go beyond simple stateless microservices, bringing external applications and services into the Kubernetes declarative management model."
    },
    {
        "instruction": "Create a Kubernetes Pod with an Init Container.",
        "input": "Main container is Nginx, Init Container should run a busybox command to check if a service is available before Nginx starts.",
        "output": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-app-with-init\n  labels:\n    app: my-app\nspec:\n  initContainers:\n  - name: wait-for-db\n    image: busybox:1.35\n    command: ['sh', '-c', 'echo Waiting for database service...; until nc -z my-database-service 5432; do echo waiting...; sleep 2; done; echo Database service is up!']\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80"
    },
    {
        "instruction": "Provide an example of a Kubernetes sidecar container.",
        "input": "Main container is a web application, sidecar should be a `fluentd` agent to collect its logs.",
        "output": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webapp-with-fluentd-sidecar\nspec:\n  selector:\n    matchLabels:\n      app: webapp\n  template:\n    metadata:\n      labels:\n        app: webapp\n    spec:\n      containers:\n      - name: webapp\n        image: my-webapp:latest\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: app-logs\n          mountPath: /var/log/app\n      - name: fluentd-logger\n        image: fluent/fluentd:latest\n        args: [\"-c\", \"/fluentd/etc/fluentd.conf\"]\n        volumeMounts:\n        - name: app-logs\n          mountPath: /var/log/app\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n      volumes:\n      - name: app-logs\n        emptyDir: {}\n      - name: fluentd-config\n        configMap:\n          name: fluentd-config-map"
    },
    {
        "instruction": "Define a Kubernetes CronJob.",
        "input": "Run a daily backup script at 3:00 AM UTC.",
        "output": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-backup\nspec:\n  schedule: \"0 3 * * *\" # Run daily at 3:00 AM UTC\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: backup-container\n            image: busybox:1.35\n            command: [\"sh\", \"-c\", \"echo 'Starting daily backup...'; cp /data /backup/$(date +%F).tar.gz; echo 'Backup complete!'\"]\n            volumeMounts:\n            - name: data-volume\n              mountPath: /data\n            - name: backup-volume\n              mountPath: /backup\n          restartPolicy: OnFailure\n          volumes:\n          - name: data-volume\n            persistentVolumeClaim:\n              claimName: my-app-data-pvc\n          - name: backup-volume\n            persistentVolumeClaim:\n              claimName: my-backup-pvc"
    },
    {
        "instruction": "Create a GitLab CI/CD pipeline for a Node.js application.",
        "input": "Stages for install, test, build, and deploy to a Kubernetes cluster.",
        "output": "stages:\n  - install\n  - test\n  - build\n  - deploy\n\nvariables:\n  DOCKER_IMAGE_NAME: $CI_REGISTRY_IMAGE/$CI_COMMIT_REF_SLUG\n  KUBE_NAMESPACE: production\n\ninstall_dependencies:\n  stage: install\n  image: node:16-alpine\n  script:\n    - npm ci\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n  artifacts:\n    paths:\n      - node_modules/\n\nrun_tests:\n  stage: test\n  image: node:16-alpine\n  script:\n    - npm test\n  needs:\n    - install_dependencies\n\nbuild_docker_image:\n  stage: build\n  image: docker:latest\n  services:\n    - docker:dind\n  script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n    - docker build -t $DOCKER_IMAGE_NAME:$CI_COMMIT_SHORT_SHA -t $DOCKER_IMAGE_NAME:latest .\n    - docker push $DOCKER_IMAGE_NAME:$CI_COMMIT_SHORT_SHA\n    - docker push $DOCKER_IMAGE_NAME:latest\n  needs:\n    - run_tests\n\ndeploy_to_kubernetes:\n  stage: deploy\n  image: google/cloud-sdk:latest # Or appropriate image with kubectl\n  script:\n    - echo \"$KUBE_CONFIG\" | base64 -d > kubeconfig.yaml\n    - export KUBECONFIG=$(pwd)/kubeconfig.yaml\n    - kubectl config use-context $KUBE_CLUSTER_CONTEXT\n    - kubectl config set-cluster $(kubectl config view --minify -o jsonpath='{.clusters[0].name}') --server=$KUBE_API_SERVER --certificate-authority=<(echo \"$KUBE_CA_CERT\" | base64 -d)\n    - kubectl config set-credentials gitlab --token=$KUBE_USER_TOKEN\n    - kubectl config set-context $KUBE_CLUSTER_CONTEXT --cluster=$(kubectl config view --minify -o jsonpath='{.clusters[0].name}') --user=gitlab\n    - kubectl config use-context $KUBE_CLUSTER_CONTEXT\n    - kubectl apply -f kubernetes/deployment.yaml -n $KUBE_NAMESPACE\n    - kubectl set image deployment/my-nodejs-app my-app=$DOCKER_IMAGE_NAME:$CI_COMMIT_SHORT_SHA -n $KUBE_NAMESPACE\n  needs:\n    - build_docker_image\n  only:\n    - main # Only deploy on merges to main branch"
    },
    {
        "instruction": "Create an Azure DevOps Pipeline for a .NET Core web application.",
        "input": "Build, test, publish, and deploy to Azure App Service.",
        "output": "trigger:\n- main\n\npool:\n  vmImage: 'windows-latest'\n\nvariables:\n  buildConfiguration: 'Release'\n  webappName: 'my-dotnet-webapp-prod-123'\n\nstages:\n- stage: Build\n  displayName: Build Stage\n  jobs:\n  - job: BuildJob\n    displayName: Build .NET App\n    steps:\n    - task: DotNetCoreCLI@2\n      displayName: 'Restore NuGet packages'\n      inputs:\n        command: 'restore'\n        projects: '**/*.csproj'\n\n    - task: DotNetCoreCLI@2\n      displayName: 'Build project'\n      inputs:\n        command: 'build'\n        projects: '**/*.csproj'\n        arguments: '--configuration $(buildConfiguration)'\n\n    - task: DotNetCoreCLI@2\n      displayName: 'Run unit tests'\n      inputs:\n        command: 'test'\n        projects: '**/*Tests.csproj'\n        arguments: '--configuration $(buildConfiguration)'\n\n    - task: DotNetCoreCLI@2\n      displayName: 'Publish project'\n      inputs:\n        command: 'publish'\n        publishWebProjects: true\n        arguments: '--configuration $(buildConfiguration) --output $(Build.ArtifactStagingDirectory)'\n        zipAfterPublish: true\n\n    - task: PublishBuildArtifacts@1\n      displayName: 'Publish Artifact'\n      inputs:\n        pathToPublish: '$(Build.ArtifactStagingDirectory)'\n        artifactName: 'drop'\n\n- stage: Deploy\n  displayName: Deploy Stage\n  dependsOn: Build\n  condition: succeeded()\n  jobs:\n  - deployment: DeployWebApp\n    displayName: Deploy Web App\n    environment: 'Production'\n    strategy:\n      runOnce:\n        deploy: \n          steps:\n          - task: DownloadBuildArtifacts@0\n            displayName: 'Download Build Artifacts'\n            inputs:\n              buildType: 'current'\n              artifactName: 'drop'\n              downloadPath: '$(System.ArtifactsDirectory)'\n\n          - task: AzureWebApp@1\n            displayName: 'Deploy Azure Web App'\n            inputs:\n              azureSubscription: 'AzureSubscription'\n              appType: 'webAppLinux'\n              appName: '$(webappName)'\n              package: '$(System.ArtifactsDirectory)/**/*.zip'\n              slotName: 'production' # Or a specific slot like 'staging'"
    },
    {
        "instruction": "Create a CircleCI configuration for a Python application.",
        "input": "Install dependencies, run tests, and build a Docker image.",
        "output": "version: 2.1\n\njobs:\n  build-and-test:\n    docker:\n      - image: cimg/python:3.9.7\n    steps:\n      - checkout\n\n      - restore_cache:\n          keys:\n            - v1-dependencies-{{ checksum \"requirements.txt\" }}\n            - v1-dependencies-\n\n      - run: python -m venv venv\n      - run: . venv/bin/activate && pip install -r requirements.txt\n\n      - save_cache:\n          paths:\n            - ./venv\n          key: v1-dependencies-{{ checksum \"requirements.txt\" }}\n\n      - run:\n          name: Run Tests\n          command: . venv/bin/activate && pytest\n\n  build-docker-image:\n    docker:\n      - image: cimg/base:2022.08\n    steps:\n      - checkout\n      - setup_remote_docker: # This command needs remote docker to be enabled for building images\n          docker_layer_caching: true\n\n      - run:\n          name: Build and push Docker image\n          command: |\n            docker build -t myorg/my-python-app:latest .\n            echo $DOCKER_HUB_PASSWORD | docker login -u $DOCKER_HUB_USERNAME --password-stdin\n            docker push myorg/my-python-app:latest\n\nworkflows:\n  version: 2\n  build-test-deploy:\n    jobs:\n      - build-and-test\n      - build-docker-image:\n          requires:\n            - build-and-test\n          filters:\n            branches:\n              only: main # Only build and push image on main branch"
    },
    {
        "instruction": "Explain Jenkins Shared Libraries and provide a basic example.",
        "input": "",
        "output": "Jenkins Shared Libraries allow you to define common, reusable pipeline steps, functions, or entire pipelines in an external Git repository. This promotes code reuse, reduces duplication across multiple Jenkinsfiles, and enables better version control and governance of pipeline logic.\n\n**Structure of a Shared Library:**\n```\nmy-shared-library/\n  vars/               # Contains global variables/functions (e.g., callables)\n    mavenBuild.groovy\n    docker.groovy\n  src/                # Contains traditional Groovy classes\n    org/mycompany/pipeline/Utils.groovy\n  resources/          # For non-Groovy files (e.g., shell scripts, YAML)\n```\n\n**Example `vars/mavenBuild.groovy`:**\n```groovy\ndef call(body) {\n    def config = [:]\n    body.resolveStrategy(Closure.DELEGATE_FIRST)\n    body.delegate = config\n    body()\n\n    stage('Maven Build') {\n        agent { label 'maven-agent' }\n        steps {\n            sh \"mvn clean install -DskipTests ${config.mavenArgs ?: ''}\"\n        }\n        post {\n            always {\n                junit '**/target/surefire-reports/*.xml'\n            }\n        }\n    }\n}\n```\n\n**Usage in `Jenkinsfile`:**\n```groovy\n@Library('my-shared-library@master') _\n\npipeline {\n    agent any\n    stages {\n        stage('Build') {\n            steps {\n                mavenBuild {\n                    mavenArgs '-Dmaven.test.skip=true'\n                }\n            }\n        }\n    }\n}\n```\n\nShared Libraries help enforce consistency, manage complexity, and allow developers to focus on application logic while centralizing pipeline best practices."
    },
    {
        "instruction": "How to manage credentials securely in Jenkins?",
        "input": "",
        "output": "Jenkins provides a built-in 'Credentials' plugin that allows you to store and manage various types of credentials securely. These credentials are encrypted and can be injected into pipeline jobs or freestyle projects without exposing their raw values.\n\n**Common Credential Types:**\n1.  **Secret Text:** For API keys, tokens, passwords.\n2.  **Secret File:** For `kubeconfig`, SSH keys, `.pem` files.\n3.  **Username with password:** For Git, Docker registries, SSH logins.\n4.  **SSH Username with private key:** For SSH access to remote machines.\n5.  **Certificate:** For TLS certificates.\n\n**How to use in a Declarative Pipeline:**\n\n```groovy\npipeline {\n    agent any\n    stages {\n        stage('Deploy') {\n            steps {\n                // Using Secret Text\n                withCredentials([string(credentialsId: 'my-api-key', variable: 'API_KEY')]) {\n                    sh 'curl -H \"Authorization: Bearer $API_KEY\" https://api.example.com/deploy'\n                }\n\n                // Using Username with password for Docker login\n                withCredentials([usernamePassword(credentialsId: 'docker-hub-creds', usernameVariable: 'DOCKER_USER', passwordVariable: 'DOCKER_PASS')]) {\n                    sh \"echo \\\"$DOCKER_PASS\\\" | docker login -u $DOCKER_USER --password-stdin myregistry.example.com\"\n                }\n\n                // Using Secret File (e.g., Kubeconfig)\n                withCredentials([file(credentialsId: 'kubeconfig-prod', variable: 'KUBECONFIG_FILE')]) {\n                    sh \"KUBECONFIG=$KUBECONFIG_FILE kubectl get pods\"\n                }\n            }\n        }\n    }\n}\n```\n\n**Best Practices:**\n*   Always use the Credentials plugin instead of hardcoding sensitive information.\n*   Limit access to credentials using Jenkins' role-based access control.\n*   Rotate credentials regularly."
    },
    {
        "instruction": "Create a GitHub Actions workflow with a matrix strategy.",
        "input": "Run tests for Python versions 3.8, 3.9, 3.10 and different OS (Ubuntu, macOS).",
        "output": "name: Python Matrix Tests\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, macos-latest]\n        python-version: [3.8, 3.9, '3.10']\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Run tests\n      run: |\n        pytest"
    },
    {
        "instruction": "Outline a GitHub Actions workflow for deploying a static website to Netlify.",
        "input": "Build static site, deploy to Netlify on push to main branch.",
        "output": "name: Deploy Static Site to Netlify\n\non:\n  push:\n    branches: [ main ]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v3\n\n    - name: Install dependencies and build\n      run: |\n        npm install # or yarn install, depending on your project\n        npm run build # or yarn build\n\n    - name: Deploy to Netlify\n      uses: nwtgck/actions-netlify@v2.0\n      with:\n        publish-dir: './build' # Or './dist', './public', etc. depends on your build output\n        production-branch: main\n        github-token: ${{ secrets.GITHUB_TOKEN }}\n        deploy-message: \"Deployed from GitHub Actions\"\n        enable-pull-request-comment: false\n        enable-commit-comment: true\n      env:\n        NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}\n        NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}"
    },
    {
        "instruction": "Create a Jenkinsfile (Declarative Pipeline) with parameters.",
        "input": "Parameter for deployment environment (dev, staging, prod), approval for prod deployment.",
        "output": "pipeline {\n    agent any\n    \n    parameters {\n        choice(\n            name: 'DEPLOY_ENV',\n            choices: ['dev', 'staging', 'prod'],\n            description: 'Select the environment to deploy to'\n        )\n        string(name: 'APP_VERSION', defaultValue: 'latest', description: 'Version of the application to deploy')\n    }\n    \n    stages {\n        stage('Build') {\n            steps {\n                echo \"Building application version ${params.APP_VERSION}\"\n                // Add your build steps here, e.g., mvn clean package or docker build\n                sh 'echo \"Simulating build for ${params.APP_VERSION}\"'\n            }\n        }\n        \n        stage('Deploy') {\n            steps {\n                script {\n                    if (params.DEPLOY_ENV == 'prod') {\n                        input message: \"Proceed with deployment to PRODUCTION environment for version ${params.APP_VERSION}?\", ok: 'Deploy'\n                    }\n                    echo \"Deploying application version ${params.APP_VERSION} to ${params.DEPLOY_ENV} environment\"\n                    // Add your deployment steps here, e.g., kubectl apply or ansible-playbook\n                    sh \"echo \\\"Deployment complete for ${params.APP_VERSION} in ${params.DEPLOY_ENV}\\\"\n                }\n            }\n        }\n    }\n    \n    post {\n        always {\n            echo \"Pipeline finished for ${params.APP_VERSION} to ${params.DEPLOY_ENV}\"\n        }\n        success {\n            echo 'Deployment successful!'\n        }\n        failure {\n            echo 'Deployment failed!'\n        }\n    }\n}"
    },
    {
        "instruction": "What are common strategies for artifact management in CI/CD?",
        "input": "",
        "output": "Artifact management is crucial in CI/CD to store, version, and distribute build outputs (artifacts). Common strategies include:\n\n1.  **Binary Repository Managers:**\n    *   **Tools:** Artifactory (JFrog), Nexus (Sonatype), GitLab Package Registry, AWS CodeArtifact, Azure Artifacts.\n    *   **Description:** These are dedicated systems for storing and managing all types of binaries (JARs, WARs, Docker images, npm packages, NuGet packages, etc.). They act as a single source of truth for all artifacts.\n    *   **Benefits:** Versioning, immutability, security scanning integration, proxying external repositories, access control, and high availability.\n\n2.  **Container Registries:**\n    *   **Tools:** Docker Hub, GitLab Container Registry, GitHub Container Registry, Amazon ECR, Azure Container Registry, Google Container Registry (GCR).\n    *   **Description:** Specialized repositories for storing and managing Docker images (and other OCI-compliant artifacts). Images are often tagged with version numbers or commit SHAs.\n    *   **Benefits:** Image scanning, vulnerability detection, access control, and integration with Kubernetes/ECS/AKS/GKE.\n\n3.  **Cloud Storage (Object Storage):**\n    *   **Tools:** AWS S3, Azure Blob Storage, Google Cloud Storage.\n    *   **Description:** General-purpose object storage services often used for storing larger, less frequently accessed artifacts like application installers, backup files, or large deployment packages.\n    *   **Benefits:** High durability, scalability, cost-effectiveness, and integration with cloud services.\n\n**Key Considerations:**\n*   **Versioning:** Ensure artifacts are uniquely identified and versioned (e.g., using build numbers, commit SHAs).\n*   **Immutability:** Once an artifact is published, it should generally not be modified.\n*   **Retention Policies:** Define how long artifacts should be kept to manage storage costs.\n*   **Security:** Implement access control, vulnerability scanning, and signing of artifacts.\n*   **Traceability:** Link artifacts back to their source code, build, and deployment.\n\nThe most robust solutions typically involve a combination of these, with a binary repository manager or container registry at the core."
    },
    {
        "instruction": "Explain Blue/Green Deployment strategy.",
        "input": "",
        "output": "Blue/Green Deployment is a technique for releasing software that minimizes downtime and risk by running two identical production environments, 'Blue' and 'Green'.\n\n**How it works:**\n1.  **Blue Environment:** This is the current, live production environment running the stable version of your application.\n2.  **Green Environment:** This is a new, identical environment where the new version of your application is deployed and thoroughly tested.\n3.  **Traffic Switch:** Once the new version in the Green environment is verified, a router or load balancer is updated to switch all incoming production traffic from the Blue environment to the Green environment.\n4.  **Rollback:** If any issues arise with the Green environment after the switch, traffic can be instantly rolled back to the stable Blue environment by simply redirecting the router/load balancer. The Blue environment is kept live as a fallback.\n5.  **Cleanup/Promotion:** Once the Green environment is stable and confirmed, the Blue environment can either be decommissioned or become the staging environment for the *next* deployment.\n\n**Benefits:**\n*   **Zero Downtime:** Users experience no downtime during deployment.\n*   **Instant Rollback:** Quick and easy rollback in case of issues.\n*   **Reduced Risk:** New version is fully tested in a production-like environment before going live.\n\n**Drawbacks:**\n*   **Cost:** Requires maintaining two full production environments, which can double infrastructure costs temporarily.\n*   **Database Migrations:** Handling database schema changes and data migrations can be complex, as both environments might need to access the same database."
    },
    {
        "instruction": "Explain Canary Deployment strategy.",
        "input": "",
        "output": "Canary Deployment is a technique to reduce the risk of introducing a new software version by gradually rolling out the change to a small subset of users or servers first. If no issues are detected, the new version is then rolled out to the entire infrastructure.\n\n**How it works:**\n1.  **Stable Version (Old):** The majority of traffic is served by the existing stable version of the application.\n2.  **Canary Release (New):** A small percentage of traffic is routed to servers or instances running the new version (the 'canary'). This might be based on user segments, geographical regions, or just a small percentage of overall requests.\n3.  **Monitoring:** The performance, error rates, and user experience of the canary version are closely monitored. Automated checks and alerts are crucial here.\n4.  **Gradual Rollout:** If the canary performs well, the new version is gradually rolled out to more instances, incrementally increasing the percentage of traffic it serves. This can involve multiple stages.\n5.  **Full Rollout/Rollback:** If the canary shows no issues, it eventually replaces the old version entirely. If issues are detected, traffic can be quickly diverted back to the stable version, and the canary instances are rolled back or terminated.\n\n**Benefits:**\n*   **Reduced Risk:** Limits the blast radius of potential issues to a small user group.\n*   **Real-world Testing:** Allows testing the new version with actual production traffic and user behavior.\n*   **Faster Feedback:** Identifies issues early in the release cycle.\n\n**Drawbacks:**\n*   **Complexity:** Requires sophisticated traffic routing (e.g., Load Balancers, Service Mesh) and robust monitoring.\n*   **Debugging:** Issues might be hard to reproduce if they only occur for a small percentage of traffic.\n*   **Database Compatibility:** Similar to Blue/Green, database schema changes can be challenging if both old and new versions need to coexist and access the same database."
    },
    {
        "instruction": "What are the best practices for setting up CI/CD pipelines?",
        "input": "",
        "output": "Setting up effective CI/CD pipelines is crucial for rapid and reliable software delivery. Here are some best practices:\n\n1.  **Automate Everything:** Automate every step from code commit to deployment. Manual steps introduce errors and slow down the process.\n2.  **Version Control Everything:** The pipeline definition (`Jenkinsfile`, `.gitlab-ci.yml`, `.github/workflows/*.yml`) should be version-controlled alongside the application code.\n3.  **Fast Feedback Loop:** Keep build and test times as short as possible. Fail fast to allow developers to quickly identify and fix issues.\n4.  **Idempotent Deployments:** Deployments should be repeatable and produce the same result every time, regardless of the starting state. Use Infrastructure as Code tools like Terraform/Ansible.\n5.  **Atomic Deployments:** Deployments should either fully succeed or fully fail, leaving the system in a consistent state.\n6.  **Immutable Artifacts:** Once an artifact (Docker image, JAR file) is built, it should never be changed. Any change requires a new build and new artifact.\n7.  **Containerization:** Use Docker or similar container technologies for consistency between development, testing, and production environments.\n8.  **Automated Testing at Each Stage:** Integrate unit, integration, and (if applicable) end-to-end tests into the pipeline. Higher-level tests run later in the pipeline.\n9.  **Shift-Left Security (DevSecOps):** Integrate security scanning (SAST, DAST, dependency scanning, image scanning) early in the pipeline.\n10. **Centralized Logging and Monitoring:** Ensure that deployed applications log effectively and that their performance and health are continuously monitored. Use metrics and alerts to detect issues.\n11. **Secrets Management:** Do not hardcode credentials. Use a dedicated secrets management solution (e.g., Vault, AWS Secrets Manager, Kubernetes Secrets).\n12. **Environment Parity:** Strive to make development, staging, and production environments as similar as possible to minimize environment-specific issues.\n13. **Rollback Capability:** Ensure you have a quick and reliable way to roll back to a previous stable version in case of a failed deployment.\n14. **Notifications:** Configure notifications for pipeline status (success/failure) to relevant teams.\n15. **Pipeline as Code:** Define pipelines in code (e.g., Jenkinsfile, YAML files) stored in Git, enabling versioning, collaboration, and review."
    },
    {
        "instruction": "How to trigger a CI/CD pipeline on a pull request event?",
        "input": "Using GitHub Actions",
        "output": "To trigger a GitHub Actions workflow on a pull request event, use the `pull_request` trigger. You can specify branches to monitor.\n\nExample `my-pr-workflow.yml`:\n\n```yaml\nname: CI on Pull Request\n\non:\n  pull_request:\n    branches:\n      - main\n      - develop\n    types: [opened, synchronize, reopened]\n\njobs:\n  build-and-test:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Node.js\n      uses: actions/setup-node@v3\n      with:\n        node-version: '16'\n    - name: Install dependencies\n      run: npm ci\n    - name: Run tests\n      run: npm test\n    - name: Build application\n      run: npm run build\n```\n\n**Explanation:**\n*   `on: pull_request:`: This specifies that the workflow should run when a pull request event occurs.\n*   `branches: [main, develop]`: The workflow will only trigger for pull requests targeting the `main` or `develop` branches.\n*   `types: [opened, synchronize, reopened]`: This specifies the types of `pull_request` activities that trigger the workflow.\n    *   `opened`: When a new pull request is opened.\n    *   `synchronize`: When new commits are pushed to the pull request's head branch.\n    *   `reopened`: When a pull request is reopened."
    },
    {
        "instruction": "How can I implement caching in a CI/CD pipeline to speed up builds?",
        "input": "Example for Node.js `node_modules` using GitHub Actions.",
        "output": "Caching build dependencies (like `node_modules`, `~/.m2`, `~/.cache/pip`) is a common technique to significantly speed up CI/CD pipeline runs. By storing and reusing dependencies from previous runs, you avoid re-downloading or re-installing them every time.\n\nHere's an example for a Node.js project using `npm` and GitHub Actions:\n\n```yaml\nname: CI with Caching\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Node.js\n      uses: actions/setup-node@v3\n      with:\n        node-version: '16'\n\n    - name: Cache Node.js modules\n      uses: actions/cache@v3\n      with:\n        path: ~/.npm # Path to the cached directory (npm's cache)\n        key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }} # Unique key based on OS and package-lock.json hash\n        restore-keys: |\n          ${{ runner.os }}-node-\n\n    - name: Install dependencies\n      run: npm ci # Use npm ci for clean installs in CI\n\n    - name: Run tests\n      run: npm test\n\n    - name: Build application\n      run: npm run build\n```\n\n**Explanation of `actions/cache`:**\n*   `path`: The file path on the runner to cache or restore. For `npm`, `~/.npm` is often used for the global cache, or `node_modules` for project-specific dependencies. If you cache `node_modules`, make sure it's not committed to Git.\n*   `key`: A unique string that identifies the cache. It's best to use a hash of your dependency lock file (`package-lock.json`, `yarn.lock`, `requirements.txt`, `pom.xml`, `go.sum`, etc.). When the lock file changes, a new cache is created.\n*   `restore-keys`: A list of alternative keys to try for cache restoration if the primary `key` doesn't find a match. This allows for fallback to older caches (e.g., if a lock file changes, but a similar dependency set might still be cached)."
    },
    {
        "instruction": "How to set up Slack notifications in GitLab CI/CD?",
        "input": "Notify on pipeline success and failure.",
        "output": "To set up Slack notifications in GitLab CI/CD, you typically use a webhook. GitLab provides built-in integration for Slack, or you can use `curl` to send messages to a Slack incoming webhook.\n\n**1. Using GitLab's built-in Slack integration (Recommended):**\n   *   Go to your project settings: `Settings > Integrations > Slack notifications`.\n   *   Activate the integration, paste your Slack Incoming Webhook URL (which you get from Slack's app directory or custom integrations).\n   *   Select the events you want to be notified about (e.g., pipeline success, failure, fixed, etc.).\n   *   (Optional) Customize the notification settings, including branch filters.\n\n**2. Using `curl` in your `.gitlab-ci.yml` (for more custom messages):**\n   This method gives you more control over the message content.\n\n   ```yaml\n   stages:\n     - build\n     - deploy\n     - notify\n\n   variables:\n     SLACK_WEBHOOK_URL: $SLACK_WEBHOOK_URL # Set this as a CI/CD variable in GitLab\n\n   build_job:\n     stage: build\n     script:\n       - echo \"Building the app...\"\n       # Your build commands\n     allow_failure: false\n\n   deploy_job:\n     stage: deploy\n     script:\n       - echo \"Deploying the app...\"\n       # Your deploy commands\n     allow_failure: false\n\n   # Notification on pipeline success\n   slack_success_notification:\n     stage: notify\n     image: curlimages/curl:latest # Or any image with curl\n     script:\n       - |\n         curl -X POST -H 'Content-type: application/json' --data '{\n           \"text\": \":white_check_mark: Pipeline *${CI_PROJECT_NAME}* (${CI_COMMIT_REF_NAME}) succeeded!\\n<${CI_PIPELINE_URL}|View Pipeline>\"\n         }' \"$SLACK_WEBHOOK_URL\"\n     when: on_success # Only run on success\n     allow_failure: true # Don't fail the pipeline if notification fails\n     needs: [] # Run after all previous stages complete, but not dependent on their success for its own execution\n\n   # Notification on pipeline failure\n   slack_failure_notification:\n     stage: notify\n     image: curlimages/curl:latest\n     script:\n       - |\n         curl -X POST -H 'Content-type: application/json' --data '{\n           \"text\": \":x: Pipeline *${CI_PROJECT_NAME}* (${CI_COMMIT_REF_NAME}) *FAILED*!\\n<${CI_PIPELINE_URL}|View Pipeline>\"\n         }' \"$SLACK_WEBHOOK_URL\"\n     when: on_failure # Only run on failure\n     allow_failure: true\n     needs: []\n   ```\n\n   **Important:** Store your `SLACK_WEBHOOK_URL` as a protected CI/CD variable in GitLab (`Settings > CI/CD > Variables`) to keep it secure."
    },
    {
        "instruction": "Integrate SonarQube analysis into a GitLab CI/CD pipeline.",
        "input": "Java Maven project, use SonarScanner CLI, fail pipeline if quality gate fails.",
        "output": "stages:\n  - build\n  - test\n  - sonarqube\n\nvariables:\n  SONAR_USER_HOME: \"${CI_PROJECT_DIR}/.sonar\"\n  GIT_DEPTH: \"0\"\n\ncache:\n  key: \"${CI_COMMIT_REF_SLUG}\"\n  paths:\n    - .sonar/cache\n\nbuild_and_test:\n  stage: build\n  image: maven:3.8.4-jdk-11\n  script:\n    - mvn clean verify # Build and run tests\n    - cp target/*.jar build/app.jar\n  artifacts:\n    paths:\n      - build/app.jar\n    expire_in: 1 week\n\nsonarqube_analysis:\n  stage: sonarqube\n  image: maven:3.8.4-jdk-11\n  script:\n    - >\n      mvn sonar:sonar \\\n        -Dsonar.projectKey=${CI_PROJECT_NAME} \\\n        -Dsonar.host.url=${SONAR_HOST_URL} \\\n        -Dsonar.token=${SONAR_TOKEN} \\\n        -Dsonar.qualitygate.wait=true \\\n        -Dsonar.qualitygate.timeout=300 # Wait up to 5 minutes for quality gate\n\n  # This job will fail if the quality gate does not pass within the timeout\n  # and the 'sonar.qualitygate.wait=true' property is set.\n  needs:\n    - build_and_test\n  # Only run SonarQube for main branch and merge requests\n  only:\n    - main\n    - merge_requests\n\n  # Define SONAR_HOST_URL and SONAR_TOKEN as CI/CD variables in GitLab\n  # SONAR_TOKEN should be a protected variable"
    },
    {
        "instruction": "How to create a CI/CD pipeline for a monorepo in GitLab CI?",
        "input": "Only run specific jobs if changes are detected in certain subdirectories.",
        "output": "For monorepos, you want to avoid running all jobs for every commit, especially if changes are isolated to a specific service or application within the repo. GitLab CI's `rules:changes` keyword is perfect for this.\n\n```yaml\n# .gitlab-ci.yml\n\nstages:\n  - lint\n  - test\n  - build\n  - deploy\n\n# --- Service A (e.g., Node.js App) ---\n\n.service_a_template:\n  image: node:16-alpine\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}-service-a\n    paths:\n      - service-a/node_modules/\n  before_script:\n    - cd service-a\n    - npm ci\n  variables:\n    # Define variables specific to Service A here\n    SERVICE_A_IMAGE: ${CI_REGISTRY_IMAGE}/service-a:${CI_COMMIT_SHORT_SHA}\n\nservice_a_lint:\n  stage: lint\n  extends: .service_a_template\n  script:\n    - npm run lint\n  rules:\n    - changes: # Only run if files in service-a/ or common/ changed\n        - service-a/**/*\n        - common/**/*\n\nservice_a_test:\n  stage: test\n  extends: .service_a_template\n  script:\n    - npm test\n  rules:\n    - changes:\n        - service-a/**/*\n        - common/**/*\n  needs: [service_a_lint]\n\nservice_a_build:\n  stage: build\n  image: docker:latest\n  services:\n    - docker:dind\n  script:\n    - cd service-a\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n    - docker build -t ${SERVICE_A_IMAGE} .\n    - docker push ${SERVICE_A_IMAGE}\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      changes:\n        - service-a/**/*\n        - common/**/*\n  needs: [service_a_test]\n\nservice_a_deploy:\n  stage: deploy\n  image: alpine/k8s:1.24 # Or your kubectl image\n  script:\n    - echo \"Deploying Service A to K8s...\"\n    - kubectl set image deployment/service-a-deployment service-a=${SERVICE_A_IMAGE} -n service-a-namespace\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      changes:\n        - service-a/**/*\n        - kubernetes/service-a/**/*\n  needs: [service_a_build]\n\n# --- Service B (e.g., Python API) ---\n\n.service_b_template:\n  image: python:3.9-slim-buster\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}-service-b\n    paths:\n      - service-b/venv/\n  before_script:\n    - cd service-b\n    - python -m venv venv\n    - . venv/bin/activate\n    - pip install -r requirements.txt\n  variables:\n    SERVICE_B_IMAGE: ${CI_REGISTRY_IMAGE}/service-b:${CI_COMMIT_SHORT_SHA}\n\nservice_b_test:\n  stage: test\n  extends: .service_b_template\n  script:\n    - pytest\n  rules:\n    - changes:\n        - service-b/**/*\n        - common/**/*\n\nservice_b_build:\n  stage: build\n  image: docker:latest\n  services:\n    - docker:dind\n  script:\n    - cd service-b\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n    - docker build -t ${SERVICE_B_IMAGE} .\n    - docker push ${SERVICE_B_IMAGE}\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      changes:\n        - service-b/**/*\n        - common/**/*\n  needs: [service_b_test]\n\nservice_b_deploy:\n  stage: deploy\n  image: alpine/k8s:1.24\n  script:\n    - echo \"Deploying Service B to K8s...\"\n    - kubectl set image deployment/service-b-deployment service-b=${SERVICE_B_IMAGE} -n service-b-namespace\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      changes:\n        - service-b/**/*\n        - kubernetes/service-b/**/*\n  needs: [service_b_build]\n```\n\n**Key Concepts:**\n*   **`rules:changes`**: This is the core mechanism. A job with `rules:changes` will only run if any files matching the specified glob patterns (`**/*`) in the `changes` list have been modified in the current commit or merge request.\n*   **`.templates` (hidden jobs)**: Used to define common configurations for jobs related to a specific service, reducing duplication.\n*   **`needs`**: Ensures jobs run in the correct order and only if their predecessors succeed.\n*   **`CI_COMMIT_REF_SLUG`**: A built-in variable useful for caching keys, unique per branch or tag.\n*   **`$CI_REGISTRY_IMAGE`**: Built-in variable for the project's container registry path.\n*   **Conditional Deployments**: Use `rules:if` along with `$CI_COMMIT_BRANCH` to ensure deployments only happen on the `main` branch (or similar).\n\nThis setup ensures that a change in `service-a` won't trigger builds/tests/deploys for `service-b`, making the monorepo pipeline efficient."
    },
    {
        "instruction": "Create a Terraform configuration for an AWS S3 bucket with versioning and public access blocked.",
        "input": "Bucket name: my-secure-app-logs",
        "output": "resource \"aws_s3_bucket\" \"logs_bucket\" {\n  bucket = \"my-secure-app-logs\"\n\n  tags = {\n    Environment = \"production\"\n    ManagedBy   = \"Terraform\"\n  }\n}\n\n# Enable versioning for the bucket\nresource \"aws_s3_bucket_versioning\" \"logs_bucket_versioning\" {\n  bucket = aws_s3_bucket.logs_bucket.id\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\n# Block all public access for the bucket\nresource \"aws_s3_bucket_public_access_block\" \"logs_bucket_public_access_block\" {\n  bucket = aws_s3_bucket.logs_bucket.id\n\n  block_public_acls       = true\n  block_public_policy     = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n}"
    },
    {
        "instruction": "Create a Terraform configuration for an Azure Resource Group and a Linux Virtual Machine.",
        "input": "Resource group name: my-rg, VM name: my-linux-vm, size: Standard_B1s, Ubuntu image.",
        "output": "provider \"azurerm\" {\n  features {}\n}\n\nresource \"azurerm_resource_group\" \"rg\" {\n  name     = \"my-rg\"\n  location = \"East US\"\n}\n\nresource \"azurerm_virtual_network\" \"vnet\" {\n  name                = \"my-vnet\"\n  address_space       = [\"10.0.0.0/16\"]\n  location            = azurerm_resource_group.rg.location\n  resource_group_name = azurerm_resource_group.rg.name\n}\n\nresource \"azurerm_subnet\" \"subnet\" {\n  name                 = \"my-subnet\"\n  resource_group_name  = azurerm_resource_group.rg.name\n  virtual_network_name = azurerm_virtual_network.vnet.name\n  address_prefixes     = [\"10.0.1.0/24\"]\n}\n\nresource \"azurerm_network_interface\" \"nic\" {\n  name                = \"my-linux-vm-nic\"\n  location            = azurerm_resource_group.rg.location\n  resource_group_name = azurerm_resource_group.rg.name\n\n  ip_configuration {\n    name                          = \"internal\"\n    subnet_id                     = azurerm_subnet.subnet.id\n    private_ip_address_allocation = \"Dynamic\"\n    public_ip_address_id          = azurerm_public_ip.public_ip.id\n  }\n}\n\nresource \"azurerm_public_ip\" \"public_ip\" {\n  name                = \"my-linux-vm-public-ip\"\n  location            = azurerm_resource_group.rg.location\n  resource_group_name = azurerm_resource_group.rg.name\n  allocation_method   = \"Dynamic\"\n}\n\nresource \"azurerm_linux_virtual_machine\" \"vm\" {\n  name                = \"my-linux-vm\"\n  resource_group_name = azurerm_resource_group.rg.name\n  location            = azurerm_resource_group.rg.location\n  size                = \"Standard_B1s\"\n  admin_username      = \"azureuser\"\n  network_interface_ids = [\n    azurerm_network_interface.nic.id,\n  ]\n\n  admin_ssh_key {\n    username   = \"azureuser\"\n    public_key = file(\"~/.ssh/id_rsa.pub\") # Ensure this path is correct or replace with your public key\n  }\n\n  os_disk {\n    caching              = \"ReadWrite\"\n    storage_account_type = \"Standard_LRS\"\n  }\n\n  source_image_reference {\n    publisher = \"Canonical\"\n    offer     = \"UbuntuServer\"\n    sku       = \"18.04-LTS\"\n    version   = \"latest\"\n  }\n}"
    },
    {
        "instruction": "Create a Terraform configuration for a GCP Cloud Storage bucket.",
        "input": "Bucket name: my-gcp-app-data, region: us-central1, standard storage class.",
        "output": "provider \"google\" {\n  project = \"your-gcp-project-id\"\n  region  = \"us-central1\"\n}\n\nresource \"google_storage_bucket\" \"data_bucket\" {\n  name          = \"my-gcp-app-data\"\n  location      = \"US-CENTRAL1\"\n  storage_class = \"STANDARD\"\n  force_destroy = false # Set to true carefully for development/testing\n\n  uniform_bucket_level_access = true\n\n  versioning {\n    enabled = true\n  }\n\n  labels = {\n    environment = \"production\"\n    managedby   = \"terraform\"\n  }\n\n  # Optional: Lifecycle rules\n  lifecycle_rule {\n    action {\n      type = \"Delete\"\n    }\n    condition {\n      age = 365 # Delete objects older than 365 days\n    }\n  }\n\n  lifecycle_rule {\n    action {\n      type          = \"SetStorageClass\"\n      storage_class = \"NEARLINE\"\n    }\n    condition {\n      age = 30 # Move objects to Nearline after 30 days\n    }\n  }\n}"
    },
    {
        "instruction": "How to manage Terraform state remotely using an S3 backend?",
        "input": "",
        "output": "Managing Terraform state remotely is crucial for collaboration and reliability. An S3 backend (for AWS) is a common and robust choice.\n\n**Steps:**\n\n1.  **Create an S3 Bucket:**\n    You need an S3 bucket to store your Terraform state files. It's best practice to enable versioning on this bucket to keep a history of your state changes.\n    ```bash\n    aws s3 mb s3://my-terraform-state-bucket-12345\n    aws s3api put-bucket-versioning --bucket my-terraform-state-bucket-12345 --versioning-configuration '{\"Status\": \"Enabled\"}'\n    ```\n\n2.  **Create a DynamoDB Table (for state locking):**\n    To prevent concurrent Terraform runs from corrupting your state file, use a DynamoDB table for state locking. The table must have a primary key named `LockID` of type String.\n    ```bash\n    aws dynamodb create-table \\\n        --table-name my-terraform-state-lock \\\n        --attribute-definitions AttributeName=LockID,AttributeType=S \\\n        --key-schema AttributeName=LockID,KeyType=HASH \\\n        --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5\n    ```\n\n3.  **Configure `backend` in `main.tf` (or a separate `backend.tf`):**\n    Add the `backend` block to your Terraform configuration. This block *must* be in the root of your configuration and cannot use variables.\n\n    ```terraform\n    terraform {\n      backend \"s3\" {\n        bucket         = \"my-terraform-state-bucket-12345\"\n        key            = \"path/to/my/state/terraform.tfstate\"\n        region         = \"us-east-1\"\n        encrypt        = true # Encrypt state file at rest\n        dynamodb_table = \"my-terraform-state-lock\"\n      }\n    }\n\n    # ... rest of your Terraform configuration ...\n    ```\n\n4.  **Initialize Terraform:**\n    When you first run `terraform init` with the `backend` configured, Terraform will detect the remote backend and offer to copy your local state to S3 (if you have existing state) or create a new empty state.\n    ```bash\n    terraform init\n    ```\n\nNow, all subsequent `terraform plan`, `apply`, and `destroy` commands will automatically use the S3 bucket for state storage and the DynamoDB table for locking."
    },
    {
        "instruction": "Explain Terraform Workspaces and when to use them.",
        "input": "",
        "output": "Terraform Workspaces allow you to manage multiple distinct states for a single Terraform configuration. They are essentially named pointers to different state files for the same infrastructure code. This means you can use the same set of `.tf` files to deploy and manage environments like `dev`, `staging`, and `production`.\n\n**Key Commands:**\n*   `terraform workspace new <name>`: Creates a new workspace and switches to it.\n*   `terraform workspace list`: Lists all existing workspaces.\n*   `terraform workspace select <name>`: Switches to an existing workspace.\n*   `terraform workspace show`: Shows the name of the current workspace.\n*   `terraform workspace delete <name>`: Deletes a workspace (only if it's empty).\n\n**When to Use Workspaces:**\n1.  **Multiple Environments from a Single Configuration:** This is the primary use case. You can have a `dev` workspace, `staging` workspace, and `production` workspace, each with its own state file, but all managed by the same `.tf` files.\n    *   Example: `terraform workspace new dev`, `terraform apply -var 'env=dev'`\n    *   `terraform workspace select prod`, `terraform apply -var 'env=prod'`\n\n2.  **Feature Branches (Less Common, Prefer Separate Directories):** Some teams might use workspaces for deploying feature branches to temporary environments. However, creating separate directories for environments is often cleaner for complex scenarios.\n\n**When NOT to Use Workspaces:**\n*   **Managing different logical components:** If you have entirely separate parts of your infrastructure (e.g., a VPC vs. an EKS cluster), it's better to use separate root Terraform configurations in different directories/repositories, perhaps combined with Terraform modules.\n*   **Complex or divergent environments:** If your `dev` and `production` environments become significantly different (e.g., completely different resource types or very different variable values), managing them through a single configuration with workspaces can become unwieldy. In such cases, breaking them into separate root modules is often better.\n\n**Important Note:** When using workspaces, it's common to pass in variables to differentiate configurations between environments (e.g., `instance_type`, `database_size`). The `terraform.workspace` variable can also be used directly within your configuration."
    },
    {
        "instruction": "Write an Ansible Playbook to install Nginx and deploy a simple static HTML page.",
        "input": "Target host: webserver, HTML content: '<h1>Welcome to Ansible!</h1>'",
        "output": "```yaml\n---\n- name: Deploy Nginx and Static Website\n  hosts: webserver\n  become: true # Run tasks with sudo/root privileges\n  tasks:\n    - name: Ensure Nginx is installed\n      ansible.builtin.apt:\n        name: nginx\n        state: present\n        update_cache: yes\n\n    - name: Ensure Nginx service is running and enabled\n      ansible.builtin.service:\n        name: nginx\n        state: started\n        enabled: yes\n\n    - name: Remove default Nginx site configuration\n      ansible.builtin.file:\n        path: /etc/nginx/sites-enabled/default\n        state: absent\n      notify: Reload Nginx\n\n    - name: Copy custom Nginx configuration for static site\n      ansible.builtin.copy:\n        content: |\n          server {\n              listen 80 default_server;\n              listen [::]:80 default_server;\n\n              root /var/www/html;\n              index index.html index.htm;\n\n              server_name _;\n\n              location / {\n                  try_files $uri $uri/ =404;\n              }\n          }\n        dest: /etc/nginx/sites-available/default.conf\n        owner: root\n        group: root\n        mode: '0644'\n      notify: Reload Nginx\n\n    - name: Enable custom Nginx site configuration\n      ansible.builtin.file:\n        src: /etc/nginx/sites-available/default.conf\n        dest: /etc/nginx/sites-enabled/default.conf\n        state: link\n      notify: Reload Nginx\n\n    - name: Deploy index.html\n      ansible.builtin.copy:\n        content: '<h1>Welcome to Ansible!</h1>'\n        dest: /var/www/html/index.html\n        owner: www-data\n        group: www-data\n        mode: '0644'\n\n  handlers:\n    - name: Reload Nginx\n      ansible.builtin.service:\n        name: nginx\n        state: reloaded\n```\n\n**To run this playbook:**\n1.  Save it as `nginx_deploy.yml`.\n2.  Create an `inventory.ini` file:\n    ```ini\n    [webserver]\n    your_server_ip_or_hostname ansible_user=your_ssh_user\n    ```\n3.  Run the playbook:\n    `ansible-playbook -i inventory.ini nginx_deploy.yml`\n\nReplace `your_server_ip_or_hostname` and `your_ssh_user` with your actual server details."
    },
    {
        "instruction": "Create an AWS CloudFormation template for a simple Lambda function.",
        "input": "Python 3.9 runtime, triggered by API Gateway, function code in a zip file.",
        "output": "AWSTemplateFormatVersion: '2010-09-09'\nDescription: A simple Lambda function triggered by API Gateway.\n\nParameters:\n  FunctionName:\n    Type: String\n    Default: MySimpleLambdaFunction\n    Description: Name for the Lambda function.\n  HandlerName:\n    Type: String\n    Default: index.handler\n    Description: The function entrypoint.\n  S3BucketName:\n    Type: String\n    Description: S3 bucket name containing the Lambda deployment package.\n  S3Key:\n    Type: String\n    Default: function.zip\n    Description: S3 key for the Lambda deployment package.\n\nResources:\n  LambdaExecutionRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: '2012-10-17'\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service: lambda.amazonaws.com\n            Action: sts:AssumeRole\n      ManagedPolicyArns:\n        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n      Path: /\n\n  MyLambdaFunction:\n    Type: AWS::Lambda::Function\n    Properties:\n      FunctionName: !Ref FunctionName\n      Handler: !Ref HandlerName\n      Runtime: python3.9\n      Role: !GetAtt LambdaExecutionRole.Arn\n      Code:\n        S3Bucket: !Ref S3BucketName\n        S3Key: !Ref S3Key\n      Timeout: 30 # seconds\n      MemorySize: 128 # MB\n\n  ApiGatewayRestApi:\n    Type: AWS::ApiGateway::RestApi\n    Properties:\n      Name: !Join ['-', [!Ref FunctionName, 'API']]\n\n  ApiGatewayResource:\n    Type: AWS::ApiGateway::Resource\n    Properties:\n      ParentId: !GetAtt ApiGatewayRestApi.RootResourceId\n      PathPart: '{proxy+}' # Catch-all path for flexibility\n      RestApiId: !Ref ApiGatewayRestApi\n\n  ApiGatewayMethod:\n    Type: AWS::ApiGateway::Method\n    Properties:\n      HttpMethod: ANY # Allows GET, POST, PUT, DELETE, etc.\n      ResourceId: !Ref ApiGatewayResource\n      RestApiId: !Ref ApiGatewayRestApi\n      AuthorizationType: NONE\n      Integration:\n        IntegrationHttpMethod: POST # Lambda receives POST from API Gateway\n        Type: AWS_PROXY\n        Uri: !Sub 'arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${MyLambdaFunction.Arn}/invocations'\n\n  LambdaPermissionApiGateway:\n    Type: AWS::Lambda::Permission\n    Properties:\n      Action: lambda:InvokeFunction\n      FunctionName: !GetAtt MyLambdaFunction.Arn\n      Principal: apigateway.amazonaws.com\n      SourceArn: !Sub 'arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:${ApiGatewayRestApi.Arn}/*/*'\n\n  ApiGatewayDeployment:\n    Type: AWS::ApiGateway::Deployment\n    DependsOn:\n      - ApiGatewayMethod\n    Properties:\n      RestApiId: !Ref ApiGatewayRestApi\n      Description: Initial deployment\n\n  ApiGatewayStage:\n    Type: AWS::ApiGateway::Stage\n    Properties:\n      DeploymentId: !Ref ApiGatewayDeployment\n      RestApiId: !Ref ApiGatewayRestApi\n      StageName: Prod\n\nOutputs:\n  LambdaFunctionName:\n    Description: The name of the Lambda function.\n    Value: !Ref MyLambdaFunction\n  ApiGatewayEndpoint:\n    Description: The invoke URL for the API Gateway endpoint.\n    Value: !Sub 'https://${ApiGatewayRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/'\n  LambdaExecutionRoleArn:\n    Description: ARN of the Lambda execution role.\n    Value: !GetAtt LambdaExecutionRole.Arn"
    },
    {
        "instruction": "Create a Terraform module for an AWS EC2 instance.",
        "input": "Includes instance type, AMI, security group, and key pair. Provide example usage.",
        "output": "```terraform\n# modules/ec2_instance/main.tf\n\nresource \"aws_instance\" \"app_server\" {\n  ami           = var.ami_id\n  instance_type = var.instance_type\n  key_name      = var.key_pair_name\n  subnet_id     = var.subnet_id\n\n  vpc_security_group_ids = [aws_security_group.app_sg.id]\n\n  tags = {\n    Name        = \"${var.name_prefix}-server\"\n    Environment = var.environment\n    ManagedBy   = \"Terraform\"\n  }\n}\n\nresource \"aws_security_group\" \"app_sg\" {\n  name_prefix = \"${var.name_prefix}-sg-\"\n  vpc_id      = var.vpc_id\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = var.ssh_ingress_cidrs\n  }\n\n  ingress {\n    from_port   = var.app_port\n    to_port     = var.app_port\n    protocol    = \"tcp\"\n    cidr_blocks = var.app_ingress_cidrs\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name        = \"${var.name_prefix}-sg\"\n    Environment = var.environment\n  }\n}\n\n# modules/ec2_instance/variables.tf\n\nvariable \"name_prefix\" {\n  description = \"Prefix for resource names.\"\n  type        = string\n}\n\nvariable \"ami_id\" {\n  description = \"The AMI ID for the EC2 instance.\"\n  type        = string\n}\n\nvariable \"instance_type\" {\n  description = \"The type of EC2 instance to launch.\"\n  type        = string\n  default     = \"t3.micro\"\n}\n\nvariable \"key_pair_name\" {\n  description = \"The name of the EC2 Key Pair to use.\"\n  type        = string\n}\n\nvariable \"subnet_id\" {\n  description = \"The ID of the subnet to launch the instance into.\"\n  type        = string\n}\n\nvariable \"vpc_id\" {\n  description = \"The ID of the VPC where resources will be created.\"\n  type        = string\n}\n\nvariable \"ssh_ingress_cidrs\" {\n  description = \"List of CIDR blocks allowed to SSH into the instance.\"\n  type        = list(string)\n  default     = [\"0.0.0.0/0\"]\n}\n\nvariable \"app_port\" {\n  description = \"The application port to open on the security group.\"\n  type        = number\n  default     = 80\n}\n\nvariable \"app_ingress_cidrs\" {\n  description = \"List of CIDR blocks allowed to access the application port.\"\n  type        = list(string)\n  default     = [\"0.0.0.0/0\"]\n}\n\nvariable \"environment\" {\n  description = \"Environment tag for resources.\"\n  type        = string\n  default     = \"dev\"\n}\n\n# modules/ec2_instance/outputs.tf\n\noutput \"instance_id\" {\n  description = \"The ID of the EC2 instance.\"\n  value       = aws_instance.app_server.id\n}\n\noutput \"public_ip\" {\n  description = \"The public IP address of the EC2 instance.\"\n  value       = aws_instance.app_server.public_ip\n}\n\noutput \"private_ip\" {\n  description = \"The private IP address of the EC2 instance.\"\n  value       = aws_instance.app_server.private_ip\n}\n\noutput \"security_group_id\" {\n  description = \"The ID of the security group created for the instance.\"\n  value       = aws_security_group.app_sg.id\n}\n\n\n```\n\n**Example Usage of the Module (in your root `main.tf`):**\n\n```terraform\n# main.tf (in your root project directory)\n\nprovider \"aws\" {\n  region = \"us-east-1\"\n}\n\n# Look up a public AMI (e.g., latest Ubuntu 20.04 LTS)\ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\"]\n  }\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n  owners = [\"099720109477\"] # Canonical\n}\n\n# Example VPC and Subnet (replace with your existing VPC/Subnet IDs or create them)\nresource \"aws_vpc\" \"example_vpc\" {\n  cidr_block = \"10.0.0.0/16\"\n  enable_dns_hostnames = true\n  tags = { Name = \"example-vpc\" }\n}\n\nresource \"aws_subnet\" \"example_subnet\" {\n  vpc_id            = aws_vpc.example_vpc.id\n  cidr_block        = \"10.0.1.0/24\"\n  availability_zone = \"us-east-1a\"\n  map_public_ip_on_launch = true\n  tags = { Name = \"example-public-subnet\" }\n}\n\n# Create a key pair (if you don't have one)\nresource \"tls_private_key\" \"pk\" {\n  algorithm = \"RSA\"\n  rsa_bits  = 4096\n}\n\nresource \"aws_key_pair\" \"generated_key\" {\n  key_name   = \"my-terraform-key\"\n  public_key = tls_private_key.pk.public_key_openssh\n}\n\nresource \"local_file\" \"ssh_key\" {\n  content  = tls_private_key.pk.private_key_pem\n  filename = \"./my-terraform-key.pem\"\n  file_permission = \"0400\"\n}\n\n# Use the EC2 instance module\nmodule \"web_server\" {\n  source = \"./modules/ec2_instance\"\n\n  name_prefix     = \"web-app\"\n  ami_id          = data.aws_ami.ubuntu.id\n  instance_type   = \"t3.medium\"\n  key_pair_name   = aws_key_pair.generated_key.key_name\n  subnet_id       = aws_subnet.example_subnet.id\n  vpc_id          = aws_vpc.example_vpc.id\n  ssh_ingress_cidrs = [\"0.0.0.0/0\"] # BE CAREFUL: This allows SSH from anywhere!\n  app_port          = 80\n  app_ingress_cidrs = [\"0.0.0.0/0\"]\n  environment       = \"production\"\n}\n\noutput \"web_server_public_ip\" {\n  value = module.web_server.public_ip\n}\n\noutput \"web_server_instance_id\" {\n  value = module.web_server.instance_id\n}\n\n```"
    },
    {
        "instruction": "Explain the common Terraform commands: `init`, `plan`, `apply`, `destroy`.",
        "input": "",
        "output": "These four commands are the fundamental lifecycle commands for working with Terraform:\n\n1.  **`terraform init`**\n    *   **Purpose:** Initializes a Terraform working directory. This command is the first one you should run in a new or cloned Terraform project.\n    *   **What it does:**\n        *   Downloads and installs the necessary provider plugins (e.g., `aws`, `azurerm`, `google`) required by your configuration.\n        *   Initializes backends for state management (e.g., S3, Azure Blob Storage), configuring where Terraform will store your state file.\n        *   Discovers and loads any modules used in your configuration.\n    *   **When to run:** Every time you start working in a new Terraform directory, or after pulling changes that modify provider versions, backend configuration, or add new modules.\n\n2.  **`terraform plan`**\n    *   **Purpose:** Generates an execution plan, showing you exactly what Terraform will do to achieve the desired state defined in your configuration.\n    *   **What it does:**\n        *   Reads the current state of your infrastructure (from the state file or by querying the cloud provider).\n        *   Compares it with the desired state defined in your `.tf` files.\n        *   Determines what actions (create, update, replace, destroy) are needed to reach the desired state.\n        *   Outputs a detailed list of changes.\n    *   **When to run:** Always run `terraform plan` before `terraform apply` to review the proposed changes and ensure they align with your expectations. It's a 'dry run' that helps prevent unintended modifications.\n\n3.  **`terraform apply`**\n    *   **Purpose:** Executes the actions proposed in a Terraform plan (or generates a plan and applies it if no explicit plan file is provided).\n    *   **What it does:**\n        *   Prompts for confirmation (unless `-auto-approve` is used). After confirmation, it performs the planned actions on your infrastructure.\n        *   Updates the Terraform state file to reflect the new actual state of your infrastructure.\n    *   **When to run:** When you are ready to provision or modify your infrastructure according to your Terraform configuration. Always review the plan first!\n\n4.  **`terraform destroy`**\n    *   **Purpose:** Destroys all infrastructure resources managed by the current Terraform configuration.\n    *   **What it does:**\n        *   Reads the current state and generates a plan to destroy all resources defined in that state.\n        *   Prompts for confirmation (unless `-auto-approve` is used).\n        *   Deletes the resources from your cloud provider.\n        *   Clears the Terraform state file.\n    *   **When to run:** Use with extreme caution! Typically used for tearing down development/test environments or for cleaning up resources after a project is finished. Never use it on production environments without explicit and careful planning."
    },
    {
        "instruction": "What are the best practices for Infrastructure as Code (IaC)?",
        "input": "",
        "output": "Infrastructure as Code (IaC) is about managing and provisioning infrastructure through code instead of manual processes. Best practices ensure maintainability, reliability, and scalability:\n\n1.  **Version Control Everything:** Store all IaC definitions (Terraform, CloudFormation, Ansible playbooks) in a version control system (Git) alongside your application code. This enables history, collaboration, and rollback.\n2.  **Idempotency:** IaC scripts should be idempotent, meaning running them multiple times yields the same result without unintended side effects. They should achieve the desired state regardless of the initial state.\n3.  **Modularity and Reusability:** Break down your infrastructure into reusable modules (Terraform modules, Ansible roles, CloudFormation Nested Stacks). This reduces duplication and promotes consistency.\n4.  **Use Parameters/Variables:** Avoid hardcoding values. Use variables, parameters, or input files to make your configurations flexible and environment-agnostic.\n5.  **Small, Incremental Changes:** Apply changes in small, manageable chunks. This reduces the blast radius of potential issues and makes troubleshooting easier.\n6.  **Automated Testing:** Implement tests for your infrastructure code (e.g., `terraform validate`, `terraform fmt --check`, `terratest`, `Serverspec`). This catches errors early.\n7.  **Environments as Code:** Define separate environments (dev, staging, prod) using the same IaC codebase, parameterized for differences. This ensures environment parity.\n8.  **Secrets Management:** Never hardcode sensitive information (API keys, passwords) in your IaC files. Use dedicated secrets management solutions (e.g., Vault, AWS Secrets Manager, Azure Key Vault).\n9.  **Drift Detection:** Monitor for 'drift'  discrepancies between your IaC definition and the actual deployed infrastructure. Tools like `terraform plan` on a schedule can help.\n10. **Immutable Infrastructure:** Prefer building new infrastructure with every change rather than modifying existing components in place. This reduces configuration drift and makes rollbacks simpler.\n11. **Automated Pipelines:** Integrate IaC into your CI/CD pipelines to automatically provision and update infrastructure. This ensures consistency and speed.\n12. **Document Your Code:** Even though code is documentation, provide comments and READMEs for complex configurations or design decisions.\n13. **Least Privilege:** When defining IAM roles or service accounts for your IaC tools, grant only the minimum necessary permissions.\n14. **State Management:** Use remote state management (e.g., Terraform Cloud, S3/DynamoDB for Terraform) to enable collaboration, provide locking, and secure your state file."
    },
    {
        "instruction": "How to deploy a static website to AWS S3 and CloudFront?",
        "input": "",
        "output": "Deploying a static website to AWS S3 and CloudFront is a common, cost-effective, and highly scalable solution. S3 stores your files, and CloudFront acts as a CDN for performance and security.\n\n**Steps:**\n\n1.  **Create an S3 Bucket:**\n    *   Go to S3 console -> Create bucket.\n    *   **Bucket name:** Should match your domain name (e.g., `example.com`). If you use a subdomain (e.g., `www.example.com`), create a separate bucket for it.\n    *   **AWS Region:** Choose a region close to your users.\n    *   **Block Public Access settings:** You should initially block all public access. CloudFront will access S3 using an Origin Access Control (OAC), not public access.\n    *   **Enable Static Website Hosting:** (Optional but useful for direct S3 access during testing). Under 'Properties', enable static website hosting and set index and error documents (e.g., `index.html`, `404.html`). Note the endpoint URL.\n\n2.  **Upload Website Files:**\n    *   Upload your static HTML, CSS, JavaScript, images, etc., to the S3 bucket.\n\n3.  **Configure S3 Bucket Policy (for CloudFront OAC):**\n    *   Instead of making the bucket publicly accessible, you create an Origin Access Control (OAC) in CloudFront and then grant *only* that OAC permission to read from your S3 bucket via a bucket policy. This is the secure best practice.\n    *   (This policy will be generated when you create the CloudFront distribution and choose the S3 bucket as origin with OAC.)\n\n4.  **Create a CloudFront Distribution:**\n    *   Go to CloudFront console -> Create distribution.\n    *   **Origin domain:** Select your S3 bucket from the dropdown (it will show as `your-bucket-name.s3.amazonaws.com`).\n    *   **S3 bucket access:** Select 'Yes, use OAC (recommended)'. Click 'Create new OAC' and 'Create'.\n    *   **Bucket policy:** Click 'Copy policy' and then 'Go to S3 bucket permissions' to paste the generated policy into your S3 bucket policy editor. This grants CloudFront access.\n    *   **Viewer protocol policy:** Redirect HTTP to HTTPS (Recommended).\n    *   **Default root object:** `index.html` (or your main HTML file).\n    *   **Price class:** Choose based on your audience's geographic distribution.\n    *   **Alternate domain names (CNAMES):** Add your custom domain (e.g., `example.com`, `www.example.com`).\n    *   **SSL certificate:** Use a custom SSL certificate from AWS Certificate Manager (ACM) for your custom domain.\n    *   Create the distribution.\n\n5.  **Configure DNS (Route 53 or other DNS provider):**\n    *   Once the CloudFront distribution is deployed (can take 5-15 minutes), copy its `Domain Name` (e.g., `d12345abcdef.cloudfront.net`).\n    *   Go to your DNS provider (e.g., AWS Route 53).\n    *   Create an `A` record (for the root domain, `example.com`) and a `CNAME` record (for `www.example.com` or other subdomains) that point to the CloudFront distribution domain name. For Route 53, you can use an ALIAS record that points directly to the CloudFront distribution.\n\n6.  **Optional: Configure Routing for `www` (or other subdomains):**\n    *   If you created two S3 buckets (e.g., `example.com` and `www.example.com`), one for the root domain and one for `www`, configure the `www` bucket for static website hosting and set it to redirect all requests to `example.com` (your primary domain). Then, ensure your CloudFront distribution for `www.example.com` points to this redirection bucket. Alternatively, use a single S3 bucket and manage redirects/aliases via CloudFront CNAMEs and DNS.\n\nYour website should now be accessible via your custom domain, served securely and efficiently by CloudFront."
    },
    {
        "instruction": "Create an AWS Lambda function triggered by an S3 object creation event.",
        "input": "Function in Python 3.9, processes new images uploaded to 'my-image-uploads' bucket.",
        "output": "AWSTemplateFormatVersion: '2010-09-09'\nDescription: Lambda function triggered by S3 object creation to process images.\n\nParameters:\n  ImageUploadBucketName:\n    Type: String\n    Description: Name of the S3 bucket where images will be uploaded.\n\nResources:\n  # IAM Role for Lambda function\n  LambdaExecutionRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: '2012-10-17'\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service: lambda.amazonaws.com\n            Action: sts:AssumeRole\n      ManagedPolicyArns:\n        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole # Allows logging to CloudWatch\n      Policies:\n        - PolicyName: S3ReadAccess\n          PolicyDocument:\n            Version: '2012-10-17'\n            Statement:\n              - Effect: Allow\n                Action:\n                  - s3:GetObject\n                Resource: !Sub arn:aws:s3:::${ImageUploadBucketName}/*\n\n  # Lambda Function\n  ImageProcessorFunction:\n    Type: AWS::Lambda::Function\n    Properties:\n      Handler: index.handler\n      Runtime: python3.9\n      Code:\n        ZipFile: |\n          import json\n          import boto3\n          import os\n\n          s3_client = boto3.client('s3')\n\n          def handler(event, context):\n              print(\"Received event: \" + json.dumps(event))\n              for record in event['Records']:\n                  bucket_name = record['s3']['bucket']['name']\n                  object_key = record['s3']['object']['key']\n                  file_size = record['s3']['object']['size']\n\n                  print(f\"New object created in bucket '{bucket_name}': '{object_key}' with size {file_size} bytes\")\n\n                  try:\n                      # Example: Download and re-upload (e.g., resize, add watermark)\n                      # This is a placeholder for actual image processing logic\n                      print(f\"Processing image: {object_key}\")\n                      # obj = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n                      # image_data = obj['Body'].read()\n                      # s3_client.put_object(Bucket='my-processed-images', Key=f'processed/{object_key}', Body=image_data)\n                      print(f\"Successfully processed {object_key}\")\n                  except Exception as e:\n                      print(f\"Error processing {object_key}: {e}\")\n                      raise e\n\n              return {\n                  'statusCode': 200,\n                  'body': json.dumps('Processing complete!')\n              }\n\n      Role: !GetAtt LambdaExecutionRole.Arn\n      Timeout: 60 # seconds\n      MemorySize: 256 # MB\n\n  # S3 Bucket to trigger Lambda\n  ImageUploadBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: !Ref ImageUploadBucketName\n      NotificationConfiguration:\n        LambdaConfigurations:\n          - Event: s3:ObjectCreated:*\n            Function: !GetAtt ImageProcessorFunction.Arn\n\n  # Permission for S3 to invoke Lambda\n  LambdaInvokePermission:\n    Type: AWS::Lambda::Permission\n    Properties:\n      FunctionName: !GetAtt ImageProcessorFunction.Arn\n      Action: lambda:InvokeFunction\n      Principal: s3.amazonaws.com\n      SourceAccount: !Ref 'AWS::AccountId'\n      SourceArn: !GetAtt ImageUploadBucket.Arn\n\nOutputs:\n  LambdaFunctionName:\n    Description: The name of the Lambda function.\n    Value: !Ref ImageProcessorFunction\n  ImageUploadBucketArn:\n    Description: ARN of the S3 bucket configured for image uploads.\n    Value: !GetAtt ImageUploadBucket.Arn\n"
    },
    {
        "instruction": "Create an AWS RDS PostgreSQL instance using CloudFormation.",
        "input": "DB instance identifier: my-app-db, engine version: 13.7, instance type: db.t3.micro, master username/password, linked to existing VPC/subnet group.",
        "output": "AWSTemplateFormatVersion: '2010-09-09'\nDescription: Creates an AWS RDS PostgreSQL instance.\n\nParameters:\n  DBInstanceIdentifier:\n    Type: String\n    Default: my-app-db\n    Description: The ID for the DB instance.\n  DBEngineVersion:\n    Type: String\n    Default: 13.7\n    Description: The version of the PostgreSQL database engine.\n  DBInstanceClass:\n    Type: String\n    Default: db.t3.micro\n    Description: The compute and memory capacity of the DB instance.\n  MasterUsername:\n    Type: String\n    Description: Username for the master database user.\n  MasterUserPassword:\n    Type: String\n    Description: Password for the master database user. (Use AWS Secrets Manager in production!)\n    NoEcho: 'true'\n  VPCSecurityGroupIds:\n    Type: CommaDelimitedList\n    Description: Comma-separated list of VPC security group IDs to associate with the DB instance.\n  DBSubnetGroupName:\n    Type: String\n    Description: Name of the DB subnet group to associate with the DB instance.\n\nResources:\n  MyAppDBInstance:\n    Type: AWS::RDS::DBInstance\n    Properties:\n      AllocatedStorage: '20'\n      DBInstanceClass: !Ref DBInstanceClass\n      DBInstanceIdentifier: !Ref DBInstanceIdentifier\n      Engine: postgres\n      EngineVersion: !Ref DBEngineVersion\n      MasterUsername: !Ref MasterUsername\n      MasterUserPassword: !Ref MasterUserPassword\n      MultiAZ: 'false' # Set to 'true' for production high availability\n      PubliclyAccessible: 'false' # Set to 'false' for security best practices\n      VPCSecurityGroups: !Ref VPCSecurityGroupIds\n      DBSubnetGroupName: !Ref DBSubnetGroupName\n      BackupRetentionPeriod: 7 # Days\n      PreferredBackupWindow: '03:00-05:00'\n      PreferredMaintenanceWindow: 'Sun:06:00-Sun:07:00'\n      Tags:\n        - Key: Environment\n          Value: Production\n        - Key: Application\n          Value: MyApp\n        - Key: ManagedBy\n          Value: CloudFormation\n\nOutputs:\n  DBInstanceEndpoint:\n    Description: The endpoint address of the RDS instance.\n    Value: !GetAtt MyAppDBInstance.Endpoint.Address\n  DBInstancePort:\n    Description: The port of the RDS instance.\n    Value: !GetAtt MyAppDBInstance.Endpoint.Port\n  DBInstanceReadEndpoint:\n    Description: The read endpoint of the RDS instance (if Multi-AZ and Read Replica enabled).\n    Value: !If\n      - HasMultiAZ\n      - !GetAtt MyAppDBInstance.ReadReplicaSourceDBInstanceIdentifier\n      - !Ref 'AWS::NoValue'\n  DBInstanceARN:\n    Description: ARN of the RDS instance.\n    Value: !Ref MyAppDBInstance"
    },
    {
        "instruction": "Launch an EC2 instance using the AWS CLI with user data script.",
        "input": "Ubuntu 22.04 LTS AMI, t2.micro, install Nginx and start it.",
        "output": "First, find an appropriate AMI ID for Ubuntu 22.04 LTS in your desired region. You can find it on the AWS console or using:\n`aws ec2 describe-images --owners 099720109477 --filters 'Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*' 'Name=virtualization-type,Values=hvm' --query 'Images[0].ImageId' --output text`\n\nThen, use the `aws ec2 run-instances` command with `--user-data`:\n\n```bash\naws ec2 run-instances \\\n    --image-id ami-xxxxxxxxxxxxxxxxx \\ # Replace with the actual Ubuntu 22.04 AMI ID for your region\n    --instance-type t2.micro \\\n    --key-name your-key-pair-name \\\n    --security-group-ids sg-xxxxxxxxxxxxxxxxx \\ # Replace with your security group ID that allows inbound HTTP (port 80) and SSH (port 22) traffic\n    --subnet-id subnet-xxxxxxxxxxxxxxxxx \\ # Replace with your subnet ID\n    --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=MyNginxWebserver}]' \\\n    --user-data file://nginx_install.sh\n```\n\n**`nginx_install.sh` content (make sure this file is in the same directory):**\n```bash\n#!/bin/bash\n\n# Update package lists\nsudo apt update -y\n\n# Install Nginx\nsudo apt install nginx -y\n\n# Start Nginx service\nsudo systemctl start nginx\nsudo systemctl enable nginx\n\n# Optional: Deploy a simple index.html\nsudo echo \"<h1>Hello from EC2 Nginx!</h1>\" | sudo tee /var/www/html/index.html\n```"
    },
    {
        "instruction": "Create an IAM Role for an EC2 instance that allows it to read from S3.",
        "input": "Role name: my-ec2-s3-reader-role, policy: read-only access to 'my-data-bucket'.",
        "output": "First, create a trust policy JSON file (e.g., `trust-policy.json`) that allows EC2 instances to assume this role:\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"ec2.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n```\n\nThen, create the role:\n`aws iam create-role --role-name my-ec2-s3-reader-role --assume-role-policy-document file://trust-policy.json`\n\nNext, create an inline policy JSON file (e.g., `s3-read-policy.json`) for S3 read access:\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::my-data-bucket\",\n        \"arn:aws:s3:::my-data-bucket/*\"\n      ]\n    }\n  ]\n}\n```\n\nAttach the inline policy to the role:\n`aws iam put-role-policy --role-name my-ec2-s3-reader-role --policy-name S3ReadOnlyAccess --policy-document file://s3-read-policy.json`\n\nFinally, to use this role with an EC2 instance, you need to create an Instance Profile and associate the role with it:\n`aws iam create-instance-profile --instance-profile-name my-ec2-s3-reader-profile`\n`aws iam add-role-to-instance-profile --instance-profile-name my-ec2-s3-reader-profile --role-name my-ec2-s3-reader-role`\n\nWhen launching an EC2 instance, specify this instance profile using `--iam-instance-profile Name=my-ec2-s3-reader-profile`."
    },
    {
        "instruction": "Create an AWS CloudWatch alarm for EC2 CPU utilization.",
        "input": "Alarm when CPU exceeds 70% for 5 minutes, send notification to an SNS topic 'my-devops-alerts'.",
        "output": "AWSTemplateFormatVersion: '2010-09-09'\nDescription: CloudWatch alarm for EC2 CPU utilization.\n\nParameters:\n  InstanceId:\n    Type: String\n    Description: The ID of the EC2 instance to monitor.\n  AlarmName:\n    Type: String\n    Default: EC2-CPU-High-Alarm\n    Description: Name for the CloudWatch alarm.\n  SNSTopicArn:\n    Type: String\n    Description: ARN of the SNS topic to send notifications to.\n\nResources:\n  # SNS Topic for notifications (if not already existing)\n  DevOpsAlertSNSTopic:\n    Type: AWS::SNS::Topic\n    Properties:\n      TopicName: my-devops-alerts\n\n  EC2CPUUtilizationAlarm:\n    Type: AWS::CloudWatch::Alarm\n    Properties:\n      AlarmName: !Ref AlarmName\n      ComparisonOperator: GreaterThanOrEqualToThreshold\n      EvaluationPeriods: 1\n      MetricName: CPUUtilization\n      Namespace: AWS/EC2\n      Period: 300 # 5 minutes (300 seconds)\n      Statistic: Average\n      Threshold: 70\n      AlarmDescription: \"Alarm when EC2 CPU utilization exceeds 70% for 5 minutes\"\n      Dimensions:\n        - Name: InstanceId\n          Value: !Ref InstanceId\n      AlarmActions:\n        - !Ref DevOpsAlertSNSTopic\n      OKActions:\n        - !Ref DevOpsAlertSNSTopic # Optional: Send OK notification\n\nOutputs:\n  AlarmName:\n    Description: The name of the CloudWatch alarm.\n    Value: !Ref EC2CPUUtilizationAlarm\n  SNSTopicArn:\n    Description: ARN of the SNS Topic used for notifications.\n    Value: !Ref DevOpsAlertSNSTopic\n"
    },
    {
        "instruction": "Define an AWS ECS Fargate service with a Load Balancer.",
        "input": "Service for a web app, public facing, with desired count 2, exposing port 80.",
        "output": "AWSTemplateFormatVersion: '2010-09-09'\nDescription: ECS Fargate service with an Application Load Balancer.\n\nParameters:\n  VpcId:\n    Type: String\n    Description: The ID of the VPC where the ECS cluster and services will be deployed.\n  SubnetIds:\n    Type: CommaDelimitedList\n    Description: Comma-separated list of subnet IDs for the ALB and ECS tasks (private subnets recommended for tasks).\n  DesiredCount:\n    Type: Number\n    Default: 2\n    Description: The desired number of tasks to run in the service.\n  ContainerImage:\n    Type: String\n    Default: nginx:latest\n    Description: The Docker image to deploy.\n  ContainerPort:\n    Type: Number\n    Default: 80\n    Description: The port the container exposes.\n\nResources:\n  # ECS Cluster\n  ECSCluster:\n    Type: AWS::ECS::Cluster\n    Properties:\n      ClusterName: MyFargateCluster\n\n  # Task Execution Role (allows ECS to pull images, send logs)\n  TaskExecutionRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: '2012-10-17'\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service: ecs-tasks.amazonaws.com\n            Action: sts:AssumeRole\n      ManagedPolicyArns:\n        - arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\n\n  # Task Definition\n  TaskDefinition:\n    Type: AWS::ECS::TaskDefinition\n    Properties:\n      Family: WebAppTask\n      Cpu: '256'\n      Memory: '512'\n      NetworkMode: awsvpc\n      RequiresCompatibilities:\n        - FARGATE\n      ExecutionRoleArn: !GetAtt TaskExecutionRole.Arn\n      ContainerDefinitions:\n        - Name: webapp-container\n          Image: !Ref ContainerImage\n          PortMappings:\n            - ContainerPort: !Ref ContainerPort\n              Protocol: tcp\n          LogConfiguration:\n            LogDriver: awslogs\n            Options:\n              awslogs-group: /ecs/webapp-logs\n              awslogs-region: !Ref 'AWS::Region'\n              awslogs-stream-prefix: ecs\n\n  # CloudWatch Log Group for ECS Task Logs\n  WebAppLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: /ecs/webapp-logs\n      RetentionInDays: 7 # Retain logs for 7 days\n\n  # Application Load Balancer\n  ApplicationLoadBalancer:\n    Type: AWS::ElasticLoadBalancingV2::LoadBalancer\n    Properties:\n      Scheme: internet-facing\n      Subnets: !Ref SubnetIds\n      Tags:\n        - Key: Environment\n          Value: Prod\n\n  ALBListener:\n    Type: AWS::ElasticLoadBalancingV2::Listener\n    Properties:\n      LoadBalancerArn: !Ref ApplicationLoadBalancer\n      Port: 80\n      Protocol: HTTP\n      DefaultActions:\n        - Type: forward\n          TargetGroupArn: !Ref ALBTg\n\n  ALBTg:\n    Type: AWS::ElasticLoadBalancingV2::TargetGroup\n    Properties:\n      HealthCheckPath: /\n      HealthCheckIntervalSeconds: 30\n      HealthCheckTimeoutSeconds: 5\n      HealthyThresholdCount: 2\n      Port: !Ref ContainerPort\n      Protocol: HTTP\n      VpcId: !Ref VpcId\n      TargetType: ip # Important for Fargate\n      UnhealthyThresholdCount: 2\n\n  # ECS Service\n  ECSService:\n    Type: AWS::ECS::Service\n    Properties:\n      Cluster: !Ref ECSCluster\n      ServiceName: MyWebAppService\n      LaunchType: FARGATE\n      DesiredCount: !Ref DesiredCount\n      TaskDefinition: !Ref TaskDefinition\n      DeploymentConfiguration:\n        MinimumHealthyPercent: 50\n        MaximumPercent: 200\n      NetworkConfiguration:\n        AwsvpcConfiguration:\n          AssignPublicIp: ENABLED # Set to DISABLED if using private subnets and NAT Gateway\n          Subnets: !Ref SubnetIds\n          SecurityGroups: \n            - !GetAtt WebAppSecurityGroup.GroupId # Ensure this SG allows inbound traffic from ALB\n      LoadBalancers:\n        - ContainerName: webapp-container\n          ContainerPort: !Ref ContainerPort\n          TargetGroupArn: !Ref ALBTg\n\n  # Security Group for ECS tasks (allows inbound from ALB)\n  WebAppSecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupName: WebAppSecurityGroup\n      GroupDescription: Enable HTTP access to Fargate containers\n      VpcId: !Ref VpcId\n      SecurityGroupIngress:\n        - IpProtocol: tcp\n          FromPort: !Ref ContainerPort\n          ToPort: !Ref ContainerPort\n          SourceSecurityGroupId: !GetAtt ApplicationLoadBalancer.SecurityGroups.0 # Ingress from ALB\n      Tags:\n        - Key: Name\n          Value: WebAppECSAppSG\n\nOutputs:\n  ECSClusterName:\n    Description: The name of the ECS Cluster.\n    Value: !Ref ECSCluster\n  WebAppServiceURL:\n    Description: The URL of the load balancer for the web application.\n    Value: !GetAtt ApplicationLoadBalancer.DNSName"
    },
    {
        "instruction": "Set an S3 lifecycle policy to transition objects.",
        "input": "Transition objects to Glacier after 90 days, delete after 365 days.",
        "output": "You can define an S3 lifecycle policy using the AWS CLI or CloudFormation. Here's how to do it with the AWS CLI (using a JSON file) and an example CloudFormation snippet.\n\n**1. Using AWS CLI (JSON configuration):**\n\nCreate a JSON file named `lifecycle-policy.json`:\n\n```json\n{\n  \"Rules\": [\n    {\n      \"ID\": \"MoveToGlacierAfter90DaysAndDeleteAfter365Days\",\n      \"Filter\": {\n        \"Prefix\": \"\"\n      },\n      \"Status\": \"Enabled\",\n      \"Transitions\": [\n        {\n          \"Days\": 90,\n          \"StorageClass\": \"GLACIER\"\n        }\n      ],\n      \"Expiration\": {\n        \"Days\": 365\n      }\n    }\n  ]\n}\n```\n\nApply the policy to your S3 bucket:\n\n`aws s3api put-bucket-lifecycle-configuration --bucket YOUR_BUCKET_NAME --lifecycle-configuration file://lifecycle-policy.json`\n\n**2. Using AWS CloudFormation:**\n\n```yaml\nResources:\n  MyS3Bucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: my-data-archive-bucket\n      VersioningConfiguration:\n        Status: Enabled # Recommended for lifecycle policies\n      LifecycleConfiguration:\n        Rules:\n          - Id: MoveToGlacierAndDelete\n            Status: Enabled\n            Transitions:\n              - TransitionInDays: 90\n                StorageClass: GLACIER\n            ExpirationInDays: 365\n```\n\n**Explanation:**\n*   `Transitions`: Specifies when and to what storage class objects should be moved. Here, `GLACIER` after 90 days.\n*   `Expiration`: Specifies when objects should be permanently deleted. Here, after 365 days.\n*   `Filter.Prefix`: If you want the rule to apply only to specific objects (e.g., objects under a certain folder), you can specify a prefix like `'Prefix': 'logs/'`. An empty prefix `\"\"` applies the rule to all objects in the bucket.\n*   `VersioningConfiguration: Status: Enabled`: It's highly recommended to enable versioning on buckets with lifecycle rules, especially if you have `Expiration` rules, to prevent accidental permanent deletion of an object version. The `ExpirationInDays` typically applies to the *current* version of an object."
    },
    {
        "instruction": "Deploy a simple web app to Azure App Service using Azure CLI.",
        "input": "Resource group: my-app-rg, App Service Plan: my-app-plan (Linux, B1), Web App: my-webapp-001, Docker image: nginx.",
        "output": "First, create a resource group:\n`az group create --name my-app-rg --location eastus`\n\nThen, create an App Service Plan (defines the underlying compute resources):\n`az appservice plan create --name my-app-plan --resource-group my-app-rg --is-linux --sku B1`\n\nFinally, create the Web App and deploy the Docker image. For a custom Docker image, you specify the image name.\n`az webapp create --resource-group my-app-rg --plan my-app-plan --name my-webapp-001 --docker-image nginx:latest`\n\nAfter creation, the app should be accessible via its default URL (e.g., `my-webapp-001.azurewebsites.net`)."
    },
    {
        "instruction": "How to create an Azure Blob Storage container and upload a file using Azure CLI?",
        "input": "Storage account: mystorageaccount123, container: myfiles, file: local_file.txt",
        "output": "First, ensure you have an existing storage account. If not, create one:\n`az storage account create --name mystorageaccount123 --resource-group my-rg --location eastus --sku Standard_LRS`\n\nThen, retrieve the connection string or account key for authentication. For security, using `--auth-mode login` (Azure AD authentication) is preferred if your user has the correct RBAC role.\n\n**Option A: Using Connection String (less secure for scripts, but easy):**\n`ACCOUNT_KEY=$(az storage account keys list --resource-group my-rg --account-name mystorageaccount123 --query '[0].value' --output tsv)`\n`export AZURE_STORAGE_ACCOUNT=mystorageaccount123`\n`export AZURE_STORAGE_KEY=$ACCOUNT_KEY`\n\n**Option B: Using Azure AD Login (Recommended):**\n`az login`\n(Ensure your logged-in user has 'Storage Blob Data Contributor' role on the storage account.)\n\nNow, create the container:\n`az storage container create --name myfiles --account-name mystorageaccount123 --auth-mode login # or --connection-string`\n\nFinally, upload the file (assuming `local_file.txt` exists in your current directory):\n`az storage blob upload --container-name myfiles --file local_file.txt --name local_file.txt --account-name mystorageaccount123 --auth-mode login # or --connection-string`\n\nYou can then verify:\n`az storage blob list --container-name myfiles --account-name mystorageaccount123 --output table --auth-mode login`"
    },
    {
        "instruction": "How to retrieve a secret from Azure Key Vault using Azure CLI?",
        "input": "Key Vault name: my-secure-kv, secret name: my-app-secret",
        "output": "To retrieve a secret from Azure Key Vault, ensure your Azure CLI is authenticated and your user or service principal has appropriate access policies on the Key Vault (at least 'Get' for secrets).\n\n`az keyvault secret show --name my-app-secret --vault-name my-secure-kv --query value --output tsv`\n\n**Explanation:**\n*   `--name`: The name of the secret.\n*   `--vault-name`: The name of your Azure Key Vault instance.\n*   `--query value`: This uses JMESPath to extract only the `value` of the secret from the JSON output.\n*   `--output tsv`: This prints the output as tab-separated values, which is useful for direct use in scripts.\n\n**Before running:**\n1.  **Login:** `az login`\n2.  **Grant Access:** Ensure the identity running the command (your user or a service principal) has a Key Vault access policy for 'Get' secrets. This can be configured in the Azure portal under Key Vault -> Access policies."
    },
    {
        "instruction": "Create an Azure Function App triggered by HTTP.",
        "input": "Function in Python 3.9, consumption plan, resource group: func-rg.",
        "output": "First, create a resource group:\n`az group create --name func-rg --location westus2`\n\nThen, create an Azure Storage Account, which is required for the Function App:\n`az storage account create --name funcsastorage123 --location westus2 --resource-group func-rg --sku Standard_LRS`\n\nNext, create a Consumption Plan for serverless scaling:\n`az functionapp plan create --name func-plan --resource-group func-rg --location westus2 --sku Consumption --is-linux`\n\nFinally, create the Function App itself:\n`az functionapp create --resource-group func-rg --consumption-plan-location westus2 --runtime python --runtime-version 3.9 --functions-version 4 --name my-http-func-app --storage-account funcsastorage123`\n\nTo add an HTTP-triggered function, you would typically use Azure Functions Core Tools locally (`func new`) to create the function code and then `func azure functionapp publish` to deploy it to the created Function App."
    },
    {
        "instruction": "Outline an Azure DevOps multi-stage YAML pipeline for a Java Spring Boot application.",
        "input": "Stages for Build, Test, and Deploy to Azure Kubernetes Service (AKS).",
        "output": "```yaml\n# azure-pipelines.yml\n\ntrigger:\n  branches:\n    include:\n      - main\n\nvariables:\n  - group: AzureDevOpsJavaVars # Link to a variable group for shared secrets like AZURE_SUBSCRIPTION\n  - name: dockerRegistryServiceConnection\n    value: 'DockerHubConnection' # Your Docker Registry Service Connection name\n  - name: imageRepository\n    value: 'myorg/springboot-app'\n  - name: imageTag\n    value: '$(Build.BuildId)'\n  - name: aksClusterName\n    value: 'my-aks-cluster'\n  - name: aksResourceGroup\n    value: 'my-aks-rg'\n  - name: kubernetesNamespace\n    value: 'default'\n\nstages:\n- stage: Build\n  displayName: Build Stage\n  jobs:\n  - job: BuildAndTest\n    displayName: Build and Test Java App\n    pool:\n      vmImage: 'ubuntu-latest'\n    steps:\n    - task: Maven@3\n      displayName: 'Maven Build and Tests'\n      inputs:\n        mavenPomFile: 'pom.xml'\n        goals: 'clean install -DskipTests=false'\n        publishJUnitResults: true\n        testResultsFiles: '**/surefire-reports/TEST-*.xml'\n        javaHomeOption: 'JDKVersion'\n        jdkVersionOption: '1.11'\n        mavenOptions: '-Xmx3072m'\n\n    - task: Maven@3\n      displayName: 'Maven Package (skip tests)'\n      inputs:\n        mavenPomFile: 'pom.xml'\n        goals: 'package -DskipTests=true'\n        javaHomeOption: 'JDKVersion'\n        jdkVersionOption: '1.11'\n        mavenOptions: '-Xmx3072m'\n\n    - task: Docker@2\n      displayName: 'Build and Push Docker Image'\n      inputs:\n        containerRegistry: '$(dockerRegistryServiceConnection)'\n        repository: '$(imageRepository)'\n        command: 'buildAndPush'\n        Dockerfile: 'Dockerfile'\n        tags: | # Use multiple tags for flexibility\n          $(imageTag)\n          latest\n\n    - publish: $(System.DefaultWorkingDirectory)/target\n      artifact: jar_artifact\n\n- stage: DeployToDev\n  displayName: Deploy to Dev\n  dependsOn: Build\n  jobs:\n  - deployment: DeployDev\n    displayName: Deploy to Dev AKS\n    environment: 'DevAKS'\n    pool:\n      vmImage: 'ubuntu-latest'\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - task: KubernetesManifest@1\n            displayName: 'Deploy to Kubernetes'\n            inputs:\n              action: 'deploy'\n              kubernetesServiceConnection: 'KubernetesServiceConnectionDev' # Your K8s Service Connection\n              namespace: '$(kubernetesNamespace)'\n              manifests: |\n                $(System.DefaultWorkingDirectory)/kubernetes/dev-deployment.yaml\n                $(System.DefaultWorkingDirectory)/kubernetes/service.yaml\n              # Replace image in YAML (if not using templating)\n              # OR configure your YAML to use '$(imageRepository):$(imageTag)' directly\n              # OR use a tool like kustomize or helm\n              # Recommended: use 'update' action to update image in existing deployment\n              arguments: --set image='$(imageRepository):$(imageTag)'\n\n- stage: DeployToProd\n  displayName: Deploy to Production\n  dependsOn: DeployToDev\n  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main')) # Only deploy to prod from main branch\n  jobs:\n  - deployment: DeployProd\n    displayName: Deploy to Prod AKS\n    environment: 'ProdAKS'\n    pool:\n      vmImage: 'ubuntu-latest'\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - task: KubernetesManifest@1\n            displayName: 'Deploy to Production Kubernetes'\n            inputs:\n              action: 'deploy'\n              kubernetesServiceConnection: 'KubernetesServiceConnectionProd'\n              namespace: '$(kubernetesNamespace)'\n              manifests: |\n                $(System.DefaultWorkingDirectory)/kubernetes/prod-deployment.yaml\n                $(System.DefaultWorkingDirectory)/kubernetes/service.yaml\n              arguments: --set image='$(imageRepository):$(imageTag)'\n\n```\n\n**Assumptions & Setup:**\n*   **Dockerfile:** Assumes you have a `Dockerfile` in your repository root.\n*   **Kubernetes YAMLs:** Assumes you have Kubernetes deployment (`dev-deployment.yaml`, `prod-deployment.yaml`) and service (`service.yaml`) definitions in a `kubernetes/` folder.\n*   **Service Connections:** You'll need to create: \n    *   **Docker Registry service connection** (for Docker Hub, Azure Container Registry, etc.).\n    *   **Kubernetes service connections** for your Dev and Prod AKS clusters. These can be configured using a Service Principal or KubeConfig.\n*   **Variable Group:** Create a variable group named `AzureDevOpsJavaVars` and link it to your pipeline. This is where you'd store subscription IDs, and potentially `AZURE_SERVICE_CONNECTION` if you use an ARM service connection."
    },
    {
        "instruction": "Create an Azure Monitor alert rule for a Virtual Machine's CPU usage.",
        "input": "VM: my-vm, resource group: my-rg, alert if CPU > 90% for 5 minutes, severity 1, email to admin@example.com.",
        "output": "You can create an Azure Monitor alert rule using the Azure CLI. First, make sure you have an action group set up for notifications.\n\n**1. Create an Action Group (if you don't have one):**\nThis group defines where notifications go (email, SMS, webhook, etc.).\n\n```bash\naz monitor action-group create \\\n    --resource-group my-rg \\\n    --name VM-HighCPU-ActionGroup \\\n    --short-name VMCPUAlert \\\n    --action email admin_email admin@example.com\n```\n\n**2. Create the Metric Alert Rule:**\n\n```bash\naz monitor metrics alert create \\\n    --resource-group my-rg \\\n    --name HighCPUAlertForMyVM \\\n    --scopes /subscriptions/<YOUR_SUBSCRIPTION_ID>/resourceGroups/my-rg/providers/Microsoft.Compute/virtualMachines/my-vm \\\n    --condition 'avg Percentage CPU > 90' \\\n    --evaluation-frequency 5m \\\n    --window-size 5m \\\n    --severity 1 \\\n    --description 'CPU utilization exceeds 90% for my-vm' \\\n    --action VM-HighCPU-ActionGroup\n```\n\n**Replace placeholders:**\n*   `<YOUR_SUBSCRIPTION_ID>`: Your Azure subscription ID.\n*   `my-vm`: The name of your Virtual Machine.\n\n**Explanation:**\n*   `--scopes`: The resource ID of the VM you want to monitor. You can get this with `az vm show --name my-vm --resource-group my-rg --query id --output tsv`.\n*   `--condition 'avg Percentage CPU > 90'`: Defines the metric (`Percentage CPU`), aggregation (`avg`), and threshold (90%).\n*   `--evaluation-frequency 5m`: How often the condition is checked (every 5 minutes).\n*   `--window-size 5m`: The period over which the metric data is aggregated for evaluation (the average over the last 5 minutes).\n*   `--severity 1`: Sets the severity level (0-4, 0 being critical).\n*   `--action VM-HighCPU-ActionGroup`: Links the alert to the action group created earlier for notifications."
    },
    {
        "instruction": "Launch a Google Compute Engine (GCE) instance using `gcloud` CLI.",
        "input": "Instance name: my-gce-vm, machine type: e2-medium, Ubuntu 20.04 LTS, allow HTTP traffic.",
        "output": "First, ensure you are authenticated and have the correct project set:\n`gcloud auth login`\n`gcloud config set project your-gcp-project-id`\n\nThen, create the instance:\n\n```bash\ngcloud compute instances create my-gce-vm \\\n    --project=your-gcp-project-id \\\n    --zone=us-central1-a \\\n    --machine-type=e2-medium \\\n    --image-family=ubuntu-2004-lts \\\n    --image-project=ubuntu-os-cloud \\\n    --tags=http-server,https-server \\\n    --metadata startup-script='#! /bin/bash\\n sudo apt-get update && sudo apt-get install -y nginx && sudo service nginx start'\n```\n\n**Explanation:**\n*   `--zone`: Specifies the zone. Choose one relevant to you.\n*   `--image-family` and `--image-project`: Specify the OS image.\n*   `--tags=http-server,https-server`: These tags are used to apply firewall rules to allow HTTP/HTTPS traffic. You might need to create these firewall rules if they don't exist:\n    ```bash\n    gcloud compute firewall-rules create default-allow-http \\\n        --direction=INGRESS \\\n        --priority=1000 \\\n        --network=default \\\n        --action=ALLOW \\\n        --rules=tcp:80 \\\n        --source-ranges=0.0.0.0/0 \\\n        --target-tags=http-server\n\n    gcloud compute firewall-rules create default-allow-https \\\n        --direction=INGRESS \\\n        --priority=1000 \\\n        --network=default \\\n        --action=ALLOW \\\n        --rules=tcp:443 \\\n        --source-ranges=0.0.0.0/0 \\\n        --target-tags=https-server\n    ```\n*   `--metadata startup-script`: A simple script to install and start Nginx on boot.\n\nTo view the instance details:\n`gcloud compute instances describe my-gce-vm --zone us-central1-a`\n\nTo get the external IP:\n`gcloud compute instances describe my-gce-vm --zone us-central1-a --format='get(networkInterfaces[0].accessConfigs[0].natIP)'`"
    },
    {
        "instruction": "Deploy a containerized application to Google Cloud Run.",
        "input": "Service name: my-app-service, image: gcr.io/cloudrun/hello, allow unauthenticated access.",
        "output": "Before deploying, ensure you are authenticated and have the correct project set:\n`gcloud auth login`\n`gcloud config set project your-gcp-project-id`\n\nDeploy the service:\n\n```bash\ngcloud run deploy my-app-service \\\n    --image gcr.io/cloudrun/hello \\\n    --platform managed \\\n    --region us-central1 \\\n    --allow-unauthenticated \\\n    --project=your-gcp-project-id\n```\n\n**Explanation:**\n*   `gcloud run deploy`: The command to deploy or update a Cloud Run service.\n*   `my-app-service`: The desired name for your Cloud Run service.\n*   `--image gcr.io/cloudrun/hello`: The Docker image to deploy. You can replace this with your own image from Container Registry or Artifact Registry (e.g., `gcr.io/your-project-id/your-image:tag`).\n*   `--platform managed`: Specifies the fully managed Cloud Run environment.\n*   `--region us-central1`: The GCP region where your service will be deployed. Choose one close to your users.\n*   `--allow-unauthenticated`: Makes the service publicly accessible without requiring authentication. Omit this for internal or secured services.\n\nAfter deployment, the command will output the Service URL (e.g., `https://my-app-service-xxxxxx-uc.a.run.app`)."
    },
    {
        "instruction": "Set a lifecycle policy for a Google Cloud Storage bucket.",
        "input": "Bucket: my-gcp-archive, move to Coldline after 30 days, delete after 365 days.",
        "output": "You define Cloud Storage bucket lifecycle rules in a JSON file and then apply it to the bucket using `gsutil`.\n\n**1. Create the lifecycle JSON file (e.g., `lifecycle.json`):**\n\n```json\n{\n  \"rule\": [\n    {\n      \"action\": {\n        \"type\": \"SetStorageClass\",\n        \"storageClass\": \"COLDLINE\"\n      },\n      \"condition\": {\n        \"age\": 30\n      }\n    },\n    {\n      \"action\": {\n        \"type\": \"Delete\"\n      },\n      \"condition\": {\n        \"age\": 365\n      }\n    }\n  ]\n}\n```\n\n**2. Apply the policy to your bucket:**\n\n`gsutil lifecycle set lifecycle.json gs://my-gcp-archive`\n\n**Explanation:**\n*   `rule`: An array of rules.\n*   `action`: What action to perform (`SetStorageClass` or `Delete`).\n*   `storageClass`: The target storage class (e.g., `COLDLINE`, `ARCHIVE`, `NEARLINE`, `STANDARD`).\n*   `condition.age`: The number of days after object creation for the rule to apply.\n\nThis will move objects older than 30 days to the `COLDLINE` storage class and delete them after 365 days."
    },
    {
        "instruction": "Deploy a simple Google Cloud Function (Python) triggered by HTTP.",
        "input": "Function name: hello-http, runtime: python39, region: us-central1, allow unauthenticated.",
        "output": "First, create a Python file (e.g., `main.py`) for your function:\n\n```python\n# main.py\n\ndef hello_http(request):\n    \"\"\"Responds to an HTTP request.\n\n    Args:\n        request (flask.Request): The request object.\n        <https://flask.palletsprojects.com/en/1.1.x/api/#incoming-request-data>\n    Returns:\n        The response text, or any set of values that can be turned into a Response\n        object using `make_response`\n        <https://flask.palletsprojects.com/en/1.1.x/api/#flask.json.html>\n    \"\"\"\n    request_json = request.get_json(silent=True)\n    request_args = request.args\n\n    if request_json and 'name' in request_json:\n        name = request_json['name']\n    elif request_args and 'name' in request_args:\n        name = request_args['name']\n    else:\n        name = 'World'\n    return f'Hello, {name}!'\n```\n\nThen, deploy the function using `gcloud functions deploy`:\n\n```bash\ngcloud functions deploy hello-http \\\n    --runtime python39 \\\n    --trigger-http \\\n    --entry-point hello_http \\\n    --region us-central1 \\\n    --allow-unauthenticated \\\n    --source .\n```\n\n**Explanation:**\n*   `--runtime python39`: Specifies the Python 3.9 runtime.\n*   `--trigger-http`: Sets the function to be triggered by HTTP requests.\n*   `--entry-point hello_http`: The name of the function within your `main.py` file to execute.\n*   `--region us-central1`: The GCP region where the function will be deployed.\n*   `--allow-unauthenticated`: Makes the function publicly accessible via its URL.\n*   `--source .`: Deploys the function from the current directory. You can also specify a GCS bucket or a Git repository.\n\nAfter deployment, the command will output the `httpsTrigger.url` which is the endpoint for your function."
    },
    {
        "instruction": "Create a Google Cloud SQL (PostgreSQL) instance.",
        "input": "Instance name: my-sql-instance, database: myappdb, region: us-central1, tier: db-f1-micro, PostgreSQL 13.",
        "output": "```bash\n# Create the Cloud SQL instance\ngcloud sql instances create my-sql-instance \\\n    --database-version=POSTGRES_13 \\\n    --region=us-central1 \\\n    --tier=db-f1-micro \\\n    --root-password=YOUR_ROOT_PASSWORD_HERE \\\n    --enable-bin-log \\\n    --backup-start-time=\"03:00\"\n\n# Create a database within the instance\ngcloud sql databases create myappdb \\\n    --instance=my-sql-instance\n\n# Create a user for the database (optional, but good practice)\ngcloud sql users create myappuser \\\n    --host=% \\\n    --instance=my-sql-instance \\\n    --password=YOUR_APP_USER_PASSWORD_HERE\n\n# Configure authorized networks (example: allow all public IPs, NOT RECOMMENDED FOR PROD)\n# Replace 0.0.0.0/0 with specific CIDR blocks of your application servers or VPN\ngcloud sql instances patch my-sql-instance \\\n    --assign-ip \\\n    --authorized-networks=0.0.0.0/0\n```\n\n**Important Considerations:**\n*   **`YOUR_ROOT_PASSWORD_HERE` and `YOUR_APP_USER_PASSWORD_HERE`:** Replace these with strong, generated passwords. For production, consider using Secrets Manager or a similar secure method for credentials.\n*   **`--authorized-networks=0.0.0.0/0`:** This allows access from anywhere on the internet. **This is highly insecure for production environments.** You should restrict this to the IP addresses or CIDR blocks of your application servers, VPN, or Cloud SQL Auth Proxy.\n*   **Private IP:** For more secure and performant connections, use Private IP. This requires VPC Peering to your application's VPC network.\n    *   To enable private IP (and disable public IP):\n        `gcloud sql instances patch my-sql-instance --no-assign-ip --network=projects/YOUR_PROJECT_ID/global/networks/YOUR_VPC_NETWORK`\n*   **Backups & High Availability:** The example includes `backup-start-time`. For production, consider `--database-flags` (e.g., for `cloudsql.enable_pg_audit`), and a `HA` (High Availability) configuration for automatic failover (`--availability-type=REGIONAL`)."
    },
    {
        "instruction": "Set up a basic Google Cloud Load Balancer (external HTTP(S) LB).",
        "input": "Load balance traffic to a managed instance group of Nginx VMs.",
        "output": "Setting up a Google Cloud External HTTP(S) Load Balancer involves several components: a Managed Instance Group (MIG) for backend VMs, a health check, a backend service, a URL map, and a target HTTP(S) proxy and forwarding rule.\n\n**1. Create a Managed Instance Group (MIG):**\n   First, define an instance template and then create a MIG from it. This example uses a simple Nginx setup.\n\n   ```bash\n   # Create an instance template\n   gcloud compute instance-templates create nginx-template \\\n       --machine-type=e2-medium \\\n       --image-family=ubuntu-2004-lts \\\n       --image-project=ubuntu-os-cloud \\\n       --tags=http-server \\\n       --metadata startup-script='#! /bin/bash\\n sudo apt-get update && sudo apt-get install -y nginx && sudo service nginx start'\n\n   # Create a regional managed instance group\n   gcloud compute instance-groups managed create nginx-mig \\\n       --template=nginx-template \\\n       --size=2 \\\n       --zones=us-central1-a,us-central1-b\n   ```\n\n**2. Create a Health Check:**\n   This checks the health of your backend instances.\n\n   ```bash\n   gcloud compute health-checks create http http-health-check \\\n       --request-path=/ \\\n       --port=80\n   ```\n\n**3. Create a Backend Service:**\n   This defines where to send traffic and uses the health check.\n\n   ```bash\n   gcloud compute backend-services create nginx-backend-service \\\n       --protocol=HTTP \\\n       --health-checks=http-health-check \\\n       --global # Use --region for regional LB\n   ```\n\n**4. Add the MIG to the Backend Service:**\n\n   ```bash\n   gcloud compute backend-services add-backend nginx-backend-service \\\n       --instance-group=nginx-mig \\\n       --instance-group-zone=us-central1-a # Add for each zone in your MIG\n   gcloud compute backend-services add-backend nginx-backend-service \\\n       --instance-group=nginx-mig \\\n       --instance-group-zone=us-central1-b\n   ```\n\n**5. Create a URL Map:**\n   For simple HTTP load balancing, a default URL map is sufficient.\n\n   ```bash\n   gcloud compute url-maps create web-map \\\n       --default-service=nginx-backend-service\n   ```\n\n**6. Create a Target HTTP Proxy:**\n   This proxy uses the URL map to route requests.\n\n   ```bash\n   gcloud compute target-http-proxies create http-proxy \\\n       --url-map=web-map\n   ```\n\n**7. Create a Global Forwarding Rule:**\n   This assigns an external IP address and port to the load balancer.\n\n   ```bash\n   gcloud compute forwarding-rules create http-lb-forwarding-rule \\\n       --address=lb-ip-static # Optional: use a static IP if you reserve one\n       --global \\\n       --target-http-proxy=http-proxy \\\n       --ports=80\n   ```\n\n**Optional: Reserve a Static IP for the Load Balancer:**\n`gcloud compute addresses create lb-ip-static --global`\n\nAfter these steps, your Nginx instances should be accessible via the external IP address of the forwarding rule. You can get the IP address using:\n`gcloud compute forwarding-rules describe http-lb-forwarding-rule --global --format='get(IPAddress)'`"
    },
    {
        "instruction": "Outline a basic Prometheus and Grafana setup for monitoring.",
        "input": "Use Docker Compose for local deployment, monitor a sample Node Exporter.",
        "output": "This setup uses Docker Compose to run Prometheus (for metrics collection) and Grafana (for visualization), along with a Node Exporter to get metrics from the host system.\n\n**1. Directory Structure:**\n```\nmonitoring/\n docker-compose.yml\n prometheus/\n    prometheus.yml\n grafana/\n     provisioning/\n         dashboards/\n             node-exporter-dashboard.json # Optional: Example dashboard\n         datasources/\n             prometheus.yml\n```\n\n**2. `prometheus/prometheus.yml` (Prometheus Configuration):**\n```yaml\nglobal:\n  scrape_interval: 15s # By default, scrape targets every 15 seconds.\n\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n\n  - job_name: 'node_exporter'\n    static_configs:\n      - targets: ['node-exporter:9100'] # Points to the node-exporter service in docker-compose\n```\n\n**3. `grafana/provisioning/datasources/prometheus.yml` (Grafana Datasource):**\n```yaml\napiVersion: 1\n\ndatasources:\n  - name: Prometheus\n    type: prometheus\n    url: http://prometheus:9090 # Points to the prometheus service in docker-compose\n    isDefault: true\n    access: proxy\n    editable: true\n```\n\n**4. `docker-compose.yml`:**\n```yaml\nversion: '3.8'\n\nservices:\n  prometheus:\n    image: prom/prometheus:latest\n    container_name: prometheus\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml\n    command: # Add this line to specify config file location\n      - '--config.file=/etc/prometheus/prometheus.yml'\n    depends_on:\n      - node-exporter\n\n  grafana:\n    image: grafana/grafana:latest\n    container_name: grafana\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - ./grafana/provisioning/:/etc/grafana/provisioning/\n      - grafana_data:/var/lib/grafana\n    depends_on:\n      - prometheus\n\n  node-exporter:\n    image: prom/node-exporter:latest\n    container_name: node-exporter\n    ports:\n      - \"9100:9100\"\n    # Optional: For host metrics, mount host system volumes\n    volumes:\n      - /proc:/host/proc:ro\n      - /sys:/host/sys:ro\n      - /:/rootfs:ro\n    command: \n      - '--path.procfs=/host/proc'\n      - '--path.sysfs=/host/sys'\n      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'\n    restart: unless-stopped\n\nvolumes:\n  grafana_data:\n```\n\n**5. Start the Stack:**\nNavigate to the `monitoring/` directory in your terminal and run:\n`docker-compose up -d`\n\n**Access:**\n*   **Grafana:** `http://localhost:3000` (default login: admin/admin, then prompts for new password)\n*   **Prometheus:** `http://localhost:9090`\n*   **Node Exporter:** `http://localhost:9100/metrics`\n\nOnce logged into Grafana, the Prometheus datasource should already be configured via provisioning. You can then import a pre-built Node Exporter dashboard (e.g., from Grafana Labs Dashboards, ID 1860) or create your own."
    },
    {
        "instruction": "Explain the ELK Stack (Elasticsearch, Logstash, Kibana) and its use in DevOps.",
        "input": "",
        "output": "The ELK Stack is a popular open-source suite of tools designed for searching, analyzing, and visualizing logs and other data in real-time. It's widely used in DevOps for centralized logging, monitoring, and troubleshooting.\n\n**Components:**\n\n1.  **Elasticsearch (E):**\n    *   **Purpose:** A distributed, RESTful search and analytics engine. It's the core data store for all your logs and metrics.\n    *   **Role in ELK:** Stores the incoming log data, indexes it for fast searching, and provides powerful querying capabilities.\n\n2.  **Logstash (L):**\n    *   **Purpose:** A server-side data processing pipeline that ingests data from multiple sources simultaneously, transforms it, and then sends it to a 'stash' like Elasticsearch.\n    *   **Role in ELK:** Collects logs from various sources (servers, applications, cloud services), processes them (filters, parses, enriches data), and sends them to Elasticsearch.\n\n3.  **Kibana (K):**\n    *   **Purpose:** A web-based user interface for searching, visualizing, and analyzing the data stored in Elasticsearch.\n    *   **Role in ELK:** Provides interactive dashboards, charts, and graphs to explore log data, identify patterns, troubleshoot issues, and monitor system health.\n\n**How it works (typical flow):**\n1.  **Log Collection:** Logstash (or lightweight agents like Filebeat/Metricbeat) collects logs from your servers, applications, and network devices.\n2.  **Processing:** Logstash transforms the raw log data into a structured format (e.g., JSON) with relevant fields for easier analysis.\n3.  **Indexing:** The processed data is sent to Elasticsearch, which indexes it for quick searching and aggregation.\n4.  **Visualization:** Kibana connects to Elasticsearch and allows users to create powerful dashboards and visualizations to gain insights from their log data.\n\n**Use in DevOps:**\n*   **Centralized Logging:** Aggregates logs from all applications and infrastructure components into a single, searchable platform.\n*   **Troubleshooting:** Rapidly diagnose issues by searching and filtering logs across multiple systems.\n*   **Monitoring & Alerting:** Create dashboards to monitor system health, application performance, and identify anomalies. With plugins or external tools like Alerting for Elasticsearch, you can set up alerts.\n*   **Security Analytics:** Analyze security logs to detect suspicious activities or breaches.\n*   **Performance Analysis:** Track application performance metrics and identify bottlenecks.\n*   **Compliance & Auditing:** Retain logs for auditing purposes and demonstrate compliance."
    },
    {
        "instruction": "Define a Prometheus Alert rule for high CPU utilization.",
        "input": "Alert if 'node_cpu_seconds_total' (idle CPU) is below 10% for 5 minutes, severity critical, alert name 'HostHighCPU'.",
        "output": "To define a Prometheus alert rule, you create a YAML file (e.g., `rules.yml`) and include it in your Prometheus configuration (`prometheus.yml`).\n\n**1. `rules.yml` (Alert Rule Definition):**\n\n```yaml\n# rules.yml\n\ngroups:\n  - name: host_alerts\n    rules:\n    - alert: HostHighCPU\n      expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 90\n      for: 5m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"High CPU utilization on {{ $labels.instance }}\"\n        description: \"CPU utilization on {{ $labels.instance }} has been over 90% for the last 5 minutes.\"\n```\n\n**Explanation of `expr` (Expression Language):**\n*   `node_cpu_seconds_total{mode=\"idle\"}`: Selects the metric for CPU time spent in idle mode.\n*   `rate(...[5m])`: Calculates the per-second average rate of increase of the idle CPU counter over the last 5 minutes.\n*   `avg by (instance)`: Averages the rate across different CPU cores on the same instance, grouping by the `instance` label.\n*   `* 100`: Converts the rate (which is a fraction) into a percentage.\n*   `100 - (...)`: Calculates the *non-idle* (used) CPU percentage.\n*   `> 90`: The alert fires if the used CPU percentage is greater than 90.\n*   `for: 5m`: The alert will only fire if the condition is true for a continuous duration of 5 minutes.\n*   `labels`: Key-value pairs attached to the alert.\n*   `annotations`: More descriptive information about the alert, often used in notifications.\n\n**2. Update `prometheus.yml`:**\n\nAdd a `rule_files` section to your Prometheus configuration (`prometheus.yml`) to include your new rules file.\n\n```yaml\n# prometheus.yml\n\nglobal:\n  scrape_interval: 15s\n\nrule_files:\n  - \"rules.yml\" # Path to your alert rules file\n\nscrape_configs:\n  - job_name: 'node_exporter'\n    static_configs:\n      - targets: ['your-node-exporter-ip:9100']\n\nalerting:\n  alertmanagers:\n  - static_configs:\n    - targets: ['localhost:9093'] # Address of your Alertmanager\n```\n\n**3. Restart Prometheus:**\nAfter modifying `prometheus.yml`, restart your Prometheus instance to load the new rules. If running in Docker: `docker restart prometheus`.\n\n**4. Configure Alertmanager (for sending notifications):**\nPrometheus sends alerts to Alertmanager, which then routes them to notification channels (Slack, PagerDuty, email, etc.). You'll need to set up Alertmanager separately and configure its routing rules."
    },
    {
        "instruction": "What are the best practices for logging in a microservices architecture?",
        "input": "",
        "output": "Effective logging is critical for debugging, monitoring, and auditing microservices. Here are best practices:\n\n1.  **Centralized Logging System:** Aggregate logs from all services and infrastructure into a single, searchable platform (e.g., ELK Stack, Splunk, Datadog, Sumo Logic, Logz.io). This is foundational for distributed systems.\n\n2.  **Structured Logging (JSON):** Log in a structured format, preferably JSON. This makes logs machine-readable, easier to parse, query, and analyze than plain text. Include key-value pairs for:\n    *   `timestamp`\n    *   `level` (INFO, WARN, ERROR, DEBUG)\n    *   `service_name`\n    *   `host` / `pod_name` / `container_id`\n    *   `message`\n    *   `trace_id` / `correlation_id` (crucial for distributed tracing)\n    *   `span_id` (for tracing sub-operations)\n    *   `user_id` / `request_id` (for correlating user actions)\n    *   Relevant business context (e.g., `order_id`, `product_id`)\n\n3.  **Distributed Tracing:** Implement distributed tracing (e.g., OpenTracing, OpenTelemetry, Zipkin, Jaeger) to visualize the flow of requests across multiple services. `trace_id` and `span_id` in logs are key for this.\n\n4.  **Contextual Logging:** Enrich logs with context relevant to the operation. If a user request fails, include the `user_id` and `request_id` in all logs related to that request across services.\n\n5.  **Log Levels and Verbosity:** Use appropriate log levels (DEBUG, INFO, WARN, ERROR, FATAL) and adjust verbosity. Avoid logging sensitive information in production logs (PII, secrets).\n\n6.  **Immutable Logs:** Once logs are written, they should not be modified. This ensures integrity for auditing and forensics.\n\n7.  **Asynchronous Logging:** Implement non-blocking/asynchronous logging to avoid impacting application performance. Logs should be sent to a buffer or queue and then forwarded to the centralized system.\n\n8.  **Sidecar Containers (Kubernetes):** For Kubernetes, a sidecar container running a logging agent (e.g., Fluentd, Logstash-forwarder) can collect logs from the main application container's shared volume and forward them.\n\n9.  **Log Retention Policies:** Define clear retention policies for different log types based on compliance, debugging needs, and cost. Archive older logs to cheaper storage.\n\n10. **Monitoring and Alerting on Logs:** Don't just collect logs; create alerts based on log patterns (e.g., high rate of ERROR logs, specific error messages) to proactively identify issues.\n\n11. **Standardized Log Format:** Enforce a consistent log format across all services to simplify parsing and analysis.\n\n12. **Infrastructure Logs:** Don't forget to collect logs from infrastructure components (load balancers, API gateways, databases, message queues, containers, orchestrators) and integrate them into the same centralized system."
    },
    {
        "instruction": "Describe a strategy for centralized logging in a cloud environment.",
        "input": "Consider AWS services.",
        "output": "A centralized logging strategy in AWS involves collecting logs from various sources (EC2, Lambda, ECS, Kubernetes, databases, etc.), streaming them to a central location, and then providing tools for analysis, visualization, and alerting.\n\nHere's a common strategy using AWS services:\n\n1.  **Log Sources:**\n    *   **EC2 Instances:** Use CloudWatch Agent to collect system logs (syslog, web server logs) and application logs.\n    *   **Lambda Functions:** Logs are automatically sent to CloudWatch Logs.\n    *   **ECS/EKS Containers:** Configure container logging drivers (e.g., `awslogs` for ECS, Fluent Bit DaemonSet for EKS) to send logs to CloudWatch Logs.\n    *   **RDS/Databases:** Enable database logs to be pushed to CloudWatch Logs or S3.\n    *   **Load Balancers (ALB/NLB), API Gateway, VPC Flow Logs, CloudTrail:** Configure these services to send their access logs and activity logs to S3 buckets.\n\n2.  **Log Ingestion and Aggregation (CloudWatch Logs & S3):**\n    *   **CloudWatch Logs:** Acts as the primary aggregation point for most real-time operational logs. It's highly scalable and integrated with many AWS services.\n    *   **S3 Buckets:** Used for long-term archiving of structured logs (e.g., access logs from ALBs, CloudTrail, VPC Flow Logs) that might not need real-time querying.\n\n3.  **Log Processing and Enrichment (Optional, for advanced use cases):**\n    *   **AWS Lambda:** Triggered by new log events in CloudWatch Logs or S3. Can be used to parse, filter, enrich, or transform log data before sending it to analytics tools.\n    *   **Kinesis Data Firehose:** Can act as a streaming ingestion layer to send logs from CloudWatch Logs to services like Elasticsearch Service, S3, or Splunk.\n\n4.  **Log Analysis and Visualization:**\n    *   **Amazon OpenSearch Service (formerly Elasticsearch Service):** A fully managed service for running Elasticsearch clusters. Ideal for real-time log searching, analysis, and visualization using Kibana.\n    *   **CloudWatch Logs Insights:** A powerful, interactive query service built into CloudWatch Logs that enables you to quickly explore, analyze, and visualize your log data without moving it.\n    *   **Amazon Athena / Redshift Spectrum:** For analyzing logs stored in S3 using standard SQL. Suitable for less frequent, large-scale analytical queries.\n\n5.  **Monitoring and Alerting:**\n    *   **CloudWatch Alarms:** Create alarms on specific log patterns, metric filters from logs, or `ERROR` counts.\n    *   **Amazon SNS:** Send notifications for alerts to email, SMS, or integrated tools (e.g., PagerDuty, Slack via Lambda).\n    *   **OpenSearch Dashboards/Kibana Alerts:** Set up alerts directly within OpenSearch Dashboards based on search queries.\n\n6.  **Log Retention and Archiving:**\n    *   Configure CloudWatch Logs retention policies to automatically delete or transition logs to S3 (less frequently accessed tiers like Glacier) after a defined period to manage costs.\n    *   Use S3 Lifecycle Policies for logs stored directly in S3.\n\nThis strategy provides a robust, scalable, and cost-effective solution for centralized logging and observability in AWS."
    },
    {
        "instruction": "What are health checks and why are they important in microservices?",
        "input": "",
        "output": "Health checks are automated diagnostic checks performed by a system to determine the operational status of an application or service instance. They are crucial in microservices architectures because these systems are distributed and dynamic, making it complex to know if all components are functioning correctly.\n\n**Types of Health Checks:**\n1.  **Liveness Probe (Are you alive?):** Determines if an application instance is running and able to serve requests. If a liveness probe fails, the orchestrator (e.g., Kubernetes, ECS) typically restarts the container.\n    *   **Example:** An HTTP endpoint `/healthz` that returns a 200 OK if the basic process is running.\n\n2.  **Readiness Probe (Are you ready to serve traffic?):** Determines if an application instance is ready to accept incoming requests. If a readiness probe fails, the orchestrator temporarily removes the instance from the load balancer, preventing traffic from being sent to it until it becomes ready again.\n    *   **Example:** An HTTP endpoint `/ready` that returns a 200 OK only after the application has initialized, connected to its database, loaded configurations, etc.\n\n**Why they are important in Microservices:**\n*   **Automated Recovery:** Orchestration platforms use health checks to automatically detect and recover from failures (e.g., restarting crashed containers, replacing unhealthy instances).\n*   **Improved Availability:** By removing unhealthy instances from the load balancer, health checks prevent user requests from being routed to services that can't process them, thus improving overall system availability.\n*   **Safe Deployments:** During rolling updates, health checks ensure that new instances are fully operational before they start receiving production traffic and that old instances are not terminated prematurely.\n*   **Resource Efficiency:** Unhealthy instances that consume resources without serving requests can be identified and terminated.\n*   **Early Problem Detection:** Frequent health checks can detect subtle degradations or issues before they lead to widespread outages.\n*   **Self-Healing Systems:** They enable the implementation of self-healing capabilities, where the system itself takes action to correct detected problems without manual intervention."
    },
    {
        "instruction": "What are best practices for secrets management in DevOps?",
        "input": "",
        "output": "Securely managing secrets (API keys, database credentials, certificates, encryption keys) is paramount in DevOps. Best practices revolve around centralizing, encrypting, auditing, and automating secret access:\n\n1.  **Never Hardcode Secrets:** Avoid embedding secrets directly in code, configuration files, or version control (Git repositories).\n\n2.  **Use a Dedicated Secrets Management Solution:**\n    *   **Tools:** HashiCorp Vault, AWS Secrets Manager, Azure Key Vault, Google Secret Manager, Kubernetes Secrets (with external secret stores/encryption).\n    *   **Benefits:** Centralized storage, encryption at rest and in transit, dynamic secret generation, fine-grained access control, auditing, and secret rotation.\n\n3.  **Encrypt Secrets:** All secrets, whether at rest or in transit, should be encrypted using strong cryptographic algorithms.\n\n4.  **Least Privilege Access:** Grant applications and users only the minimum necessary permissions to access the secrets they need. Use IAM roles, service accounts, and specific policies.\n\n5.  **Dynamic Secrets:** Whenever possible, use dynamic secrets (e.g., short-lived database credentials generated on demand) instead of static, long-lived ones. This reduces the window of exposure.\n\n6.  **Automated Secret Rotation:** Regularly rotate secrets automatically. This limits the lifespan of a compromised secret and reduces the impact of a breach.\n\n7.  **Audit Trails:** Maintain a comprehensive audit log of all secret access, modification, and creation attempts. This helps detect unauthorized access and ensures compliance.\n\n8.  **Inject Secrets at Runtime:** Secrets should be injected into applications at runtime, ideally as environment variables or mounted files, rather than being retrieved and stored by the application itself.\n\n9.  **Separate Environments:** Use different secrets for different environments (development, staging, production) to prevent a breach in one environment from compromising others.\n\n10. **Protect Access to Secrets Manager:** The secrets manager itself is a critical component. Implement robust authentication (MFA), authorization, and network security for access to the secrets management system.\n\n11. **Scan for Exposed Secrets:** Integrate tools (e.g., GitGuardian, detect-secrets, TruffleHog) into your CI/CD pipelines to scan code repositories for accidental exposure of secrets.\n\nBy following these practices, you can significantly reduce the risk of secret exposure and improve your overall security posture."
    },
    {
        "instruction": "Name and briefly describe tools for Docker image vulnerability scanning.",
        "input": "",
        "output": "Docker image vulnerability scanning tools analyze container images for known security vulnerabilities (CVEs) in their operating system packages, libraries, and application dependencies.\n\nHere are some popular tools:\n\n1.  **Trivy (Aqua Security):**\n    *   **Description:** A simple, comprehensive, and fast scanner for vulnerabilities in container images, filesystems, and Git repositories, as well as configuration issues.\n    *   **Key Features:** Supports various OS packages (Alpine, Debian, RHEL, etc.), programming language dependencies (Node.js, Python, Java, Go, etc.), and IaC scanning.\n    *   **Usage:** `trivy image alpine:3.15`\n\n2.  **Clair (Quay/CoreOS):**\n    *   **Description:** An open-source project that performs static analysis of application container images for vulnerabilities. It builds a database of known vulnerabilities and correlates them with image layers.\n    *   **Key Features:** Focuses on OS-level packages. Requires a running instance (often deployed as a service).\n\n3.  **Snyk Container:**\n    *   **Description:** A commercial security platform that includes comprehensive container image scanning. It scans both OS packages and application dependencies, providing actionable remediation advice.\n    *   **Key Features:** Integrates with registries (Docker Hub, ECR, ACR, GCR), CI/CD pipelines, and IDEs. Offers developer-first security.\n    *   **Usage:** `snyk container test my-image:latest`\n\n4.  **Anchore Engine / Syft / Grype:**\n    *   **Description:** Anchore Engine is an open-source container security and compliance platform. Syft generates Software Bill of Materials (SBOMs) from container images and filesystems. Grype uses SBOMs from Syft to find vulnerabilities.\n    *   **Key Features:** Provides policy-driven security, compliance checks, and a deeper analysis of image contents.\n    *   **Usage (Grype):** `grype my-image:latest`\n\n5.  **Harbor (Cloud Native Computing Foundation project):**\n    *   **Description:** An open-source cloud-native registry that includes built-in vulnerability scanning (often integrating with Clair or Trivy) and policy management.\n    *   **Key Features:** Provides a centralized hub for Docker images with integrated security, governance, and replication.\n\n**Why use them?**\n*   **Shift-Left Security:** Integrate scanning into CI/CD pipelines to catch vulnerabilities early in the development lifecycle.\n*   **Compliance:** Help meet regulatory and compliance requirements.\n*   **Risk Reduction:** Identify and remediate known vulnerabilities before deployment to production.\n*   **Visibility:** Gain insight into the security posture of your container images."
    },
    {
        "instruction": "Explain DevSecOps and its core principles.",
        "input": "",
        "output": "DevSecOps is the practice of integrating security into every stage of the software development lifecycle (SDLC) and DevOps pipeline, from planning and development through delivery and operations. It's about making security a shared responsibility and a continuous process, rather than a separate phase or a last-minute addition.\n\n**Core Principles:**\n\n1.  **Shift Left:**\n    *   **Principle:** Integrate security early in the development process (as far left as possible).\n    *   **Practice:** Automated security testing (SAST, DAST, SCA) in CI/CD, threat modeling during design, secure coding training for developers, and security reviews of code.\n\n2.  **Automate Security:**\n    *   **Principle:** Automate security checks and processes within the CI/CD pipeline.\n    *   **Practice:** Automated vulnerability scanning (code, dependencies, containers), security configuration checks for infrastructure as code, automated penetration testing, and automated security policy enforcement.\n\n3.  **Collaborate and Educate:**\n    *   **Principle:** Foster collaboration between development, operations, and security teams. Educate everyone on security best practices.\n    *   **Practice:** Shared security goals, cross-functional team training, security champions within development teams, and open communication channels for security feedback.\n\n4.  **Continuous Security:**\n    *   **Principle:** Security is not a one-time event but an ongoing process.\n    *   **Practice:** Continuous monitoring, real-time threat detection, regular security audits, continuous vulnerability assessment of running systems, and automated patch management.\n\n5.  **Proactive Security:**\n    *   **Principle:** Anticipate and prevent security issues rather than reacting to them.\n    *   **Practice:** Building security into architecture from the start, using secure-by-default configurations, implementing secrets management early, and adopting secure coding guidelines.\n\n6.  **Compliance as Code:**\n    *   **Principle:** Automate the enforcement and validation of compliance requirements.\n    *   **Practice:** Using IaC tools to define and enforce security policies, integrating compliance checks into CI/CD, and generating automated audit reports.\n\n7.  **Feedback Loops:**\n    *   **Principle:** Provide rapid and actionable security feedback to developers.\n    *   **Practice:** Integrating security findings into developer workflows (e.g., IDE plugins, pull request comments), clear remediation guidance, and performance metrics for security activities.\n\nBy embedding security throughout the DevOps pipeline, DevSecOps aims to improve both the speed and security of software delivery, creating more resilient and secure applications and infrastructure."
    },
    {
        "instruction": "Write a Bash script to perform a daily backup of a directory.",
        "input": "Source directory: /data/app, backup to: /backups, retain last 7 days.",
        "output": "```bash\n#!/bin/bash\n\n# Configuration\nSOURCE_DIR=\"/data/app\"\nBACKUP_DIR=\"/backups\"\nRETENTION_DAYS=7\nTIMESTAMP=$(date +%Y%m%d%H%M%S)\nBACKUP_FILE=\"${BACKUP_DIR}/app_backup_${TIMESTAMP}.tar.gz\"\nLOG_FILE=\"/var/log/daily_backup.log\"\n\n# Create backup directory if it doesn't exist\nmkdir -p \"${BACKUP_DIR}\"\nmkdir -p \"$(dirname ${LOG_FILE})\"\n\n# Function to log messages\nlog_message() {\n  echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"${LOG_FILE}\"\n}\n\nlog_message \"Starting daily backup of ${SOURCE_DIR} to ${BACKUP_FILE}\"\n\n# Perform the backup\ntar -czf \"${BACKUP_FILE}\" -C \"$(dirname ${SOURCE_DIR})\" \"$(basename ${SOURCE_DIR})\"\n\n# Check if backup was successful\nif [ $? -eq 0 ]; then\n  log_message \"Backup successful: ${BACKUP_FILE}\"\nelse\n  log_message \"ERROR: Backup failed! Check logs in ${LOG_FILE}\"\n  exit 1\nfi\n\nlog_message \"Cleaning up old backups (retaining last ${RETENTION_DAYS} days)\"\n\n# Clean up old backups (delete files older than RETENTION_DAYS)\nfind \"${BACKUP_DIR}\" -name \"app_backup_*.tar.gz\" -mtime +${RETENTION_DAYS} -delete\n\nif [ $? -eq 0 ]; then\n  log_message \"Old backups cleaned up successfully.\"\nelse\n  log_message \"WARNING: Failed to clean up old backups.\"\nfi\n\nlog_message \"Daily backup script finished.\"\n```\n\n**To use this script:**\n1.  Save it as `daily_backup.sh`.\n2.  Make it executable: `chmod +x daily_backup.sh`.\n3.  **Set up a Cron job** to run it daily (e.g., at 2 AM):\n    `crontab -e`\n    Add the following line (adjust path to your script):\n    `0 2 * * * /path/to/your/daily_backup.sh`\n\n**Important Notes:**\n*   Ensure the user running the script (e.g., root for cron) has read permissions on `SOURCE_DIR` and write permissions on `BACKUP_DIR` and `/var/log/`.\n*   For production, consider backing up to a remote location (S3, network drive) for disaster recovery.\n*   Add error handling for disk space, network issues, etc."
    },
    {
        "instruction": "Write a Python script to sync a local directory to an AWS S3 bucket.",
        "input": "Local path: /sync/data, S3 bucket: my-s3-sync-bucket, delete files in S3 if not in local.",
        "output": "```python\nimport boto3\nimport os\nimport sys\n\ndef sync_to_s3(local_path, bucket_name, delete_missing=False):\n    \"\"\"\n    Syncs a local directory to an S3 bucket.\n    Files that exist locally but not in S3 are uploaded.\n    Files that exist in S3 but not locally are deleted if delete_missing is True.\n    \"\"\"\n    s3 = boto3.client('s3')\n    print(f\"Starting sync from '{local_path}' to s3://{bucket_name}\")\n\n    # List files in local directory\n    local_files = set()\n    for root, _, files in os.walk(local_path):\n        for file in files:\n            relative_path = os.path.relpath(os.path.join(root, file), local_path)\n            local_files.add(relative_path)\n\n    # List files in S3 bucket\n    s3_files = set()\n    try:\n        paginator = s3.get_paginator('list_objects_v2')\n        pages = paginator.paginate(Bucket=bucket_name)\n        for page in pages:\n            if 'Contents' in page:\n                for obj in page['Contents']:\n                    s3_files.add(obj['Key'])\n    except s3.exceptions.NoSuchBucket: # Handle case where bucket doesn't exist\n        print(f\"Bucket '{bucket_name}' does not exist. All local files will be uploaded.\")\n\n\n    # Files to upload (exist locally, not in S3)\n    to_upload = local_files - s3_files\n\n    # Files to delete from S3 (exist in S3, not locally)\n    to_delete = s3_files - local_files\n\n    print(f\"Files to upload: {len(to_upload)}\")\n    print(f\"Files to delete (if enabled): {len(to_delete)}\")\n\n    # Upload new/modified files\n    for f in to_upload:\n        full_local_path = os.path.join(local_path, f)\n        try:\n            s3.upload_file(full_local_path, bucket_name, f)\n            print(f\"Uploaded: {f}\")\n        except Exception as e:\n            print(f\"Error uploading {f}: {e}\", file=sys.stderr)\n\n    # Delete missing files from S3\n    if delete_missing:\n        for f in to_delete:\n            try:\n                s3.delete_object(Bucket=bucket_name, Key=f)\n                print(f\"Deleted from S3: {f}\")\n            except Exception as e:\n                print(f\"Error deleting {f} from S3: {e}\", file=sys.stderr)\n    else:\n        print(\"Deletion of missing files from S3 is disabled.\")\n\n    print(\"Sync complete.\")\n\nif __name__ == \"__main__\":\n    # Configure your paths and bucket name here\n    LOCAL_DIRECTORY = \"/sync/data\"\n    S3_BUCKET_NAME = \"my-s3-sync-bucket\"\n    DELETE_MISSING_FILES = True # Set to False if you only want to upload new files\n\n    # Ensure the local directory exists\n    if not os.path.isdir(LOCAL_DIRECTORY):\n        print(f\"Error: Local directory '{LOCAL_DIRECTORY}' does not exist.\", file=sys.stderr)\n        sys.exit(1)\n\n    sync_to_s3(LOCAL_DIRECTORY, S3_BUCKET_NAME, DELETE_MISSING_FILES)\n```\n\n**To use this script:**\n1.  **Install Boto3:** `pip install boto3`\n2.  **Configure AWS Credentials:** Ensure your AWS CLI or environment variables (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`) are configured. The script will use the default `boto3` credential chain.\n3.  **Create your local directory and S3 bucket:**\n    `mkdir -p /sync/data`\n    `aws s3 mb s3://my-s3-sync-bucket`\n4.  **Save the script:** Save it as `sync_s3.py`.\n5.  **Run:** `python sync_s3.py`\n\n**Note on sync logic:** This simple script compares file names. For robust syncing (checking content changes, large files), `aws s3 sync` CLI command is often preferred, but this script demonstrates the Python `boto3` approach for programmatic control."
    },
    {
        "instruction": "Write a PowerShell script to check the status of a Windows service and restart it if stopped.",
        "input": "Service name: 'ServiceNameToMonitor'",
        "output": "```powershell\n<#\n.SYNOPSIS\n    Checks the status of a specified Windows service and restarts it if it's stopped.\n.PARAMETER ServiceName\n    The name of the Windows service to monitor and potentially restart.\n.EXAMPLE\n    .\\Monitor-Service.ps1 -ServiceName 'Spooler'\n.EXAMPLE\n    Monitor-Service -ServiceName 'MySQL80'\n#>\nparam (\n    [Parameter(Mandatory=$true)]\n    [string]$ServiceName\n)\n\n$LogFile = \"C:\\Logs\\ServiceMonitor-${ServiceName}.log\"\n\n# Create log directory if it doesn't exist\nif (-not (Test-Path (Split-Path $LogFile -Parent))) {\n    New-Item -Path (Split-Path $LogFile -Parent) -ItemType Directory -Force | Out-Null\n}\n\nfunction Write-Log {\n    param (\n        [string]$Message,\n        [string]$Level = 'INFO' # INFO, WARN, ERROR\n    )\n    $Timestamp = Get-Date -Format \"yyyy-MM-dd HH:mm:ss\"\n    $LogEntry = \"$Timestamp [$Level] - $Message\"\n    Add-Content -Path $LogFile -Value $LogEntry\n    Write-Host $LogEntry\n}\n\nWrite-Log \"Monitoring service: '$ServiceName'\"\n\ntry {\n    $Service = Get-Service -Name $ServiceName -ErrorAction Stop\n\n    if ($Service.Status -eq 'Running') {\n        Write-Log \"Service '$ServiceName' is already running.\"\n    } else {\n        Write-Log \"Service '$ServiceName' is in '$($Service.Status)' state. Attempting to restart...\"\n        \n        # Optional: Stop service first if it's in a problematic state (e.g., 'Stopping')\n        # if ($Service.Status -ne 'Stopped') {\n        #     Write-Log \"Stopping service '$ServiceName' before restart.\"\n        #     Stop-Service -Name $ServiceName -Force -ErrorAction SilentlyContinue\n        #     $Service.WaitForStatus('Stopped', 30000) # Wait up to 30 seconds\n        # }\n\n        Start-Service -Name $ServiceName -ErrorAction Stop\n        $Service.WaitForStatus('Running', 60000) # Wait up to 60 seconds for it to start\n\n        $Service = Get-Service -Name $ServiceName # Refresh status\n        if ($Service.Status -eq 'Running') {\n            Write-Log \"Service '$ServiceName' restarted successfully.\"\n        } else {\n            Write-Log \"ERROR: Service '$ServiceName' failed to start. Current status: '$($Service.Status)'\", 'ERROR'\n            exit 1 # Indicate failure\n        }\n    }\n} catch {\n    Write-Log \"ERROR: Could not get or restart service '$ServiceName'. Error: $($_.Exception.Message)\", 'ERROR'\n    exit 1 # Indicate failure\n}\n\nWrite-Log \"Service monitor script finished for '$ServiceName'.\"\n```\n\n**To use this script:**\n1.  Save it as `Monitor-Service.ps1`.\n2.  Open PowerShell (as Administrator if the service requires it).\n3.  Run it: `.\\Monitor-Service.ps1 -ServiceName 'Spooler'` (replace `Spooler` with your target service).\n\n**To automate this:**\n*   **Task Scheduler:** Create a Windows Task Scheduler task to run this script at regular intervals (e.g., every 5 minutes). Configure it to run with elevated privileges if needed."
    },
    {
        "instruction": "Give a basic example of a Git pre-commit hook.",
        "input": "Ensure Python files don't contain 'pdb.set_trace()'.",
        "output": "Git hooks are scripts that Git executes before or after events like commit, push, and receive. Pre-commit hooks are particularly useful for enforcing code quality standards before a commit is created.\n\n**1. Navigate to your Git hooks directory:**\n   Inside your Git repository, the hooks are located in `.git/hooks/`.\n   `cd .git/hooks/`\n\n**2. Create the `pre-commit` file:**\n   Create a new file named `pre-commit` (no extension) or edit the existing `pre-commit.sample`.\n\n   ```bash\n   #!/bin/bash\n\n   echo \"Running pre-commit hook...\"\n\n   # Check for 'pdb.set_trace()' in Python files\n   if git diff --cached --name-only | grep -E '\\.py$' | xargs grep -l 'pdb.set_trace()'; then\n       echo \"\n\n======================================================\nERROR: Found 'pdb.set_trace()' in staged Python files.\nPlease remove debugging statements before committing.\n======================================================\n\n\"\n       exit 1 # Abort the commit\n   fi\n\n   echo \"Pre-commit hook finished successfully.\"\n   exit 0 # Allow the commit\n   ```\n\n**3. Make the script executable:**\n   `chmod +x pre-commit`\n\n**Explanation of the script:**\n*   `#!/bin/bash`: Shebang line, specifies the interpreter.\n*   `git diff --cached --name-only`: Lists only the names of files that are currently staged (added to the index).\n*   `grep -E '\\.py$'`: Filters the list to only include files ending with `.py`.\n*   `xargs grep -l 'pdb.set_trace()'`: Takes the list of Python files and searches within each for the string `pdb.set_trace()`. The `-l` flag prints the filename if a match is found.\n*   `if ...; then ... fi`: If `grep` finds any matches (its exit code is 0), the `if` block is executed, printing an error message and exiting with `1`, which tells Git to abort the commit.\n*   `exit 0`: If no issues are found, the script exits with `0`, allowing the commit to proceed.\n\n**Important Note:** Git hooks are local to each repository clone. For team collaboration and consistent enforcement, consider using a tool like [pre-commit](https://pre-commit.com/) which manages and shares hooks more effectively across a team."
    },
    {
        "instruction": "Explain the difference between GitFlow and Trunk-Based Development branching strategies.",
        "input": "",
        "output": "Both GitFlow and Trunk-Based Development are popular Git branching strategies, but they differ significantly in their approach to managing concurrent development, releases, and hotfixes.\n\n**1. GitFlow (Feature Branches with Long-Lived Branches):**\n\n*   **Core Idea:** A highly structured model with several long-lived branches (master, develop) and short-lived branches (feature, release, hotfix).\n*   **Branches:**\n    *   `master` (or `main`): Always production-ready, contains the last stable released version.\n    *   `develop`: Integrates all completed features for the next release. This is the main development branch.\n    *   `feature/*`: Short-lived branches for new features, branched from `develop` and merged back into `develop`.\n    *   `release/*`: Branches off `develop` when a release is planned. Used for final bug fixes, testing, and preparing for production. Merged into both `master` (tagged) and `develop`.\n    *   `hotfix/*`: Branches off `master` for urgent bug fixes on production. Merged back into `master` (tagged) and `develop`.\n*   **Pros:**\n    *   Clear separation of concerns (development vs. release vs. hotfix).\n    *   Well-defined release cycles.\n    *   Suited for projects with strict release schedules or complex release processes.\n*   **Cons:**\n    *   Can lead to 'merge hell' if feature branches are long-lived and `develop` diverges significantly.\n    *   Higher complexity, more overhead for developers to manage multiple branches.\n    *   Not ideal for continuous delivery/deployment due to longer integration cycles.\n*   **Best For:** Projects with slower release cycles, or environments requiring extensive, formal testing before release.\n\n**2. Trunk-Based Development (Short-Lived Branches):**\n\n*   **Core Idea:** All developers commit small, frequent changes directly to a single, main branch (`trunk` or `main`), or to very short-lived feature branches that are merged back quickly (ideally daily).\n*   **Branches:**\n    *   `trunk` (or `main`): The single source of truth. All development happens here or is merged here very frequently.\n    *   `feature/*`: Very short-lived branches (hours, not days/weeks) off `trunk`, merged back as soon as possible.\n    *   Releases are created by tagging the `trunk` at a specific commit. Hotfixes are typically applied directly to `trunk` and then a new release is tagged.\n*   **Techniques:** Relies heavily on **feature flags** (toggles) to hide unfinished features from users in production, enabling continuous integration and deployment.\n*   **Pros:**\n    *   Promotes continuous integration and deployment.\n    *   Reduces merge conflicts due to small, frequent merges.\n    *   Simpler workflow, less branch management overhead.\n    *   Faster feedback loops and quicker time to market.\n*   **Cons:**\n    *   Requires a high level of discipline for small, frequent commits.\n    *   Needs robust automated testing to ensure the `trunk` is always stable.\n    *   Feature flags add a layer of complexity to the application code.\n*   **Best For:** Teams practicing Continuous Delivery/Deployment, agile methodologies, and frequently releasing software.\n\n**Summary:**\n| Feature           | GitFlow                               | Trunk-Based Development                  |\n| :---------------- | :------------------------------------ | :--------------------------------------- |\n| Main Branches     | `master`, `develop` (long-lived)      | `trunk`/`main` (single, continuously integrated) |\n| Feature Branches  | Longer-lived (days/weeks)             | Very short-lived (hours/days)            |\n| Integration       | Less frequent, larger merges          | Frequent, small merges                   |\n| Releases          | Dedicated `release` branches          | Tags on `trunk`/`main`                   |\n| Hotfixes          | Dedicated `hotfix` branches           | Direct to `trunk`/`main` (then tag)      |\n| Complexity        | Higher (more branch management)       | Lower (simpler branching model)          |\n| Delivery Model    | Scheduled/Batch releases              | Continuous Delivery/Deployment           |\n| Key Enabler       | Strict branching rules                | Feature flags, robust automated tests    |"
    },
    {
        "instruction": "Explain the difference between `git revert` and `git reset`.",
        "input": "",
        "output": "Both `git revert` and `git reset` are used to undo changes in Git, but they do so in fundamentally different ways and are appropriate for different scenarios.\n\n**1. `git revert`**\n\n*   **What it does:** Creates *new* commit(s) that undo the changes introduced by one or more previous commits. It does not alter the project history.\n*   **History:** The original commit(s) remain in the commit history, and a new 'revert' commit is added, showing that those changes were undone.\n*   **Safety:** `git revert` is generally considered safe for undoing changes that have already been pushed to a shared remote repository, as it doesn't rewrite history. It's non-destructive.\n*   **Use Cases:**\n    *   Undoing a commit that has already been pushed to a public branch.\n    *   Removing a bug or undesired feature that was introduced in a specific commit without losing the history of that introduction.\n    *   When you want to explicitly show that a change was made and then undone.\n*   **Example:** `git revert <commit-hash>` (creates a new commit that undoes the specified commit).\n\n**2. `git reset`**\n\n*   **What it does:** Moves the branch pointer to a specified commit, effectively rewriting the project history from that point forward. It is a destructive operation.\n*   **History:** Commits that are 'reset away' from the branch history are effectively removed (though they remain in the reflog for a period). This changes the history that was once published.\n*   **Safety:** `git reset` should generally *not* be used on commits that have already been pushed to a shared remote repository, as it rewrites history and can cause major problems for collaborators who have based their work on the original history.\n*   **Modes:**\n    *   `--soft`: Moves the HEAD pointer to the specified commit. The changes from the 'reset away' commits are kept in the staging area (index).\n    *   `--mixed` (default): Moves the HEAD pointer. Changes are kept in the working directory but *not* in the staging area.\n    *   `--hard`: Moves the HEAD pointer, and *discards all changes* from the working directory and staging area that were made since the reset target. **This is irreversible and dangerous if not used carefully.**\n*   **Use Cases:**\n    *   Cleaning up local, unpushed commits.\n    *   Undoing commits that you *just* made and haven't pushed yet.\n    *   Unstaging changes (`git reset HEAD <file>`).\n    *   Discarding local changes (`git reset --hard HEAD`).\n*   **Example:** `git reset --hard HEAD~1` (moves the HEAD pointer back by one commit, discarding all changes).\n\n"
    },
    {
        "instruction": "What is DevOps and why is it important?",
        "input": "",
        "output": "DevOps is a set of practices that combines software development (Dev) and IT operations (Ops) to shorten the systems development life cycle and provide continuous delivery with high software quality. It's a cultural shift that emphasizes communication, collaboration, integration, and automation among software developers and IT operations professionals.\n\n**Key Principles/Pillars of DevOps:**\n*   **Culture:** Fostering collaboration, shared responsibility, and blameless post-mortems.\n*   **Automation:** Automating repeatable tasks (builds, tests, deployments, infrastructure provisioning).\n*   **Lean:** Minimizing waste and maximizing value through continuous improvement.\n*   **Measurement:** Collecting metrics to track performance, identify bottlenecks, and make data-driven decisions.\n*   **Sharing:** Promoting knowledge sharing and transparency across teams.\n\n**Why is DevOps Important?**\n\n1.  **Faster Time to Market:** Automating the delivery pipeline allows for quicker and more frequent releases of new features and bug fixes.\n2.  **Improved Collaboration:** Breaks down silos between development and operations teams, leading to better communication and shared goals.\n3.  **Higher Quality Software:** Continuous integration and automated testing lead to fewer defects and more reliable software.\n4.  **Increased Efficiency:** Automation reduces manual effort, freeing up teams to focus on innovation.\n5.  **Reduced Risk:** Smaller, more frequent releases reduce the impact of potential failures, and automated testing catches issues early.\n6.  **Better Scalability and Reliability:** Infrastructure as Code (IaC) and automation ensure consistent and scalable environments.\n7.  **Cost Savings:** Optimizing infrastructure and processes can lead to reduced operational costs.\n8.  **Enhanced Customer Satisfaction:** Faster delivery of features and more stable applications lead to a better user experience.\n9.  **Innovation and Adaptability:** Teams can experiment more, deploy faster, and respond to market changes more quickly."
    },
    {
        "instruction": "What is Site Reliability Engineering (SRE)?",
        "input": "",
        "output": "Site Reliability Engineering (SRE) is a discipline that incorporates aspects of software engineering and applies them to infrastructure and operations problems. The main goals are to create highly reliable and scalable software systems. SRE was created at Google and is often described as \"what happens when you ask a software engineer to design an operations function.\"\n\n**Core Tenets of SRE:**\n\n1.  **Embracing Risk (Error Budgets):** SRE acknowledges that 100% reliability is impossible and often not cost-effective. Instead, it uses **Service Level Objectives (SLOs)** to define acceptable levels of unreliability (Error Budgets). If the error budget is exhausted, development teams may need to shift focus from new features to reliability work.\n\n2.  **Toil Reduction:** Toil refers to manual, repetitive, automatable, tactical, and devoid-of-enduring-value work. SRE aims to eliminate toil through automation, ideally keeping it below 50% of an SRE's time.\n\n3.  **Monitoring:** Implementing comprehensive monitoring to observe system health, performance, and user experience. Focus on four golden signals: Latency, Traffic, Errors, Saturation.\n\n4.  **Automation:** Automating tasks like deployments, scaling, incident response, and capacity planning to ensure consistency and reduce human error.\n\n5.  **Blameless Postmortems:** When incidents occur, SREs conduct blameless postmortems to understand the root cause and implement preventative measures, focusing on systemic issues rather than individual blame.\n\n6.  **Simplicity:** Favoring simpler designs and implementations to reduce complexity and potential failure points.\n\n7.  **Release Engineering:** Ensuring reliable, repeatable, and automated software releases.\n\n8.  **Disaster Planning and Testing:** Regularly testing disaster recovery plans to ensure resilience.\n\n**SRE vs. DevOps:**\n*   **DevOps:** A cultural movement and set of practices for improving collaboration and automation across development and operations to deliver software faster and more reliably.\n*   **SRE:** A specific implementation or a prescriptive way of achieving the goals of DevOps, focusing heavily on engineering principles to achieve operational excellence. SRE often provides the *how* for DevOps' *what*.\n\nIn essence, SRE is about applying engineering discipline to operations to achieve measurable reliability, scalability, and efficiency. It uses software to manage systems, automate tasks, and measure performance, with a strong emphasis on continuous improvement and learning from failures."
    },
    {
        "instruction": "What is the difference between CI and CD?",
        "input": "",
        "output": "CI and CD are two distinct but related practices that are fundamental to modern software delivery, often used together as \"CI/CD pipeline.\"\n\n**1. CI: Continuous Integration**\n\n*   **What it is:** A development practice where developers frequently integrate their code changes into a central repository (e.g., Git). Each integration is then verified by an automated build and automated tests.\n*   **Goal:** To detect integration issues (e.g., merge conflicts, broken builds, failed tests) early and often. It aims to prevent integration nightmares and ensure that the codebase is always in a working, releasable state.\n*   **Key Activities:**\n    *   Frequent code commits (daily or multiple times a day).\n    *   Automated builds.\n    *   Automated unit and integration tests.\n    *   Code quality checks (linters, static analysis).\n    *   Fast feedback to developers on build/test status.\n*   **Outcome:** A successfully built and tested artifact that is ready for the next stage.\n\n**2. CD: Continuous Delivery / Continuous Deployment**\n\nCD stands for two related but different concepts:\n\n*   **Continuous Delivery (CD - first meaning):**\n    *   **What it is:** An extension of CI where the successfully built and tested code (the artifact) is *always* in a state where it can be safely and quickly released to production at *any time*.\n    *   **Goal:** To ensure that software is always ready for release. The decision to release to production is a *manual* step, but the process leading up to it is fully automated and reliable.\n    *   **Key Activities:**\n        *   All CI activities.\n        *   Automated deployment to staging/testing environments.\n        *   Automated acceptance testing (E2E tests).\n        *   Manual approval gate before production deployment.\n        *   Automated provisioning of infrastructure (IaC).\n*   **Outcome:** A release candidate that is vetted and awaiting human decision for production deployment.\n\n*   **Continuous Deployment (CD - second meaning):**\n    *   **What it is:** An even further extension of Continuous Delivery where *every* change that passes all automated tests in the pipeline is *automatically* deployed to production without human intervention.\n    *   **Goal:** To achieve the fastest possible delivery of value to users, eliminating the manual release approval step.\n    *   **Key Activities:**\n        *   All Continuous Delivery activities.\n        *   No manual gates before production deployment.\n        *   Relies heavily on robust automated testing, monitoring, and potentially feature flags for safety.\n*   **Outcome:** Code changes go live to users immediately upon passing the pipeline.\n\n**Relationship:**\n*   CI is a prerequisite for CD. You can't have reliable CD without strong CI.\n*   Continuous Delivery is the ability to deploy to production at any time.\n*   Continuous Deployment is the *act* of deploying to production with every successful build.\n\nIn summary, CI focuses on integration and build quality, while CD (Delivery) ensures readiness for release, and CD (Deployment) automates the actual release to production."
    },
    {
        "instruction": "What are the key differences between Microservices and Monolith architectures?",
        "input": "",
        "output": "Microservices and Monoliths are two fundamental architectural styles for building applications, each with distinct characteristics, advantages, and disadvantages.\n\n**1. Monolith (Monolithic Architecture):**\n\n*   **Definition:** A single, self-contained, and often large application where all components (UI, business logic, data access layer, etc.) are tightly coupled and run as a single process. All code typically resides in a single codebase and is deployed as one unit.\n*   **Characteristics:**\n    *   **Single Codebase:** All features developed within one repository.\n    *   **Single Deployment Unit:** Deployed as one large executable or web archive.\n    *   **Shared Database:** Often uses a single, centralized database.\n    *   **Tight Coupling:** Components are highly interdependent.\n    *   **Unified Development:** All developers work on the same codebase.\n*   **Advantages:**\n    *   **Simplicity:** Easier to start new projects, less operational overhead initially.\n    *   **Easier Debugging:** All code runs in one process, making debugging and tracing simpler.\n    *   **Simpler Deployment:** Just one artifact to deploy.\n*   **Disadvantages:**\n    *   **Scalability Challenges:** Cannot scale individual components independently. Scaling requires scaling the entire application.\n    *   **Technology Lock-in:** Difficult to introduce new technologies or languages.\n    *   **Slower Development:** Large codebases can become complex, leading to slower development cycles and higher risk for changes.\n    *   **Single Point of Failure:** A bug in one module can potentially bring down the entire application.\n    *   **Higher Entry Barrier for New Developers:** Harder for new team members to understand the entire complex system.\n\n**2. Microservices (Microservices Architecture):**\n\n*   **Definition:** An application is built as a collection of small, independent, loosely coupled services. Each service runs in its own process, communicates via lightweight mechanisms (like APIs), and is independently deployable.\n*   **Characteristics:**\n    *   **Distributed System:** Multiple, smaller, independent services.\n    *   **Independent Deployment:** Each service can be deployed, updated, and scaled independently.\n    *   **Decentralized Data:** Each service typically owns its own database, promoting data consistency within the service.\n    *   **Loose Coupling:** Services interact via well-defined APIs.\n    *   **Polyglot:** Can use different technologies/languages for different services.\n*   **Advantages:**\n    *   **Scalability:** Individual services can be scaled independently based on demand.\n    *   **Technology Flexibility:** Teams can choose the best technology stack for each service.\n    *   **Faster Development & Deployment:** Smaller codebases, easier for teams to work in parallel.\n    *   **Resilience:** Failure in one service doesn't necessarily bring down the entire application.\n    *   **Easier Maintenance:** Smaller, focused services are easier to understand and maintain.\n*   **Disadvantages:**"
    },
    {
        "instruction": "What is Incident Management in a DevOps context?",
        "input": "",
        "output": "In a DevOps context, Incident Management is the process of responding to and resolving unplanned interruptions or reductions in the quality of a service. It's highly focused on rapid response, minimizing impact, and continuous learning to prevent future occurrences, aligning closely with SRE principles.\n\n**Key Aspects and Goals:**\n\n1.  **Minimizing Impact (MTTR - Mean Time To Restore):** The primary goal is to restore service functionality as quickly as possible to minimize the impact on users and business operations. This often means focusing on mitigation first, and root cause analysis later.\n\n2.  **Automation and Tooling:** DevOps teams leverage automation to streamline incident response:\n    *   Automated alerts from monitoring systems.\n    *   Automated runbooks or playbooks to execute common diagnostic or recovery steps.\n    *   Automated notifications to relevant teams (Slack, PagerDuty, email).\n\n3.  **Clear Communication:** During an incident, transparent and timely communication is critical, both internally (to stakeholders, development teams) and externally (to affected users/customers).\n\n4.  **Defined Roles and Responsibilities:** Establish clear roles (e.g., Incident Commander, Communications Lead, Technical Lead) to ensure organized and efficient response.\n\n5.  **Runbooks and Playbooks:** Pre-defined procedures and troubleshooting guides for common incidents. These reduce cognitive load during high-stress situations.\n\n6.  **Incident Response Platforms:** Tools like PagerDuty, Opsgenie, VictorOps help with on-call scheduling, alert routing, notification, and incident collaboration.\n\n7.  **Post-Mortem / Root Cause Analysis (Blameless):** After an incident is resolved, a blameless post-mortem is conducted to understand *what happened*, *why it happened*, and *what can be done to prevent recurrence*. The focus is on systemic improvements, not assigning blame.\n\n8.  **Continuous Improvement:** Learnings from post-mortems lead to actionable improvements:\n    *   Updating monitoring and alerting.\n    *   Improving documentation and runbooks.\n    *   Automating more recovery steps.\n    *   Implementing architectural changes or new features to enhance resilience.\n\n9.  **Service Level Objectives (SLOs) and Error Budgets:** Incidents directly impact SLOs (e.g., availability, latency). Exceeding error budgets can trigger a shift in priorities towards reliability work, reinforcing the importance of incident prevention and resolution.\n\nIn DevOps, incident management is not just about fixing things when they break; it's a vital feedback loop that drives continuous learning, builds more resilient systems, and fosters a culture of shared responsibility for operational excellence."
    },
    {
        "instruction": "Explain the concept of a 'Blameless Post-mortem' in DevOps/SRE.",
        "input": "",
        "output": "A \"Blameless Post-mortem\" (or Blameless Root Cause Analysis) is a critical practice in DevOps and Site Reliability Engineering (SRE) for learning from incidents and failures. Its core principle is to focus on understanding the systemic causes of an incident rather than assigning blame to individuals.\n\n**Key Characteristics:**\n\n1.  **No Blame:** The most crucial aspect. The goal is not to find who is at fault, but what went wrong in the system, processes, or tools. This encourages honesty, openness, and psychological safety, allowing everyone to share information freely without fear of reprisal.\n\n2.  **Focus on System and Process:** Investigations delve into:\n    *   **Technical failures:** Software bugs, infrastructure misconfigurations, hardware failures.\n    *   **Process gaps:** Insufficient testing, unclear deployment procedures, poor communication protocols.\n    *   **Tooling limitations:** Inadequate monitoring, alerting, or rollback capabilities.\n    *   **Organizational factors:** Lack of training, team silos, pressure to deliver quickly.\n\n3.  **Detailed Timeline:** Reconstruct a precise timeline of events, including actions taken, system responses, and human observations. This helps identify contributing factors.\n\n4.  **Root Cause Analysis:** Go beyond the immediate cause to identify underlying factors. Techniques like the \"5 Whys\" or Fishbone diagrams can be useful.\n\n5.  **Actionable Learnings:** The outcome is a set of concrete, actionable items to prevent recurrence or mitigate the impact of similar future incidents. These actions are assigned to specific owners and tracked.\n\n6.  **Sharing and Transparency:** Post-mortem reports are typically shared widely within the organization (and sometimes externally for major incidents) to disseminate lessons learned and improve collective knowledge.\n\n7.  **Focus on Improvement, Not Punishment:** The ultimate goal is continuous improvement of the system, processes, and culture to build more resilient and reliable services.\n\n**Why is it Important?**\n*   **Promotes Learning:** Without fear of blame, individuals are more likely to share critical information, leading to a deeper understanding of failures.\n*   **Builds Trust:** Fosters a culture of trust and transparency across teams (Dev, Ops, Security).\n*   **Improves Systems:** Leads to more effective and sustainable solutions because the focus is on fixing the system, not punishing people.\n*   **Reduces Recurrence:** By addressing systemic issues, the likelihood of similar incidents recurring is significantly reduced.\n*   **Enhances Resilience:** Contributes to building more robust and self-healing systems over time.\n\nBlameless post-mortems transform failures into valuable learning opportunities, which is a cornerstone of a mature DevOps and SRE culture."
    },
    {
        "instruction": "How to troubleshoot a network connectivity issue in a Linux server?",
        "input": "",
        "output": "Troubleshooting network connectivity on a Linux server involves a systematic approach, checking various layers from physical to application. Here's a common checklist:\n\n1.  **Check Physical/Link Layer (Basic Connectivity):**\n    *   **Is the network cable plugged in? (VMs: is the virtual NIC attached?):\n        `ip link show` or `ifconfig -a` (deprecated)\n        Look for `UP` status. If `DOWN`, try `sudo ip link set eth0 up` (replace `eth0` with your interface).\n    *   **Are there any obvious errors on the interface?** Check `RX errors`, `TX errors` from `ip link show`.\n\n2.  **Check IP Configuration:**\n    *   **Is an IP address assigned?**\n        `ip addr show`\n        Ensure the interface has an expected IP address and subnet mask.\n    *   **Is the Gateway correct?**\n        `ip route show`\n        Verify the default route points to the correct gateway IP.\n    *   **Can you ping the gateway?**\n        `ping -c 3 <gateway-ip>`\n        If not, issues are likely with the local network configuration or the gateway itself.\n\n3.  **Check DNS Resolution:**\n    *   **Can you resolve external hostnames?**\n        `ping -c 3 google.com`\n        `dig google.com` or `nslookup google.com`\n    *   **Check DNS server configuration:** Look at `/etc/resolv.conf`. Ensure it points to valid and reachable DNS servers.\n    *   **Can you ping the DNS server IPs directly?**\n        `ping -c 3 <dns-server-ip>`\n        If DNS resolution fails but you can ping external IPs, the issue is with your DNS configuration or the DNS server.\n\n4.  **Check Firewall (iptables/firewalld/ufw):**\n    *   **Is the firewall blocking outgoing or incoming connections?**\n        *   `sudo iptables -L -n -v` (for iptables)\n        *   `sudo firewall-cmd --list-all` (for firewalld)\n        *   `sudo ufw status` (for ufw)\n    *   Temporarily disable the firewall for testing (with caution on production!): `sudo systemctl stop firewalld` or `sudo ufw disable`.\n\n5.  **Test External Connectivity:**\n    *   **Can you reach an external IP?**\n        `ping -c 3 8.8.8.8` (Google's DNS)\n    *   **Can you trace the route?**\n        `traceroute google.com` or `mtr google.com` (provides more detailed path analysis)\n        This helps identify where traffic is stopping.\n\n6.  **Check Application Level Connectivity:**\n    *   **Is your application listening on the correct port?**\n        `sudo ss -tulnp | grep <port>` or `sudo netstat -tulnp | grep <port>`\n        (`-t`: TCP, `-u`: UDP, `-l`: listening, `-n`: numeric, `-p`: process name)\n    *   **Can you connect to your own application locally?**\n        `curl http://localhost:<app-port>`\n    *   **Can you connect to a remote service on a specific port?**\n        `nc -vz <remote-host> <port>` (Netcat)\n        `telnet <remote-host> <port>`\n\n**Common Issues:**\n*   Incorrect IP/subnet mask/gateway.\n*   DNS resolution failures.\n*   Firewall rules blocking traffic.\n*   Network security groups (in cloud environments) blocking ports.\n*   Proxy server issues.\n*   Application not listening or crashing."
    },
    {
        "instruction": "What is the difference between virtualization and containerization?",
        "input": "",
        "output": "Virtualization and containerization are both technologies for abstracting and isolating applications and their dependencies, but they operate at different levels of the stack and offer different benefits.\n\n**1. Virtualization (Virtual Machines - VMs):**\n\n*   **Concept:** Virtualization involves running multiple isolated guest operating systems (OS) on a single physical host machine. A hypervisor (like VMware ESXi, VirtualBox, KVM, Hyper-V) sits between the hardware and the virtual machines.\n*   **How it works:**\n    *   Each VM contains its own full guest OS (e.g., Linux, Windows), including its own kernel, libraries, and binaries.\n    *   The hypervisor virtualizes the hardware resources (CPU, memory, storage, network) and allocates them to each VM.\n    *   Applications run inside their respective guest OS.\n*   **Diagram (Conceptual):**\n    ```\n    +------------------------------------------+\n    | Hardware (Physical Server)               |\n    +------------------------------------------+\n    | Hypervisor (VMware, VirtualBox, KVM)     |\n    +------------------------------------------+\n    | VM 1         | VM 2         | VM 3      |\n    | +--------+   | +--------+   | +--------+ |\n    | | Guest  |   | | Guest  |   | | Guest  | |\n    | | OS     |   | | OS     |   | | OS     | |\n    | | +------+   | | +------+   | | +------+ |\n    | | | App  |   | | | App  |   | | | App  | |\n    | | | +----+   | | | +----+   | | | +----+ |\n    | | | Deps |   | | | Deps |   | | | Deps | |\n    | +--------+   | +--------+   | +--------+ |\n    +--------------+--------------+------------+\n    ```\n*   **Pros:**\n    *   Strong isolation between VMs.\n    *   Ability to run different OS on the same hardware.\n    *   Mature technology with robust tooling.\n*   **Cons:**\n    *   Resource-intensive (each VM requires its own OS, leading to significant overhead).\n    *   Slower startup times.\n    *   Less portable (VM images can be large and tied to hypervisor).\n\n**2. Containerization (Containers):**\n\n*   **Concept:** Containerization involves packaging an application and all its dependencies (libraries, binaries, configuration files) into a lightweight, isolated unit that shares the host OS kernel. Docker is the most popular containerization technology.\n*   **How it works:**\n    *   Containers share the host operating system's kernel.\n    *   They use OS-level virtualization features (like Linux namespaces for isolation and cgroups for resource limiting) to provide isolated environments.\n    *   Applications run directly on the host OS but within their isolated container environment.\n*   **Diagram (Conceptual):**\n"
    },
    {
        "instruction": "Explain the concept of 'Idempotency' in automation and IaC.",
        "input": "",
        "output": "In the context of automation, particularly Infrastructure as Code (IaC) and configuration management, **idempotency** means that an operation or a script, when executed multiple times with the same input, will produce the exact same result and system state as the first time it was executed, without causing unintended side effects.\n\nEssentially, if your automation is idempotent, you can run it repeatedly without worrying about breaking things or creating duplicate resources. The system will only make changes if the current state deviates from the desired state.\n\n**Why is Idempotency Important in DevOps/IaC?**\n\n1.  **Reliability and Consistency:** Ensures that environments (dev, staging, prod) are built and maintained consistently. If a build fails halfway, running the automation again should pick up where it left off or correctly re-apply necessary changes.\n\n2.  **Safety:** Prevents unintended side effects like creating duplicate VMs, services, or configurations, which can lead to unnecessary costs, resource conflicts, or system instability.\n\n3.  **Simplicity in Operations:** Operators don't need to track the current state of infrastructure or guess which changes have already been applied. They can simply apply the IaC configuration, and the tools will handle the differences.\n\n4.  **Error Recovery:** If an automation run fails mid-way, simply re-running it (assuming it's idempotent) will often resolve the issue by completing the unfinished steps or correcting any partial changes.\n\n5.  **Auditability and Rollback:** Since operations are predictable, it's easier to reason about system state and facilitate rollbacks.\n\n6.  **Continuous Delivery:** Essential for CI/CD pipelines, where automated deployments might run frequently and need to ensure the target environment always matches the desired state defined in code.\n\n**Examples of Idempotent vs. Non-Idempotent Operations:**\n\n*   **Non-Idempotent (Bad):**\n    *   `create_user('alice')`: If run twice, might create two 'alice' users or fail on the second run, but it doesn't leave the system in the *same* state as the first successful run.\n    *   `add_firewall_rule(port=80)`: If run twice, might add duplicate rules or fail.\n    *   `sudo apt-get install nginx`: If run repeatedly, it will keep attempting to install Nginx even if it's already there (though `apt-get` itself is somewhat idempotent in that it won't install multiple copies).\n\n*   **Idempotent (Good):**\n    *   **Terraform:** `resource \"aws_instance\" \"web\" {}`. Terraform checks if the instance already exists and its state. It only creates if missing, or updates if configuration drifts.\n    *   **Ansible:** `state: present` in modules like `ansible.builtin.apt` or `ansible.builtin.file`. Ansible checks if the package/file is already in the `present` state and only acts if it's not.\n    *   `set_nginx_config(port=80)`: The function first checks if Nginx is configured for port 80. If it is, it does nothing. If not, it configures it.\n\n**Achieving Idempotency:**\n*   Use declarative tools like Terraform, CloudFormation, Kubernetes YAML, which inherently aim for idempotency by managing desired state.\n*   In imperative scripts (Bash, Python), include checks for existence or desired state before performing actions (e.g., `if [ ! -f 'file.txt' ]; then touch file.txt; fi`).\n*   Leverage features of configuration management tools (Ansible, Chef, Puppet) that use `present`, `absent`, `latest` states."
    },
    {
        "instruction": "What is Chaos Engineering?",
        "input": "",
        "output": "Chaos Engineering is the discipline of experimenting on a system in production in order to build confidence in that system's capability to withstand turbulent conditions. Instead of waiting for outages to occur, chaos engineering proactively injects failures into a system to identify weaknesses and vulnerabilities before they impact customers.\n\n**Core Principles:**\n\n1.  **Hypothesize about Steady State:** Start by defining the normal, healthy behavior of your system (the \"steady state\"). This involves key metrics like throughput, latency, error rates, and resource utilization.\n\n2.  **Vary Real-World Events:** Introduce realistic disruptive events that occur in production environments. Examples include:\n    *   Crashing instances/pods.\n    *   Introducing network latency or packet loss.\n    *   Disabling services or entire data centers.\n    *   Overloading a dependency.\n    *   Simulating clock drifts.\n\n3.  **Run Experiments in Production:** The most effective chaos experiments are run directly in the production environment (though starting in lower environments is a good practice). This is because production environments exhibit the most realistic traffic patterns and system interactions.\n\n4.  **Automate Experiments:** Automate the execution of experiments and the analysis of their impact to ensure consistency and repeatability.\n\n5.  **Minimize Blast Radius:** Design experiments to affect the smallest possible population of users or systems, and have clear \"kill switches\" to stop an experiment immediately if it causes unexpected or severe degradation.\n\n**Goals of Chaos Engineering:**\n*   **Proactive Resilience Improvement:** Identify weaknesses (single points of failure, misconfigurations, inadequate timeouts, insufficient monitoring) that could lead to outages.\n*   **Verify System Design:** Confirm that redundancy, failover mechanisms, and self-healing capabilities work as expected.\n*   **Improve Incident Response:** Test and improve incident response playbooks and the team's ability to react under pressure.\n*   **Build Confidence:** Increase team confidence in the system's ability to handle unexpected failures.\n*   **Uncover Unknowns:** Discover hidden dependencies, unexpected failure modes, or silent failures that might not show up in traditional testing.\n\n**Tools for Chaos Engineering:**\n*   **Chaos Monkey (Netflix):** Randomly terminates instances.\n*   **Chaos Mesh (CNCF):** A cloud-native Chaos Engineering platform for Kubernetes.\n*   **Gremlin:** A SaaS platform for running various types of chaos experiments.\n*   **LitmusChaos (CNCF):** Another open-source Chaos Engineering platform for Kubernetes.\n\nChaos Engineering is not about causing outages, but about deliberately and controlledly inducing small failures to learn how a complex system behaves under stress, ultimately making it more robust and reliable."
    },
    {
        "instruction": "Explain how to set up GitHub webhooks for CI/CD.",
        "input": "",
        "output": "GitHub webhooks are a mechanism for GitHub to send real-time notifications to an external service when certain events occur in a repository (e.g., pushes, pull requests, issues). These notifications are typically HTTP POST requests. CI/CD systems often use webhooks to automatically trigger pipelines.\n\n**How to Set Up a GitHub Webhook:**\n\n1.  **Get your CI/CD System's Webhook URL:**\n    Your CI/CD tool (Jenkins, GitLab, CircleCI, Azure DevOps, etc.) will provide a specific webhook URL for your repository or pipeline. This URL is where GitHub will send the event payload.\n    *   **Jenkins:** Often `<JENKINS_URL>/github-webhook/` or `<JENKINS_URL>/gitlab/webhook/` (if using generic webhook trigger).\n    *   **GitLab:** Webhooks are configured per project under `Settings > Integrations > Webhooks`.\n    *   **CircleCI:** Automatically configured when setting up a project.\n    *   **Azure DevOps:** `Project Settings > Service Hooks > Create Subscription` (for external services).\n\n2.  **Navigate to GitHub Repository Settings:**\n    *   Go to your GitHub repository.\n    *   Click on `Settings` (usually near the top right).\n    *   Click on `Webhooks` in the left sidebar.\n\n3.  **Add a New Webhook:**\n    *   Click the `Add webhook` button.\n    *   **Payload URL:** Paste the webhook URL provided by your CI/CD system.\n    *   **Content type:** Choose `application/json` (recommended for most modern CI/CD tools).\n    *   **Secret (Optional but Recommended):** Provide a secret token. GitHub will use this to create a hash signature of the payload (in the `X-Hub-Signature-256` header). Your CI/CD system can then use this secret to verify the payload's authenticity, ensuring it truly came from GitHub.\n    *   **Which events would you like to trigger this webhook?**\n        *   **Just the `push` event:** Triggers on new commits to branches.\n        *   **Send me `everything`:** Triggers on all events (can be noisy).\n        *   **Let me select individual events:** This is the most common and recommended choice for CI/CD.\n            *   **`Pushes`:** For CI builds.\n            *   **`Pull requests`:** For PR validation builds and tests.\n            *   Other events like `issues`, `releases`, `deployments` might be used for specific workflow automations.\n    *   **Active:** Ensure this checkbox is ticked to enable the webhook.\n    *   Click `Add webhook`.\n\n**Webhook Payload:**\nWhen an event occurs, GitHub sends an HTTP POST request to the Payload URL. The request body contains a JSON payload describing the event (e.g., who pushed, what files changed, what branch). The `X-GitHub-Event` header indicates the type of event (e.g., `push`, `pull_request`).\n\n**Security Considerations:**\n*   **Use a Secret:** Always configure a secret token and verify the signature on your CI/CD server to ensure the webhook requests are legitimate.\n*   **HTTPS:** Use HTTPS for your Payload URL to encrypt the communication.\n*   **Firewall/Network Security:** If your CI/CD server is self-hosted, ensure its network is secure and only accepts traffic from GitHub's known IP ranges (available in GitHub's API meta information)."
    }
]